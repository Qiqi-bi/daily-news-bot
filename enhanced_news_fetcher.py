#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced News Fetcher - æ”¹è¿›ç‰ˆæ–°é—»æŠ“å–å™¨
åŒ…å«æ›´å®Œå–„çš„é”™è¯¯å¤„ç†ã€è¯·æ±‚é¢‘ç‡æ§åˆ¶å’Œåçˆ¬è™«å¯¹ç­–
"""

import feedparser
import requests
import json
import time
import logging
import random
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import os
from collections import defaultdict

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RateLimiter:
    """è¯·æ±‚é¢‘ç‡é™åˆ¶å™¨"""
    def __init__(self):
        self.last_request_time = defaultdict(float)
        self.request_count = defaultdict(int)
        self.time_window = 60  # 60ç§’çª—å£
    
    def can_make_request(self, domain):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åœ¨æŒ‡å®šåŸŸåä¸Šå‘èµ·è¯·æ±‚"""
        current_time = time.time()
        
        # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
        for dom in list(self.last_request_time.keys()):
            if current_time - self.last_request_time[dom] > self.time_window:
                del self.last_request_time[dom]
                del self.request_count[dom]
        
        # æ£€æŸ¥è¯·æ±‚é¢‘ç‡
        if domain in self.request_count:
            # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡è¯·æ±‚
            if self.request_count[domain] >= 10:
                return False
        
        return True
    
    def record_request(self, domain):
        """è®°å½•è¯·æ±‚"""
        current_time = time.time()
        self.last_request_time[domain] = current_time
        self.request_count[domain] += 1

class ProxyPool:
    """ä»£ç†æ± ç®¡ç†"""
    def __init__(self):
        # å¯ä»¥ä»å¤–éƒ¨é…ç½®æ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨
        self.proxies = [
            {'http': 'http://127.0.0.1:7890', 'https': 'http://127.0.0.1:7890'},  # ç¤ºä¾‹ä»£ç†
            {'http': 'http://127.0.0.1:7897', 'https': 'http://127.0.0.1:7897'},  # ç¤ºä¾‹ä»£ç†
            # æ›´å¤šä»£ç†...
        ]
        self.current_index = 0
    
    def get_next_proxy(self):
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨ä»£ç†"""
        if not self.proxies:
            return None
        proxy = self.proxies[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.proxies)
        return proxy

# å…¨å±€å®ä¾‹
rate_limiter = RateLimiter()
proxy_pool = ProxyPool()

def fetch_with_multiple_methods(url: str, headers: dict = None) -> Optional[requests.Response]:
    """
    ä½¿ç”¨å¤šç§æ–¹æ³•å°è¯•è·å–å†…å®¹
    """
    if headers is None:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    methods = [
        # æ–¹æ³•1: æ ‡å‡†è¯·æ±‚
        lambda h: requests.get(url, headers=h, timeout=15),
        
        # æ–¹æ³•2: æ¨¡æ‹Ÿæµè§ˆå™¨
        lambda h: requests.get(url, headers={
            **h,
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }, timeout=15),
        
        # æ–¹æ³•3: ç§»åŠ¨ç«¯æ¨¡æ‹Ÿ
        lambda h: requests.get(url, headers={
            **h,
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1',
            'Accept': '*/*',
        }, timeout=15)
    ]
    
    for i, method in enumerate(methods):
        try:
            response = method(headers.copy())
            if response.status_code == 200:
                return response
            elif response.status_code in [403, 429]:
                # è¿™äº›é”™è¯¯å€¼å¾—å°è¯•å…¶ä»–æ–¹æ³•
                continue
        except Exception:
            continue
    
    return None

def enhanced_fetch_rss_feed(url: str, max_retries: int = 5, use_proxy: bool = False) -> Optional[feedparser.FeedParserDict]:
    """
    å¢å¼ºç‰ˆRSSæŠ“å–å‡½æ•°ï¼Œé›†æˆäº†æ‰€æœ‰æ”¹è¿›æªæ–½
    
    Args:
        url: RSSæºURL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        
    Returns:
        feedparserè§£æç»“æœæˆ–None
    """
    domain = urlparse(url).netloc
    
    # æ£€æŸ¥è¯·æ±‚é¢‘ç‡é™åˆ¶
    if not rate_limiter.can_make_request(domain):
        logger.info(f"è¾¾åˆ° {domain} çš„è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…...")
        time.sleep(60)
    
    # æ™ºèƒ½å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºè§„å¾‹
    delay = random.uniform(1.0, 3.0)
    time.sleep(delay)
    
    # è®°å½•è¯·æ±‚
    rate_limiter.record_request(domain)
    
    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨æŠ“å–RSSæº: {url} (å°è¯• {attempt + 1}/{max_retries})")
            
            # å°è¯•å¤šç§è¯·æ±‚æ–¹æ³•
            response = fetch_with_multiple_methods(url)
            
            # å¦‚æœå¯ç”¨äº†ä»£ç†ä¸”åˆå§‹è¯·æ±‚å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä»£ç†
            if not response and use_proxy:
                proxy = proxy_pool.get_next_proxy()
                if proxy:
                    logger.info(f"ä½¿ç”¨ä»£ç† {proxy} é‡æ–°è¯·æ±‚")
                    response = fetch_with_multiple_methods(url)
            
            if response and response.status_code == 200:
                feed = feedparser.parse(response.content)
                logger.info(f"æˆåŠŸæŠ“å– {len(feed.entries)} æ¡æ–°é—»")
                return feed
            elif response and response.status_code == 404:
                # 404é”™è¯¯ï¼šèµ„æºä¸å­˜åœ¨ï¼Œä¸é‡è¯•
                logger.error(f"404é”™è¯¯ - RSSæºä¸å­˜åœ¨: {url}")
                return None
            elif response and response.status_code == 403:
                # 403é”™è¯¯ï¼šæœåŠ¡å™¨æ‹’ç»è®¿é—®
                logger.warning(f"403é”™è¯¯ - è®¿é—®è¢«æ‹’ç»: {url}")
                if attempt < max_retries - 1:
                    # æ›´æ¢User-Agentå†è¯•
                    time.sleep(5 * (attempt + 1))
                continue
            elif response and response.status_code == 429:
                # 429é”™è¯¯ï¼šè¯·æ±‚è¿‡å¤š
                logger.warning(f"429é”™è¯¯ - è¯·æ±‚é¢‘ç‡è¿‡é«˜: {url}")
                # è·å–é‡è¯•æ—¶é—´ï¼ˆå¦‚æœæœ‰Retry-Afterå¤´ï¼‰
                retry_after = response.headers.get('Retry-After', 60)
                try:
                    delay = int(retry_after)
                except ValueError:
                    delay = 60  # é»˜è®¤ç­‰å¾…60ç§’
                if attempt < max_retries - 1:
                    logger.info(f"ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    time.sleep(delay * (attempt + 1))
                continue
            else:
                # å…¶ä»–é”™è¯¯
                status_code = response.status_code if response else 'N/A'
                logger.warning(f"HTTPé”™è¯¯ {status_code}: {url}")
                if attempt < max_retries - 1:
                    time.sleep(3 * (attempt + 1))
                    
        except requests.exceptions.ConnectionError:
            logger.warning(f"è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}): {url}")
            if attempt < max_retries - 1:
                time.sleep(5 * (attempt + 1))
        except requests.exceptions.Timeout:
            logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}): {url}")
            if attempt < max_retries - 1:
                time.sleep(10 * (attempt + 1))
        except Exception as e:
            logger.warning(f"å…¶ä»–é”™è¯¯ (å°è¯• {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(3 * (attempt + 1))
    
    logger.error(f"RSSæŠ“å–æœ€ç»ˆå¤±è´¥: {url}")
    return None

def enhanced_extract_news_items(rss_sources: List[str]) -> List[Dict[str, str]]:
    """
    ä»å¤šä¸ªRSSæºæå–æ–°é—»æ¡ç›®ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº
    
    Args:
        rss_sources: RSSæºåˆ—è¡¨
        
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«title, summary, link, importance_score
    """
    all_news = []
    seen_titles = set()  # ç”¨äºå»é‡
    processed_urls = set()  # ä¸´æ—¶å­˜å‚¨æœ¬æ¬¡è¿è¡Œå¤„ç†çš„URL
    
    # å®šä¹‰RSSæºæƒé‡ï¼Œæƒå¨æ€§è¶Šé«˜çš„æºæƒé‡è¶Šå¤§
    source_weights = {
        "https://feeds.bbci.co.uk/news/world/rss.xml": 1.0,  # BBC World
        "https://rss.nytimes.com/services/xml/rss/nyt/World.xml": 1.0,  # NYT World
        "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664": 0.9,  # CNBC Finance
        "https://techcrunch.com/feed/": 0.8,  # TechCrunch AI & Startup
        "https://finance.yahoo.com/news/rssindex": 0.8,  # Yahoo Finance
        "https://www.coindesk.com/arc/outboundfeeds/rss/": 0.7,  # CoinDesk
        "https://oilprice.com/rss/main": 0.7,  # OilPrice.com
        "https://news.ycombinator.com/rss": 0.7,  # Hacker News
        "https://www.reddit.com/r/worldnews/top/.rss?t=day": 0.6,  # Reddit WorldNews
        "https://www.reddit.com/r/videos/top/.rss?t=day": 0.5,  # Reddit è§†é¢‘èšåˆ
        "https://www.scmp.com/rss/2/feed": 0.8,  # South China Morning Post
        "http://arxiv.org/rss/cs.AI": 0.6,  # ArXiv AI Paper Daily
        "http://news.baidu.com/n?cmd=file&format=rss&tn=rss&sub=0": 0.7,  # ç™¾åº¦æ–°é—»
        "http://rss.people.com.cn/GB/303140/index.xml": 0.9,  # äººæ°‘ç½‘
        "http://www.xinhuanet.com/politics/news_politics.xml": 0.9,  # æ–°åç½‘ - æ—¶æ”¿
        "http://www.chinanews.com/rss/scroll-news.xml": 0.7,  # ä¸­å›½æ–°é—»ç½‘
        "https://www.thepaper.cn/rss.jsp": 0.6,  # æ¾æ¹ƒæ–°é—»
        "https://www.cls.cn/v3/highlights?app_id=70301d300f0f95a1&platform=pc": 0.7,  # è´¢è”ç¤¾
        "https://www.zhihu.com/rss": 0.5,  # çŸ¥ä¹æ¯æ—¥ç²¾é€‰
        "https://www.36kr.com/feed": 0.6,  # 36æ°ª
        "https://news.qq.com/rss/channels/finance/rss.xml": 0.7,  # è…¾è®¯è´¢ç»
        "https://rss.sina.com.cn/news/china/focus15.xml": 0.7,  # æ–°æµªæ–°é—»-å›½å†…ç„¦ç‚¹
    }
    
    for rss_url in rss_sources:
        feed = enhanced_fetch_rss_feed(rss_url, use_proxy=True)
        if not feed or not feed.entries:
            continue
            
        for entry in feed.entries[:5]:  # æ¯ä¸ªæºæœ€å¤šå–5æ¡
            title = entry.get('title', '').strip()
            summary = entry.get('summary', '').strip()
            link = entry.get('link', '')
            published_time = entry.get('published_parsed', None)
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡
            if link in processed_urls:
                continue
            
            # å»é‡æ£€æŸ¥
            if not title or title in seen_titles:
                continue
                
            seen_titles.add(title)
            
            # æ¸…ç†summaryä¸­çš„HTMLæ ‡ç­¾
            import re
            summary = re.sub(r'<[^>]+>', '', summary)
            summary = summary[:200] + '...' if len(summary) > 200 else summary
            
            # è®¡ç®—æ–°é—»é‡è¦æ€§åˆ†æ•°
            importance_score = calculate_importance_score(title, summary, rss_url, published_time, source_weights)
            
            all_news.append({
                'title': title,
                'summary': summary,
                'link': link,
                'importance_score': importance_score
            })
    
    # æŒ‰é‡è¦æ€§åˆ†æ•°é™åºæ’åº
    all_news.sort(key=lambda x: x['importance_score'], reverse=True)
    
    logger.info(f"æ€»å…±æå–åˆ° {len(all_news)} æ¡å”¯ä¸€æ–°é—»ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº")
    return all_news[:10]  # æœ€å¤šå¤„ç†10æ¡æ–°é—»

def calculate_importance_score(title: str, summary: str, source_url: str, published_time, source_weights: dict) -> float:
    """
    è®¡ç®—æ–°é—»é‡è¦æ€§åˆ†æ•°
    
    Args:
        title: æ–°é—»æ ‡é¢˜
        summary: æ–°é—»æ‘˜è¦
        source_url: æ–°é—»æ¥æºURL
        published_time: å‘å¸ƒæ—¶é—´
        source_weights: æ¥æºæƒé‡å­—å…¸
    
    Returns:
        é‡è¦æ€§åˆ†æ•° (0-10)
    """
    score = 0.0
    
    # 1. æ¥æºæƒé‡ (åŸºç¡€åˆ†æ•°)
    base_weight = source_weights.get(source_url, 0.5)  # é»˜è®¤æƒé‡0.5
    score += base_weight * 4  # æƒé‡å æ¯”40%
    
    # 2. æ ‡é¢˜å…³é”®è¯åˆ†æ (æ—¶æ•ˆæ€§ã€ç´§æ€¥æ€§ã€å½±å“åŠ›)
    title_lower = title.lower()
    urgency_keywords = ['çªå‘', 'ç´§æ€¥', 'è­¦å‘Š', 'å±æœº', 'æš´è·Œ', 'æš´æ¶¨', 'æˆ˜', 'å†²çª', 'åˆ¶è£', 'æ”¿ç­–', 'å¤®è¡Œ', 'åˆ©ç‡', 'gdp', 'å°±ä¸š', 'é€šèƒ€']
    financial_keywords = ['ç»æµ', 'è‚¡å¸‚', 'åŸºé‡‘', 'å€ºåˆ¸', 'ç¾å…ƒ', 'äººæ°‘å¸', 'é»„é‡‘', 'çŸ³æ²¹', 'æ¯”ç‰¹å¸', 'ai', 'ç§‘æŠ€', 'å…¬å¸', 'è´¢æŠ¥']
    china_keywords = ['ä¸­å›½', 'chinese', 'beijing', 'shanghai', 'hk', 'æ¸¯', 'aè‚¡', 'äººæ°‘å¸', 'cny', 'è´¸æ˜“', 'ä¸­ç¾', 'ç¾ä¸­']
    
    # æ£€æŸ¥ç´§æ€¥å…³é”®è¯
    for keyword in urgency_keywords:
        if keyword in title_lower:
            score += 1.0  # æ¯ä¸ªç´§æ€¥å…³é”®è¯+1åˆ†
    
    # æ£€æŸ¥é‡‘èå…³é”®è¯
    for keyword in financial_keywords:
        if keyword in title_lower:
            score += 0.5  # æ¯ä¸ªé‡‘èå…³é”®è¯+0.5åˆ†
    
    # æ£€æŸ¥ä¸­å›½ç›¸å…³å…³é”®è¯
    for keyword in china_keywords:
        if keyword in title_lower:
            score += 1.0  # æ¯ä¸ªä¸­å›½ç›¸å…³å…³é”®è¯+1åˆ†
    
    # 3. å†…å®¹é•¿åº¦ (æ›´é•¿çš„å†…å®¹å¯èƒ½æ›´é‡è¦)
    content_length = len(title) + len(summary)
    if content_length > 200:
        score += 1.0
    elif content_length > 100:
        score += 0.5
    
    # 4. æ—¶é—´å› ç´  (å¦‚æœæ˜¯ä»Šå¤©å‘å¸ƒçš„æ–°é—»ï¼Œå¢åŠ åˆ†æ•°)
    import datetime
    if published_time:
        pub_date = datetime.datetime(*published_time[:6])
        now = datetime.datetime.now()
        hours_diff = (now - pub_date).total_seconds() / 3600
        if hours_diff <= 24:  # 24å°æ—¶å†…å‘å¸ƒçš„æ–°é—»
            score += 1.0
        elif hours_diff <= 48:  # 48å°æ—¶å†…å‘å¸ƒçš„æ–°é—»
            score += 0.5
    
    # 5. æ ‡é¢˜é•¿åº¦å’Œç‰¹å¾ (æ ‡é¢˜é•¿åº¦é€‚ä¸­ä¸”åŒ…å«æ•°å­—æˆ–ç¬¦å·å¯èƒ½æ›´é‡è¦)
    if 30 < len(title) < 100:  # æ ‡é¢˜é•¿åº¦é€‚ä¸­
        score += 0.5
    if ':' in title or '-' in title:  # åŒ…å«åˆ†éš”ç¬¦
        score += 0.3
    if any(char.isdigit() for char in title):  # åŒ…å«æ•°å­—
        score += 0.2
    
    return min(score, 10.0)  # é™åˆ¶æœ€å¤§åˆ†æ•°ä¸º10

def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºå¢å¼ºç‰ˆæ–°é—»æŠ“å–å™¨çš„ä½¿ç”¨
    """
    logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæ–°é—»æŠ“å–å™¨...")
    
    # RSSæºåˆ—è¡¨
    RSS_SOURCES = [
        "https://feeds.bbci.co.uk/news/world/rss.xml",  # BBC World
        "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",  # NYT World
        "https://techcrunch.com/feed/",  # TechCrunch AI & Startup
        "http://news.baidu.com/n?cmd=file&format=rss&tn=rss&sub=0",  # ç™¾åº¦æ–°é—»
    ]
    
    try:
        # æŠ“å–æ–°é—»
        news_items = enhanced_extract_news_items(RSS_SOURCES)
        logger.info(f"æˆåŠŸæå– {len(news_items)} æ¡æ–°é—»")
        
        # è¾“å‡ºå‰å‡ æ¡æ–°é—»ä½œä¸ºç¤ºä¾‹
        for i, item in enumerate(news_items[:3]):
            print(f"\n{i+1}. {item['title']}")
            print(f"   é‡è¦æ€§åˆ†æ•°: {item['importance_score']:.2f}")
            print(f"   é“¾æ¥: {item['link']}")
            print(f"   æ‘˜è¦: {item['summary'][:100]}...")
        
    except Exception as e:
        logger.exception(f"ä¸»å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()