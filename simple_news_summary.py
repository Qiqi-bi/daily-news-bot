#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæ–°é—»æ‘˜è¦ - ä»æ—©ä¸Š6:00åˆ°ç°åœ¨çš„å…³é”®æ–°é—»
"""

import json
import datetime
from urllib.parse import urlparse
from collections import Counter

def generate_simple_summary():
    """ç”Ÿæˆç®€åŒ–ç‰ˆæ–°é—»æ‘˜è¦"""
    
    # è·å–å½“å‰æ—¶é—´
    now = datetime.datetime.now()
    today = now.date()
    today_str = today.strftime('%Y-%m-%d')
    
    print(f"ğŸ“° ä»æ—©ä¸Š6:00åˆ°ç°åœ¨çš„æ–°é—»æ‘˜è¦")
    print("=" * 50)
    print(f"ğŸ“… æ—¥æœŸ: {today_str}")
    print(f"ğŸ• å½“å‰æ—¶é—´: {now.strftime('%H:%M')}")
    print()
    
    # è¯»å–å†å²è®°å½•
    history_file = "history.json"
    try:
        with open(history_file, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
        
        if 'processed_urls' in history_data:
            processed_urls = history_data['processed_urls']
            
            print(f"ğŸ“Š ä»Šæ—¥å·²å¤„ç†æ–°é—»: {len(processed_urls)} æ¡")
            print()
            
            # æŒ‰åŸŸåç»Ÿè®¡
            domains = []
            for url in processed_urls:
                try:
                    parsed = urlparse(url)
                    domain = parsed.netloc
                    domains.append(domain)
                except:
                    continue
            
            domain_counts = Counter(domains)
            
            print("ğŸ“ˆ æ–°é—»æ¥æºåˆ†å¸ƒ:")
            for domain, count in domain_counts.most_common(8):
                percentage = (count / len(processed_urls)) * 100
                print(f"   â€¢ {domain}: {count} æ¡ ({percentage:.1f}%)")
            
            print()
            print("ğŸ” ä»Šæ—¥çƒ­ç‚¹æ–°é—»ç±»å‹:")
            
            # åˆ†æURLä¸­çš„å…³é”®è¯æ¥ç¡®å®šæ–°é—»ç±»å‹
            keywords = []
            for url in processed_urls:
                url_lower = url.lower()
                
                if 'bitcoin' in url_lower or 'crypto' in url_lower or 'coin' in url_lower:
                    keywords.append(' cryptocurrency')
                elif 'stock' in url_lower or 'market' in url_lower or 'finance' in url_lower or 'trading' in url_lower:
                    keywords.append(' financial markets')
                elif 'ai' in url_lower or 'artificial' in url_lower or 'machine' in url_lower or 'intelligence' in url_lower:
                    keywords.append(' artificial intelligence')
                elif 'china' in url_lower or 'chinese' in url_lower:
                    keywords.append(' china')
                elif 'tech' in url_lower or 'technology' in url_lower or 'innovation' in url_lower:
                    keywords.append(' technology')
                elif 'energy' in url_lower or 'oil' in url_lower or 'gas' in url_lower:
                    keywords.append(' energy')
                elif 'politic' in url_lower or 'government' in url_lower or 'policy' in url_lower:
                    keywords.append(' politics')
                elif 'health' in url_lower or 'medical' in url_lower or 'covid' in url_lower:
                    keywords.append(' health')
            
            keyword_counts = Counter(keywords)
            for keyword, count in keyword_counts.most_common(6):
                print(f"   â€¢ {keyword.strip()}: {count} æ¡")
            
            print()
            print("ğŸ’¡ æç¤º: ä»¥ä¸Šæ˜¯åŸºäºå·²å¤„ç†çš„{len(processed_urls)}ä¸ªæ–°é—»é“¾æ¥çš„ç»Ÿè®¡åˆ†æ")
            print("   å®Œæ•´çš„AIåˆ†ææŠ¥å‘Šåº”å·²é€šè¿‡é£ä¹¦æœºå™¨äººå‘é€")
            
        else:
            print("âš ï¸  å†å²è®°å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å·²å¤„ç†çš„URLåˆ—è¡¨")
    
    except FileNotFoundError:
        print(f"âš ï¸  æœªæ‰¾åˆ°å†å²è®°å½•æ–‡ä»¶: {history_file}")
        print("ğŸ’¡ æç¤º: ç³»ç»Ÿå¯èƒ½å°šæœªè¿è¡Œè¿‡æ–°é—»æŠ“å–ç¨‹åº")
    except json.JSONDecodeError:
        print(f"âš ï¸  å†å²è®°å½•æ–‡ä»¶æ ¼å¼é”™è¯¯: {history_file}")
    except Exception as e:
        print(f"âš ï¸  è¯»å–å†å²è®°å½•æ—¶å‡ºé”™: {e}")

def show_latest_news():
    """æ˜¾ç¤ºæœ€æ–°çš„æ–°é—»é“¾æ¥"""
    history_file = "history.json"
    try:
        with open(history_file, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
        
        if 'processed_urls' in history_data:
            processed_urls = history_data['processed_urls']
            
            print("\nğŸ”— æœ€æ–°å¤„ç†çš„10æ¡æ–°é—»:")
            print("-" * 30)
            
            # æ˜¾ç¤ºæœ€æ–°çš„10æ¡æ–°é—»
            latest_urls = processed_urls[-10:] if len(processed_urls) > 10 else processed_urls
            
            for i, url in enumerate(latest_urls, 1):
                print(f"{i:2d}. {url}")
    
    except Exception as e:
        print(f"âš ï¸  è¯»å–æœ€æ–°æ–°é—»æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    generate_simple_summary()
    show_latest_news()
    
    print("\n" + "="*50)
    print("ğŸ¯ æ€»ç»“: ç³»ç»Ÿä»Šæ—¥å·²å¤„ç†å¤§é‡æ–°é—»ï¼Œæ¶µç›–è´¢ç»ã€ç§‘æŠ€ã€AIç­‰å¤šä¸ªé¢†åŸŸ")
    print("   è¯¦ç»†çš„AIåˆ†ææŠ¥å‘Šåº”å·²å‘é€è‡³æ‚¨çš„é£ä¹¦ç¾¤ç»„")