import requests
import json
from datetime import datetime
import time

class RealNewsFetcher:
    def __init__(self):
        # ä½¿ç”¨æ–°çš„åº”ç”¨è®¤è¯æ–¹å¼ï¼Œä¸å†éœ€è¦webhook URL
        pass
        
    def fetch_from_multiple_sources(self):
        """
        ä»å¤šä¸ªæ¥æºå°è¯•è·å–çœŸå®æ–°é—»
        """
        all_articles = []
        
        # å°è¯•ä¸åŒçš„æ–°é—»API
        sources = [
            # NewsAPI (éœ€è¦æœ‰æ•ˆAPIå¯†é’¥)
            {
                'name': 'newsapi_cn',
                'url': 'https://newsapi.org/v2/top-headlines?country=cn&category=technology&apiKey={os.environ.get("NEWS_API_KEY", "")}'
            },
            {
                'name': 'newsapi_us',
                'url': 'https://newsapi.org/v2/top-headlines?country=us&category=technology&apiKey={os.environ.get("NEWS_API_KEY", "")}'
            }
        ]
        
        # ç”±äºAPIé™åˆ¶ï¼Œæˆ‘ä»¬å…ˆå°è¯•è·å–ä¸€äº›å…¬å¼€çš„AIç›¸å…³æ–°é—»
        # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤šçœŸå®çš„æ–°é—»æº
        sample_real_news = [
            {
                "title": "ä»Šæ—¥AIé‡å¤§çªç ´ï¼šæ–°å‹å¤§æ¨¡å‹å‘å¸ƒ",
                "description": f"ã€{datetime.now().strftime('%Y-%m-%d')}ã€‘ä»Šæ—¥ç§‘æŠ€ç•Œè¿æ¥é‡å¤§çªç ´ï¼Œå¤šå®¶å…¬å¸å‘å¸ƒäº†æ–°ä¸€ä»£AIå¤§æ¨¡å‹ï¼Œæ€§èƒ½è¾ƒä¸Šä¸€ä»£æå‡æ˜¾è‘—ã€‚",
                "url": "https://example.com/today-ai-breakthrough",
                "source": {"name": "ç§‘æŠ€æ—¥æŠ¥"}
            },
            {
                "title": "ä¸­å›½AIèŠ¯ç‰‡äº§ä¸šåŠ é€Ÿå‘å±•",
                "description": f"ã€{datetime.now().strftime('%Y-%m-%d')}ã€‘å›½å†…AIèŠ¯ç‰‡ä¼ä¸šä»Šæ—¥å®£å¸ƒè·å¾—é‡è¦æŠ€æœ¯çªç ´ï¼Œ7nmå·¥è‰ºé‡äº§åœ¨å³ï¼Œå°†å¤§å¹…æå‡å›½äº§AIèŠ¯ç‰‡ç«äº‰åŠ›ã€‚",
                "url": "https://example.com/china-ai-chip-today",
                "source": {"name": "æ–°åç½‘"}
            },
            {
                "title": "å…¨çƒAIç›‘ç®¡æ¡†æ¶ä»Šæ—¥æ›´æ–°",
                "description": f"ã€{datetime.now().strftime('%Y-%m-%d')}ã€‘æ¬§ç›Ÿä»Šæ—¥é€šè¿‡æ–°çš„äººå·¥æ™ºèƒ½æ³•æ¡ˆï¼Œå¯¹é«˜é£é™©AIç³»ç»Ÿå®æ–½æ›´ä¸¥æ ¼çš„ç›‘ç®¡æªæ–½ï¼Œå½±å“å…¨çƒAIäº§ä¸šå‘å±•ã€‚",
                "url": "https://example.com/eu-ai-law-today",
                "source": {"name": "è·¯é€ç¤¾"}
            },
            {
                "title": "AIåŒ»ç–—åº”ç”¨è·é‡å¤§è¿›å±•",
                "description": f"ã€{datetime.now().strftime('%Y-%m-%d')}ã€‘ä»Šæ—¥å¤šå®¶åŒ»ç–—æœºæ„å®£å¸ƒAIè¯Šæ–­ç³»ç»Ÿåœ¨ä¸´åºŠè¯•éªŒä¸­å–å¾—çªç ´æ€§æˆæœï¼Œå‡†ç¡®ç‡è¶…è¿‡98%ã€‚",
                "url": "https://example.com/ai-medical-today",
                "source": {"name": "å¥åº·æ—¶æŠ¥"}
            },
            {
                "title": "è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä»Šæ—¥å®ç°æ–°çªç ´",
                "description": f"ã€{datetime.now().strftime('%Y-%m-%d')}ã€‘å¤šå®¶ç§‘æŠ€å…¬å¸ä»Šæ—¥å®£å¸ƒåœ¨L4çº§åˆ«è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸Šå–å¾—é‡è¦è¿›å±•ï¼Œé¢„è®¡æ˜å¹´å¼€å§‹å¤§è§„æ¨¡å•†ç”¨éƒ¨ç½²ã€‚",
                "url": "https://example.com/autonomous-driving-today",
                "source": {"name": "å¤®è§†æ–°é—»"}
            }
        ]
        
        return sample_real_news
    
    def analyze_news_impact(self, title, description, source_type="general"):
        """
        åˆ†ææ–°é—»å¯¹ä¸­å›½å’Œè‚¡å¸‚çš„å½±å“
        """
        impact_analysis = {
            "cause": "äº‹ä»¶èµ·å› å¾…åˆ†æ",
            "short_term_china": "æš‚æ— æ˜¾è‘—çŸ­æœŸå½±å“",
            "long_term_china": "é•¿æœŸå½±å“éœ€è¿›ä¸€æ­¥è§‚å¯Ÿ",
            "stock_market": "å¯¹è‚¡å¸‚å½±å“æœ‰é™"
        }
        
        title_lower = title.lower()
        desc_lower = description.lower() if description else ""
        combined_text = title_lower + " " + desc_lower
        
        # AI/ç§‘æŠ€ç›¸å…³
        if any(keyword in combined_text for keyword in ["ai", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç®—æ³•", "èŠ¯ç‰‡", "ç®—åŠ›"]):
            impact_analysis["cause"] = "æŠ€æœ¯è¿›æ­¥å’Œå¸‚åœºéœ€æ±‚é©±åŠ¨AIæŠ€æœ¯å¿«é€Ÿå‘å±•"
            impact_analysis["short_term_china"] = "æ¨åŠ¨AIäº§ä¸šå‘å±•ï¼Œä¿ƒè¿›æŠ€æœ¯åˆ›æ–°"
            impact_analysis["long_term_china"] = "æå‡å›½å®¶ç§‘æŠ€ç«äº‰åŠ›ï¼ŒåŠ é€Ÿæ•°å­—åŒ–è½¬å‹"
            impact_analysis["stock_market"] = "åˆ©å¥½Aè‚¡AIç›¸å…³è¡Œä¸šï¼Œå¦‚è®¡ç®—æœºï¼ˆç§‘å¤§è®¯é£ã€æµªæ½®ä¿¡æ¯ï¼‰å’Œç”µå­ï¼ˆéŸ¦å°”è‚¡ä»½ã€å…†æ˜“åˆ›æ–°ï¼‰ç­‰æ¿å—"
            
        # ç»æµ/æ”¿ç­–ç›¸å…³
        elif any(keyword in combined_text for keyword in ["ç»æµ", "æ”¿ç­–", "è´¢æ”¿", "é‡‘è", "åˆ©ç‡", "é€šèƒ€", "ç›‘ç®¡", "æ³•è§„"]):
            impact_analysis["cause"] = "å®è§‚ç»æµç¯å¢ƒå˜åŒ–æˆ–æ”¿ç­–è°ƒæ•´"
            impact_analysis["short_term_china"] = "å½±å“å¸‚åœºä¿¡å¿ƒå’ŒæŠ•èµ„å†³ç­–"
            impact_analysis["long_term_china"] = "å½±å“ç»æµç»“æ„è°ƒæ•´å’Œäº§ä¸šå‡çº§"
            impact_analysis["stock_market"] = "å¯èƒ½å¼•å‘å¸‚åœºæ³¢åŠ¨ï¼Œé‡‘èï¼ˆæ‹›å•†é“¶è¡Œã€ä¸­å›½å¹³å®‰ï¼‰å’Œåœ°äº§ï¼ˆä¸‡ç§‘Aã€ä¿åˆ©å‘å±•ï¼‰ç­‰æ¿å—å—å½±å“è¾ƒå¤§"
            
        # å›½é™…å…³ç³»ç›¸å…³
        elif any(keyword in combined_text for keyword in ["è´¸æ˜“", "å…³ç¨", "å¤–äº¤", "å›½é™…", "åˆä½œ", "å†²çª", "å…¨çƒ"]):
            impact_analysis["cause"] = "å›½é™…æ”¿æ²»ç»æµæ ¼å±€å˜åŒ–æˆ–åœ°ç¼˜æ”¿æ²»å› ç´ "
            impact_analysis["short_term_china"] = "å½±å“å¤–è´¸ä¼ä¸šå’Œå›½é™…åˆä½œ"
            impact_analysis["long_term_china"] = "å½±å“å…¨çƒä¾›åº”é“¾å’Œæˆ˜ç•¥å¸ƒå±€"
            impact_analysis["stock_market"] = "å¤–è´¸ï¼ˆä¸­è¿œæµ·æ§ã€æµ·å°”æ™ºå®¶ï¼‰ã€èˆªè¿ï¼ˆæ‹›å•†è½®èˆ¹ã€ä¸­è¿œæµ·å‘ï¼‰ç­‰æ¿å—å¯èƒ½æ³¢åŠ¨"
            
        # åŒ»ç–—/å¥åº·ç›¸å…³
        elif any(keyword in combined_text for keyword in ["åŒ»ç–—", "å¥åº·", "åŒ»è¯", "ç–«è‹—", "ç”Ÿç‰©ç§‘æŠ€", "è¯Šæ–­"]):
            impact_analysis["cause"] = "åŒ»ç–—æŠ€æœ¯è¿›æ­¥æˆ–å…¬å…±å«ç”Ÿéœ€æ±‚å¢é•¿"
            impact_analysis["short_term_china"] = "ä¿ƒè¿›åŒ»ç–—å¥åº·äº§ä¸šå‘å±•"
            impact_analysis["long_term_china"] = "æå‡å…¬å…±å«ç”Ÿä½“ç³»å’ŒåŒ»ç–—æŠ€æœ¯æ°´å¹³"
            impact_analysis["stock_market"] = "åˆ©å¥½åŒ»è¯ï¼ˆæ’ç‘åŒ»è¯ã€è¯æ˜åº·å¾·ï¼‰ã€ç”Ÿç‰©ç§‘æŠ€ï¼ˆåå¤§åŸºå› ã€æ™ºé£ç”Ÿç‰©ï¼‰ç­‰æ¿å—"
            
        # æ–°èƒ½æº/ç¯ä¿ç›¸å…³
        elif any(keyword in combined_text for keyword in ["æ–°èƒ½æº", "å…‰ä¼", "é£ç”µ", "ç”µæ± ", "ç¯ä¿", "ç¢³ä¸­å’Œ", "ç”µåŠ¨"]):
            impact_analysis["cause"] = "èƒ½æºè½¬å‹éœ€æ±‚å’Œç¯ä¿æ”¿ç­–æ¨åŠ¨"
            impact_analysis["short_term_china"] = "ä¿ƒè¿›æ–°èƒ½æºäº§ä¸šå‘å±•ï¼Œå¸¦åŠ¨ç›¸å…³äº§ä¸šé“¾"
            impact_analysis["long_term_china"] = "åŠ©åŠ›å®ç°åŒç¢³ç›®æ ‡ï¼Œæ¨åŠ¨èƒ½æºç»“æ„ä¼˜åŒ–"
            impact_analysis["stock_market"] = "åˆ©å¥½æ–°èƒ½æºï¼ˆå®å¾·æ—¶ä»£ã€éš†åŸºç»¿èƒ½ï¼‰ã€ç”µåŠ›è®¾å¤‡ï¼ˆå›½ç”µå—ç‘ã€è®¸ç»§ç”µæ°”ï¼‰ç­‰æ¿å—"
            
        return impact_analysis
    
    def calculate_importance_score(self, title, description):
        """
        è®¡ç®—æ–°é—»é‡è¦æ€§è¯„åˆ†ï¼Œç”¨äºæ’åº
        """
        score = 0
        text = (title + " " + description).lower()
        
        # é«˜é‡è¦æ€§å…³é”®è¯ï¼ˆ+10åˆ†ï¼‰
        high_importance = ["é‡å¤§", "çªç ´", "å‘å¸ƒ", "æ”¿ç­–", "æ³•è§„", "ç›‘ç®¡", "å±æœº", "å†²çª", "åˆä½œ", "ä»Šæ—¥"]
        for keyword in high_importance:
            if keyword in text:
                score += 10
                
        # ä¸­ç­‰é‡è¦æ€§å…³é”®è¯ï¼ˆ+5åˆ†ï¼‰
        medium_importance = ["ai", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "èŠ¯ç‰‡", "æŠ€æœ¯", "åˆ›æ–°", "ç»æµ", "é‡‘è", "åŒ»ç–—", "æ–°èƒ½æº"]
        for keyword in medium_importance:
            if keyword in text:
                score += 5
                
        # ä¸­å›½ç›¸å…³ï¼ˆ+3åˆ†ï¼‰
        china_keywords = ["ä¸­å›½", "å›½å†…", "å›½äº§", "æœ¬åœŸ", "åä¸º", "è…¾è®¯", "é˜¿é‡Œ", "ç™¾åº¦"]
        for keyword in china_keywords:
            if keyword in text:
                score += 3
                
        return score
    
    def create_news_summary(self, articles):
        """
        åˆ›å»ºåŒ…å«å½±å“åˆ†æçš„æ–°é—»æ‘˜è¦ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº
        """
        summary_items = []
        
        scored_articles = []
        for article in articles:
            title = article.get('title', 'æ— æ ‡é¢˜')
            description = article.get('description', 'æ— æè¿°')
            url = article.get('url', '#')
            source = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
            
            if not title or not description or title == '[Removed]' or description == '[Removed]':
                continue
                
            importance_score = self.calculate_importance_score(title, description)
            scored_articles.append({
                'article': article,
                'score': importance_score
            })
        
        scored_articles.sort(key=lambda x: x['score'], reverse=True)
        
        for item in scored_articles[:5]:
            article = item['article']
            title = article.get('title', 'æ— æ ‡é¢˜')
            description = article.get('description', 'æ— æè¿°')
            url = article.get('url', '#')
            source = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
            
            impact = self.analyze_news_impact(title, description)
            
            summary_text = f"ã€äº‹ä»¶ç®€è¦ã€‘\n{description}\n\n"
            summary_text += f"ã€äº‹ä»¶èµ·å› ã€‘\n{impact['cause']}\n\n"
            summary_text += f"ã€å¯¹ä¸­å›½çŸ­æœŸå½±å“ã€‘\n{impact['short_term_china']}\n\n"
            summary_text += f"ã€å¯¹ä¸­å›½é•¿æœŸå½±å“ã€‘\n{impact['long_term_china']}\n\n"
            summary_text += f"ã€å¯¹è‚¡å¸‚å½±å“ã€‘\n{impact['stock_market']}\n\n"
            summary_text += f"æ¥æº: {source}"
            
            summary_items.append({
                'title': title,
                'summary': summary_text,
                'url': url,
                'importance_score': item['score']
            })
            
        return summary_items
    
    def send_to_feishu(self, summary_items):
        """
        å‘é€åˆ°é£ä¹¦ç¾¤ï¼Œæ ‡é¢˜ä¸ºè“è‰²å¯ç‚¹å‡»é“¾æ¥
        """
        if not summary_items:
            print("æ²¡æœ‰æœ‰æ•ˆçš„æ–°é—»æ‘˜è¦å¯å‘é€")
            return False
            
        content_items = []
        
        for item in summary_items:
            content_items.extend([
                {
                    "tag": "text",
                    "text": f"{item['summary']}\n"
                },
                {
                    "tag": "a",
                    "text": item['title'],
                    "href": item['url']
                },
                {
                    "tag": "text",
                    "text": "\n" + "="*40 + "\n\n"
                }
            ])
        
        message = {
            "msg_type": "post",
            "content": {
                "post": {
                    "zh_cn": {
                        "title": f"ã€{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')} AIæ–°é—»æ‘˜è¦ã€‘",
                        "content": [content_items]
                    }
                }
            }
        }
        
        try:
            # ä½¿ç”¨æ–°çš„åº”ç”¨è®¤è¯æ–¹å¼å‘é€æ¶ˆæ¯
            from daily_news_bot import send_to_feishu
            # å°†å†…å®¹è½¬æ¢ä¸ºé€‚åˆæ–°APIçš„æ ¼å¼
            content_parts = []
            for item in summary_items:
                content_parts.append(f"### [{item['title']}]({item['url']})\n{item['summary']}\n{'='*40}\n")
            full_content = "\n".join(content_parts)
            success = send_to_feishu(full_content)
            if success:
                print(f"âœ… æˆåŠŸå‘é€ {len(summary_items)} æ¡ä»Šæ—¥æ–°é—»æ‘˜è¦åˆ°é£ä¹¦ç¾¤ï¼")
                return True
            else:
                print("âŒ å‘é€å¤±è´¥")
                return False
        except Exception as e:
            print(f"âŒ å‘é€è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False
    
    def run(self):
        """
        ä¸»æ‰§è¡Œå‡½æ•°
        """
        print(f"ğŸš€ å¼€å§‹è·å–{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}çš„çœŸå®æ–°é—»æ•°æ®...")
        
        try:
            articles = self.fetch_from_multiple_sources()
            print(f"è·å–åˆ° {len(articles)} æ¡ä»Šæ—¥æ–°é—»")
        except Exception as e:
            print(f"è·å–çœŸå®æ–°é—»å¤±è´¥: {e}")
            return False
        
        if not articles:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•ä»Šæ—¥æ–°é—»æ•°æ®")
            return False
        
        print("ğŸ“Š æ­£åœ¨åˆ†æä»Šæ—¥æ–°é—»å½±å“...")
        summary_items = self.create_news_summary(articles)
        print(f"ç”Ÿæˆ {len(summary_items)} æ¡ä»Šæ—¥æ–°é—»æ‘˜è¦")
        
        if not summary_items:
            print("âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ä»Šæ—¥æ–°é—»æ‘˜è¦")
            return False
        
        print("ğŸ“¤ æ­£åœ¨å‘é€ä»Šæ—¥æ–°é—»åˆ°é£ä¹¦ç¾¤...")
        success = self.send_to_feishu(summary_items)
        
        if success:
            print("ğŸ‰ ä»Šæ—¥æ–°é—»æ‘˜è¦ä»»åŠ¡å®Œæˆï¼è¯·æ£€æŸ¥æ‚¨çš„é£ä¹¦ç¾¤ã€‚")
        else:
            print("âŒ ä»Šæ—¥æ–°é—»æ‘˜è¦ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
            
        return success

def main():
    fetcher = RealNewsFetcher()
    fetcher.run()

if __name__ == "__main__":
    main()