#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Daily News Bot - å…¨çƒæƒ…æŠ¥ä¸é‡‘èåˆ†æè‡ªåŠ¨åŒ–è„šæœ¬

åŠŸèƒ½ï¼š
1. ä»å›½é™…ä¸»æµRSSæºæŠ“å–æ–°é—»ï¼ˆé€šè¿‡ä»£ç†ï¼‰
2. è°ƒç”¨DeepSeekå¤§æ¨¡å‹APIè¿›è¡Œæ·±åº¦åˆ†æ
3. å‘é€åˆ°é£ä¹¦ç¾¤ï¼ˆä½¿ç”¨webhookæ–¹å¼ï¼‰

ä½œè€…ï¼šPythoné«˜çº§å·¥ç¨‹å¸ˆ
"""

import feedparser
import requests
import json
import time
import logging
from typing import List, Dict, Optional
from urllib.parse import urljoin
import os

# ==================== é…ç½®åŒºåŸŸ ====================
# è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ä»¥ä¸‹é…ç½®

# LLM API é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
API_KEY = os.environ.get('DEEPSEEK_API_KEY', 'YOUR_DEEPSEEK_API_KEY_HERE')
BASE_URL = "https://api.deepseek.com"

# æ™ºèƒ½ä»£ç†é…ç½® - æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­
if os.environ.get('GITHUB_ACTIONS'):
    # åœ¨GitHub Actionsä¸­ï¼Œç›´æ¥è¿æ¥
    PROXIES = None
else:
    # æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨ä»£ç†
    PROXIES = {
        'http': 'http://127.0.0.1:7897',
        'https': 'http://127.0.0.1:7897'
    }

# RSSæºåˆ—è¡¨ï¼ˆç»ˆæç‰ˆæœ¬ï¼‰
RSS_SOURCES = [
    # --- 1. å›½é™…é¡¶æµ (BBC/NYT) ---
    "https://feeds.bbci.co.uk/news/world/rss.xml",  # BBC World
    "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",  # NYT World
    
    # --- 2. åå°”è¡—/é‡‘è (CNBC) ---
    "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664",  # CNBC Finance

    # --- 3. ç¡…è°·/ç§‘æŠ€ (TechCrunch) ---
    "https://techcrunch.com/feed/",  # TechCrunch AI & Startup

    # --- 4. é›…è™è´¢ç» (æ–°å¢) ---
    "https://finance.yahoo.com/news/rssindex",  # Yahoo Finance

    # --- 5. åŠ å¯†è´§å¸ (Crypto) ---
    "https://www.coindesk.com/arc/outboundfeeds/rss/",  # CoinDesk

    # --- 6. èƒ½æºä¸æˆ˜äº‰ (Energy) ---
    "https://oilprice.com/rss/main",  # OilPrice.com

    # --- 7. ç¤¾äº¤ä¸é»‘å®¢åŠ¨å‘ (æ›¿ä»£ Twitter/GitHub) ---
    # Hacker News (å…¨çƒæå®¢éƒ½åœ¨è®¨è®ºä»€ä¹ˆï¼Œæ˜¯ GitHub æœ€å¥½çš„é£å‘æ ‡)
    "https://news.ycombinator.com/rss",
    # Reddit WorldNews (å…¨çƒç½‘æ°‘æœ€çƒ­è®®çš„çªå‘äº‹ä»¶)
    "https://www.reddit.com/r/worldnews/top/.rss?t=day",
    
    # --- 8. Reddit è§†é¢‘èšåˆ (æ–°å¢) ---
    "https://www.reddit.com/r/videos/top/.rss?t=day",  # Reddit è§†é¢‘èšåˆ - å…¨çƒ24å°æ—¶å†…æœ€çƒ­é—¨çš„è§†é¢‘é›†åˆ
    
    # --- 9. äºšæ´²/ä¸­å›½å•†ä¸š (æ–°å¢) ---
    "https://www.scmp.com/rss/2/feed",  # South China Morning Post (å—åæ—©æŠ¥ - ä¸­å›½å•†ä¸šç‰ˆå—)
    
    # --- 10. å­¦æœ¯/AIç ”ç©¶ (æ–°å¢) ---
    "http://arxiv.org/rss/cs.AI",  # ArXiv AI Paper Daily (å­¦æœ¯æº)
    
    # --- 11. å›½å†…ä¸»æµæ–°é—»æº (æ–°å¢) ---
    "http://news.baidu.com/n?cmd=file&format=rss&tn=rss&sub=0",  # ç™¾åº¦æ–°é—»
    "http://rss.people.com.cn/GB/303140/index.xml",  # äººæ°‘ç½‘
    "http://www.xinhuanet.com/politics/news_politics.xml",  # æ–°åç½‘ - æ—¶æ”¿
    "http://www.chinanews.com/rss/scroll-news.xml",  # ä¸­å›½æ–°é—»ç½‘
    "https://www.thepaper.cn/rss.jsp",  # æ¾æ¹ƒæ–°é—»
    "http://www.ce.cn/cysc/jg/zxbd/rss2.xml",  # ä¸­å›½ç»æµç½‘
    "https://www.cls.cn/v3/highlights?app_id=70301d300f0f95a1&platform=pc",  # è´¢è”ç¤¾ (éœ€è¦é€‚é…)
    
    # --- 12. å›½å†…ç§‘æŠ€æ–°é—» (æ–°å¢) ---
    "https://www.zhihu.com/rss",  # çŸ¥ä¹æ¯æ—¥ç²¾é€‰
    "https://www.36kr.com/feed",  # 36æ°ª
    "https://news.qq.com/rss/channels/finance/rss.xml",  # è…¾è®¯è´¢ç»
    "https://rss.sina.com.cn/news/china/focus15.xml",  # æ–°æµªæ–°é—»-å›½å†…ç„¦ç‚¹
]

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== ç¼“å­˜ç®¡ç† ====================

def load_cache() -> set:
    """
    ä»history.jsonåŠ è½½å·²å¤„ç†çš„URLç¼“å­˜
    """
    if os.path.exists('history.json'):
        try:
            with open('history.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('processed_urls', []))
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return set()
    else:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ç¼“å­˜æ–‡ä»¶
        save_cache(set())
        return set()

def save_cache(processed_urls: set):
    """
    ä¿å­˜å·²å¤„ç†çš„URLåˆ°history.json
    """
    try:
        with open('history.json', 'w', encoding='utf-8') as f:
            json.dump({'processed_urls': list(processed_urls)}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def send_to_feishu(message: str, max_retries: int = MAX_RETRIES) -> bool:
    """
    ä½¿ç”¨é£ä¹¦webhookå‘é€æ¶ˆæ¯åˆ°ç¾¤ç»„
    
    Args:
        message: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        å‘é€æ˜¯å¦æˆåŠŸ
    """
    # ç›´æ¥ä½¿ç”¨webhookæ–¹å¼å‘é€
    return send_to_feishu_webhook(message, max_retries)


def send_to_feishu_webhook(message: str, max_retries: int = MAX_RETRIES) -> bool:
    """
    ä½¿ç”¨é£ä¹¦webhookå‘é€æ¶ˆæ¯åˆ°ç¾¤ç»„ï¼ˆå¯Œæ–‡æœ¬æ ¼å¼ï¼‰
    
    Args:
        message: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        å‘é€æ˜¯å¦æˆåŠŸ
    """
    # ä»ç¯å¢ƒå˜é‡è·å–webhook URLï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å ä½ç¬¦
    webhook_url = os.environ.get('FEISHU_WEBHOOK_URL', 'YOUR_FEISHU_WEBHOOK_URL_HERE')
    # å‡†å¤‡æ¶ˆæ¯å†…å®¹ï¼ˆè½¬æ¢ä¸ºé€‚åˆå¯Œæ–‡æœ¬çš„æ ¼å¼ï¼‰
    # ç§»é™¤å¯èƒ½å¼•èµ·é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦å’Œæ ¼å¼ï¼Œä¼˜åŒ–æ’ç‰ˆ
    clean_message = message.replace('\ud83d', '').replace('\ude0a', '')  # ç§»é™¤æŸäº›emoji
    clean_message = clean_message.replace('---', '\nâ”€â”€â”€â”€â”€â”€\n')  # åªä¿ç•™ä¸€æ¡ç®€æ´çš„åˆ†éš”çº¿
    clean_message = clean_message.replace('####', '###')  # ç»Ÿä¸€æ ‡é¢˜å±‚çº§
    clean_message = clean_message.replace('###', '\nâ— ')  # å°†ä¸‰çº§æ ‡é¢˜æ”¹ä¸ºåœ†ç‚¹
    clean_message = clean_message.replace('##', '\nâ—† ')  # å°†äºŒçº§æ ‡é¢˜æ”¹ä¸ºè±å½¢ç¬¦å·
    clean_message = clean_message.replace('#', '\nâ˜… ')  # å°†ä¸€çº§æ ‡é¢˜æ”¹ä¸ºæ˜Ÿå·

    # æ„å»ºå¯Œæ–‡æœ¬æ¶ˆæ¯ï¼ˆä½¿ç”¨interactiveç±»å‹å®ç°å¡ç‰‡æ•ˆæœï¼‰
    data = {
        "msg_type": "interactive",
        "card": {
            "config": {
                "wide_screen_mode": True
            },
            "header": {
                "template": "blue",
                "title": {
                    "content": "ğŸŒ å…¨çƒæƒ…æŠ¥ä¸é‡‘èåˆ†ææ—¥æŠ¥",
                    "tag": "plain_text"
                }
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": clean_message
                },
                {
                    "tag": "note",
                    "elements": [
                        {
                            "tag": "plain_text",
                            "content": "ğŸ¤– DeepSeek-V3 æ™ºèƒ½åˆ†æç³»ç»Ÿ | ğŸ“… " + time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                    ]
                }
            ]
        }
    }

    headers = {
        "Content-Type": "application/json; charset=utf-8"
    }

    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨å‘é€æ¶ˆæ¯åˆ°é£ä¹¦webhook (å°è¯• {attempt + 1}/{max_retries})")
            response = requests.post(webhook_url, headers=headers, json=data, timeout=10)
            if response.status_code == 200:
                result = response.json()
                if result.get('StatusCode') == 0 or result.get('code') == 0:
                    logger.info("âœ… æ¶ˆæ¯æˆåŠŸå‘é€åˆ°é£ä¹¦ï¼")
                    return True
                else:
                    logger.error(f"é£ä¹¦webhookè¿”å›é”™è¯¯: {result.get('msg') or result.get('message')}")
            else:
                logger.error(f"HTTPé”™è¯¯: {response.status_code} - {response.text}")
        except Exception as e:
            logger.error(f"å‘é€é£ä¹¦webhookæ¶ˆæ¯å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY)
    
    logger.error("âŒ æ¶ˆæ¯å‘é€æœ€ç»ˆå¤±è´¥")
    return False


# ==================== æ ¸å¿ƒåŠŸèƒ½æ¨¡å— ====================

def fetch_rss_feed(url: str, max_retries: int = MAX_RETRIES) -> Optional[feedparser.FeedParserDict]:
    """
    ä»RSSæºæŠ“å–æ–°é—»ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œä»£ç†æ”¯æŒ
    
    Args:
        url: RSSæºURL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        feedparserè§£æç»“æœæˆ–None
    """
    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨æŠ“å–RSSæº: {url} (å°è¯• {attempt + 1}/{max_retries})")
            
            # ä½¿ç”¨ä»£ç†æŠ“å–RSSï¼ˆå¦‚æœå¯ç”¨ï¼‰
            response = requests.get(
                url, 
                proxies=PROXIES, 
                timeout=10,
                headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            )
            response.raise_for_status()
            
            # è§£æRSS
            feed = feedparser.parse(response.content)
            logger.info(f"æˆåŠŸæŠ“å– {len(feed.entries)} æ¡æ–°é—»")
            return feed
            
        except Exception as e:
            logger.warning(f"æŠ“å–RSSå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(RETRY_DELAY)
            else:
                logger.error(f"RSSæŠ“å–æœ€ç»ˆå¤±è´¥: {url}")
                return None
    
    return None

def extract_news_items() -> List[Dict[str, str]]:
    """
    ä»å¤šä¸ªRSSæºæå–æ–°é—»æ¡ç›®ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«title, summary, link, importance_score
    """
    all_news = []
    seen_titles = set()  # ç”¨äºå»é‡
    processed_urls = load_cache()  # åŠ è½½å·²å¤„ç†çš„URLç¼“å­˜
    
    # å®šä¹‰RSSæºæƒé‡ï¼Œæƒå¨æ€§è¶Šé«˜çš„æºæƒé‡è¶Šå¤§
    source_weights = {
        "https://feeds.bbci.co.uk/news/world/rss.xml": 1.0,  # BBC World
        "https://rss.nytimes.com/services/xml/rss/nyt/World.xml": 1.0,  # NYT World
        "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664": 0.9,  # CNBC Finance
        "https://techcrunch.com/feed/": 0.8,  # TechCrunch AI & Startup
        "https://finance.yahoo.com/news/rssindex": 0.8,  # Yahoo Finance
        "https://www.coindesk.com/arc/outboundfeeds/rss/": 0.7,  # CoinDesk
        "https://oilprice.com/rss/main": 0.7,  # OilPrice.com
        "https://news.ycombinator.com/rss": 0.7,  # Hacker News
        "https://www.reddit.com/r/worldnews/top/.rss?t=day": 0.6,  # Reddit WorldNews
        "https://www.reddit.com/r/videos/top/.rss?t=day": 0.5,  # Reddit è§†é¢‘èšåˆ
        "https://www.scmp.com/rss/2/feed": 0.8,  # South China Morning Post
        "http://arxiv.org/rss/cs.AI": 0.6,  # ArXiv AI Paper Daily
        "http://news.baidu.com/n?cmd=file&format=rss&tn=rss&sub=0": 0.7,  # ç™¾åº¦æ–°é—»
        "http://rss.people.com.cn/GB/303140/index.xml": 0.9,  # äººæ°‘ç½‘
        "http://www.xinhuanet.com/politics/news_politics.xml": 0.9,  # æ–°åç½‘ - æ—¶æ”¿
        "http://www.chinanews.com/rss/scroll-news.xml": 0.7,  # ä¸­å›½æ–°é—»ç½‘
        "https://www.thepaper.cn/rss.jsp": 0.6,  # æ¾æ¹ƒæ–°é—»
        "https://www.cls.cn/v3/highlights?app_id=70301d300f0f95a1&platform=pc": 0.7,  # è´¢è”ç¤¾
        "https://www.zhihu.com/rss": 0.5,  # çŸ¥ä¹æ¯æ—¥ç²¾é€‰
        "https://www.36kr.com/feed": 0.6,  # 36æ°ª
        "https://news.qq.com/rss/channels/finance/rss.xml": 0.7,  # è…¾è®¯è´¢ç»
        "https://rss.sina.com.cn/news/china/focus15.xml": 0.7,  # æ–°æµªæ–°é—»-å›½å†…ç„¦ç‚¹
    }
    
    for rss_url in RSS_SOURCES:
        feed = fetch_rss_feed(rss_url)
        if not feed or not feed.entries:
            continue
            
        for entry in feed.entries[:5]:  # æ¯ä¸ªæºæœ€å¤šå–5æ¡
            title = entry.get('title', '').strip()
            summary = entry.get('summary', '').strip()
            link = entry.get('link', '')
            published_time = entry.get('published_parsed', None)
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡
            if link in processed_urls:
                continue
            
            # å»é‡æ£€æŸ¥
            if not title or title in seen_titles:
                continue
                
            seen_titles.add(title)
            
            # æ¸…ç†summaryä¸­çš„HTMLæ ‡ç­¾
            import re
            summary = re.sub(r'<[^>]+>', '', summary)
            summary = summary[:200] + '...' if len(summary) > 200 else summary
            
            # è®¡ç®—æ–°é—»é‡è¦æ€§åˆ†æ•°
            importance_score = calculate_importance_score(title, summary, rss_url, published_time, source_weights)
            
            all_news.append({
                'title': title,
                'summary': summary,
                'link': link,
                'importance_score': importance_score
            })
    
    # æŒ‰é‡è¦æ€§åˆ†æ•°é™åºæ’åº
    all_news.sort(key=lambda x: x['importance_score'], reverse=True)
    
    # æ›´æ–°ç¼“å­˜ï¼Œæ·»åŠ æ–°å¤„ç†çš„URL
    for item in all_news:
        processed_urls.add(item['link'])
    save_cache(processed_urls)
    
    logger.info(f"æ€»å…±æå–åˆ° {len(all_news)} æ¡å”¯ä¸€æ–°é—»ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº")
    return all_news[:10]  # æœ€å¤šå¤„ç†10æ¡æ–°é—»

def get_asset_price(asset_name: str) -> Optional[str]:
    """
    è·å–æŒ‡å®šèµ„äº§çš„å®æ—¶ä»·æ ¼ï¼ˆä½¿ç”¨å…è´¹APIï¼‰
    æ”¯æŒæ¯”ç‰¹å¸ã€é»„é‡‘ã€è‹±ä¼Ÿè¾¾è‚¡ç¥¨ç­‰
    """
    try:
        # æ ¹æ®èµ„äº§åç§°é€‰æ‹©ä¸åŒçš„API
        if asset_name.lower() in ['bitcoin', 'btc', 'æ¯”ç‰¹å¸']:
            # ä½¿ç”¨CoinGecko APIè·å–æ¯”ç‰¹å¸ä»·æ ¼
            response = requests.get("https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['bitcoin']['usd']
                return f"${price:,}"
        elif asset_name.lower() in ['ethereum', 'eth', 'ä»¥å¤ªåŠ']:
            # ä½¿ç”¨CoinGecko APIè·å–ä»¥å¤ªåŠä»·æ ¼
            response = requests.get("https://api.coingecko.com/api/v3/simple/price?ids=ethereum&vs_currencies=usd", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['ethereum']['usd']
                return f"${price:,}"
        elif asset_name.lower() in ['gold', 'é»„é‡‘']:
            # ä½¿ç”¨è´µé‡‘å±APIè·å–é»„é‡‘ä»·æ ¼ï¼ˆUSD/ç›å¸ï¼‰
            response = requests.get("https://api.metals.live/v1/spot/gold", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['value']
                return f"${price:.2f}/oz"
        elif asset_name.lower() in ['nvidia', 'nvda', 'è‹±ä¼Ÿè¾¾']:
            # ä½¿ç”¨Yahoo Finance APIè·å–è‹±ä¼Ÿè¾¾è‚¡ç¥¨ä»·æ ¼
            response = requests.get(f"https://query1.finance.yahoo.com/v8/finance/chart/NVDA", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['chart']['result'][0]['meta']['regularMarketPrice']
                return f"${price:.2f}"
        elif asset_name.lower() in ['apple', 'aapl', 'è‹¹æœ']:
            # ä½¿ç”¨Yahoo Finance APIè·å–è‹¹æœè‚¡ç¥¨ä»·æ ¼
            response = requests.get(f"https://query1.finance.yahoo.com/v8/finance/chart/AAPL", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['chart']['result'][0]['meta']['regularMarketPrice']
                return f"${price:.2f}"
        elif asset_name.lower() in ['s&p 500', 'sp500', 'æ ‡æ™®500']:
            # ä½¿ç”¨Yahoo Finance APIè·å–æ ‡æ™®500ä»·æ ¼
            response = requests.get(f"https://query1.finance.yahoo.com/v8/finance/chart/SPY", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['chart']['result'][0]['meta']['regularMarketPrice']
                return f"${price:.2f}"
    except Exception as e:
        logger.warning(f"è·å–{asset_name}ä»·æ ¼å¤±è´¥: {e}")
        return None

def analyze_news_with_llm(news_items: List[Dict[str, str]], report_type: str = 'daily') -> str:
    """
    è°ƒç”¨LLM APIå¯¹æ–°é—»è¿›è¡Œæ·±åº¦åˆ†æï¼ˆå¸¦å»é‡é€»è¾‘ã€æƒ…ç»ªè¯„åˆ†ã€ä»·æ ¼æ³¨å…¥å’Œå›¾ç‰‡ä¿¡æ¯ï¼‰
    
    Args:
        news_items: æ–°é—»æ¡ç›®åˆ—è¡¨
        report_type: æŠ¥å‘Šç±»å‹ ('morning', 'noon', 'evening', 'summary', 'daily')
        
    Returns:
        LLMç”Ÿæˆçš„Markdownæ ¼å¼åˆ†ææŠ¥å‘Š
    """
    if not news_items:
        return "ä»Šæ—¥æ— é‡è¦æ–°é—»æ›´æ–°ã€‚"
    
    # æ„å»ºæ–°é—»å†…å®¹ï¼Œä¸€æ¬¡æ€§å‘é€ç»™LLMè¿›è¡Œå»é‡åˆ†æ
    news_content = ""
    for i, item in enumerate(news_items):
        # æ£€æŸ¥æ–°é—»ä¸­æ˜¯å¦åŒ…å«éœ€è¦ä»·æ ¼æ³¨å…¥çš„å…³é”®è¯
        title_lower = item['title'].lower()
        summary_lower = item['summary'].lower()
        price_info = ""
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³èµ„äº§å…³é”®è¯
        assets_to_check = ['bitcoin', 'btc', 'ethereum', 'eth', 'gold', 'nvidia', 'nvda', 'apple', 'aapl', 's&p 500', 'sp500']
        for asset in assets_to_check:
            if asset in title_lower or asset in summary_lower:
                price = get_asset_price(asset)
                if price:
                    price_info = f" (å½“å‰ä»·æ ¼ï¼š{price})"
                break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±åœæ­¢
        
        news_content += f"**ID**: {i+1}\n**æ ‡é¢˜**: {item['title']}{price_info}\n**æ‘˜è¦**: {item['summary']}\n**é“¾æ¥**: {item['link']}\n\n"
    
    # æ ¹æ®æŠ¥å‘Šç±»å‹ç”Ÿæˆå®šåˆ¶åŒ–çš„ç³»ç»Ÿæç¤ºè¯
    SYSTEM_PROMPT = """# Role
ä½ æ˜¯ç”±é«˜ç››å…¨çƒå®è§‚ç»„ä¸é¡¶çº§æ¸¸èµ„æ“ç›˜æ‰‹è”åˆè®­ç»ƒçš„é¦–å¸­ç­–ç•¥åˆ†æå¸ˆã€‚
ä½ çš„æœåŠ¡å¯¹è±¡ï¼šèº«åœ¨ä¸­å›½çš„èµ„æ·±æ‰“å·¥äºº/ä¸ªä½“åˆ›ä¸šè€…ï¼Œæå…¶åŒæ¶è¢«ä¸»æµåª’ä½“å¿½æ‚ ã€‚
ä»»åŠ¡ï¼š**é€è§†æ–°é—»è¡¨è±¡ï¼Œæ‹†è§£åˆ©å°±ç›Šé“¾æ¡ï¼Œç»™å‡ºå†·è¡€åˆ¤æ–­ã€‚**

# Constraints
1. **æåº¦ç²¾ç®€**ï¼šå…¨ç¯‡ä¸¥æ ¼æ§åˆ¶åœ¨ 300 å­—ä»¥å†…ï¼Œç”µæŠ¥é£æ ¼ã€‚
2. **ä¸¥ç¦åºŸè¯**ï¼šåªè¦ç»“è®ºï¼Œä¸è¦èƒŒæ™¯ã€‚
3. **æ—¶é—´ç²¾ç¡®**ï¼šå¿…é¡»è¾“å‡º **åŒ—äº¬æ—¶é—´ (YYYY-MM-DD HH:mm)**ã€‚
4. **ä¸¥ç¦åˆ†å‰²çº¿**ï¼šä¸è¦è¾“å‡ºä»»ä½• "---" æˆ–æ¨ªçº¿ã€‚
5. **ä¸¥ç¦æè¿°å›¾ç‰‡**ï¼šç»å¯¹ä¸è¦è¾“å‡ºä»»ä½•å›¾ç‰‡æè¿°æ–‡å­—ã€‚
6. **é˜´è°‹è®ºè§†è§’**ï¼šé»˜è®¤å¸‚åœºæ˜¯æ®‹é…·çš„ï¼Œæ–°é—»æ˜¯èµ„æœ¬çš„å·¥å…·ã€‚

# Analysis Framework (Markdown Output)
è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

### [æƒ…ç»ªåˆ† | åˆ†æ•°] æ–°é—»æ ‡é¢˜ (ä¸­æ–‡ï¼ŒåŠ ç²—)

> [ğŸ”— ç›´è¾¾åŸæ–°é—»](æ–°é—»URL) | æ¥æºï¼šæ–°é—»Source

* **â° å‘å¸ƒæ—¶é—´**ï¼šYYYY-MM-DD HH:mm (åŒ—äº¬æ—¶é—´)
* **ğŸ“ æ ¸å¿ƒäº‹å®**ï¼šä¸€å¥è¯æ¦‚æ‹¬ (Who + What)ã€‚
* **ğŸ§  åº•å±‚é€»è¾‘**ï¼šåº„å®¶çœŸå®æ„å›¾ä¸èµ„é‡‘ä¼ å¯¼ (ç”¨ `->` è¡¨ç¤º)ã€‚
* **ğŸ‡¨ğŸ‡³ ä¸­å›½å½±å“**ï¼š
    * **âš¡ çŸ­æœŸ**ï¼šå¯¹æ±‡ç‡/æƒ…ç»ª/å…·ä½“è¡Œä¸šçš„ç›´æ¥å†²å‡»ã€‚
    * **â³ é•¿æœŸ**ï¼šæ˜¯å¦æ”¹å˜å›½è¿æˆ–æ‰“å·¥äººç”Ÿå­˜ç¯å¢ƒï¼Ÿ
* **ğŸ“‰ è‚¡å¸‚é’±åŒ…**ï¼š
    * **åˆ©å¥½**ï¼š[ä»£ç /æ¿å—]
    * **åˆ©ç©º**ï¼š[ä»£ç /æ¿å—]
* **ğŸ›‘ æ“ä½œå»ºè®®**ï¼š[ç©ºä»“/æ­¢ç›ˆ/æŠ„åº•/è§‚æœ›] + ä¸€å¥è¯å…·ä½“ç†ç”±ï¼ˆæ‹’ç»æ¨¡æ£±ä¸¤å¯ï¼‰ã€‚"""

    system_prompt = SYSTEM_PROMPT

    # ç”¨æˆ·æ¶ˆæ¯
    user_message = f"è¯·åˆ†æä»¥ä¸‹æ–°é—»ï¼ˆå…±{min(len(news_items), 10)}æ¡ï¼‰ï¼Œå¹¶å¯¹é‡å¤è¯é¢˜è¿›è¡Œåˆå¹¶ï¼Œä¸ºæ¯æ¡æ–°é—»æ·»åŠ æƒ…ç»ªè¯„åˆ†å’Œä»·æ ¼ä¿¡æ¯ï¼š\n\n{news_content}"
    
    # è°ƒç”¨DeepSeek APIï¼ˆä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼ï¼‰
    headers = {
        'Authorization': f'Bearer {API_KEY}',
        'Content-Type': 'application/json'
    }
    
    payload = {
        "model": "deepseek-chat",  # DeepSeek V3.2çš„æ¨¡å‹åç§°
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ],
        "temperature": 0.7,
        "max_tokens": 4000  # å¢åŠ tokené™åˆ¶ä»¥å¤„ç†å¤šæ¡æ–°é—»
    }
    
    for attempt in range(MAX_RETRIES):
        try:
            logger.info(f"æ­£åœ¨è°ƒç”¨DeepSeek APIè¿›è¡Œæ–°é—»åˆ†æ (å°è¯• {attempt + 1}/{MAX_RETRIES})")
            
            # DeepSeek APIåœ¨ä¸­å›½å¢ƒå†…ï¼Œä¸éœ€è¦ä»£ç†
            response = requests.post(
                f"{BASE_URL}/chat/completions",
                headers=headers,
                json=payload,
                proxies=None,  # ä¸ä½¿ç”¨ä»£ç†è®¿é—®DeepSeek API
                timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
            )
            
            if response.status_code == 200:
                result = response.json()
                analysis = result['choices'][0]['message']['content']
                logger.info("LLMåˆ†æå®Œæˆ")
                return analysis
            else:
                logger.warning(f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.warning(f"LLM APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
            
        if attempt < MAX_RETRIES - 1:
            time.sleep(RETRY_DELAY)
    
    # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬
    logger.error("LLMåˆ†æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬")
    fallback_analysis = ""
    for i, item in enumerate(news_items[:3], 1):
        fallback_analysis += f"### {i}. [ç‚¹å‡»ç›´è¾¾ï¼š{item['title']}]({item['link']})\n"
        fallback_analysis += "- **ğŸ“… æ¥æº**: å›½é™…åª’ä½“\n"
        fallback_analysis += f"- **ğŸ“ æ ¸å¿ƒäº‹å®**: {item['summary'][:30]}...\n\n"
        fallback_analysis += "#### ğŸ“Š æ·±åº¦ç ”æŠ¥\n"
        fallback_analysis += "* **ğŸ‡¨ğŸ‡³ å¯¹ä¸­å›½çŸ­æœŸå½±å“**: å¾…åˆ†æ\n"
        fallback_analysis += "* **ğŸ”® å¯¹ä¸­å›½é•¿æœŸå½±å“**: å¾…åˆ†æ\n"
        fallback_analysis += "* **ğŸ“ˆ è‚¡å¸‚å½±å“ (Aè‚¡/æ¸¯è‚¡/ç¾è‚¡)**:\n"
        fallback_analysis += "    * *åˆ©å¥½/åˆ©ç©ºæ¿å—*: å¾…åˆ†æ\n"
        fallback_analysis += "    * *åº•å±‚é€»è¾‘*: å¾…åˆ†æ\n\n"
        fallback_analysis += "---\n"
    
    return fallback_analysis

def calculate_importance_score(title: str, summary: str, source_url: str, published_time, source_weights: dict) -> float:
    """
    è®¡ç®—æ–°é—»é‡è¦æ€§åˆ†æ•°
    
    Args:
        title: æ–°é—»æ ‡é¢˜
        summary: æ–°é—»æ‘˜è¦
        source_url: æ–°é—»æ¥æºURL
        published_time: å‘å¸ƒæ—¶é—´
        source_weights: æ¥æºæƒé‡å­—å…¸
    
    Returns:
        é‡è¦æ€§åˆ†æ•° (0-10)
    """
    score = 0.0
    
    # 1. æ¥æºæƒé‡ (åŸºç¡€åˆ†æ•°)
    base_weight = source_weights.get(source_url, 0.5)  # é»˜è®¤æƒé‡0.5
    score += base_weight * 4  # æƒé‡å æ¯”40%
    
    # 2. æ ‡é¢˜å…³é”®è¯åˆ†æ (æ—¶æ•ˆæ€§ã€ç´§æ€¥æ€§ã€å½±å“åŠ›)
    title_lower = title.lower()
    urgency_keywords = ['çªå‘', 'ç´§æ€¥', 'è­¦å‘Š', 'å±æœº', 'æš´è·Œ', 'æš´æ¶¨', 'æˆ˜', 'å†²çª', 'åˆ¶è£', 'æ”¿ç­–', 'å¤®è¡Œ', 'åˆ©ç‡', 'gdp', 'å°±ä¸š', 'é€šèƒ€']
    financial_keywords = ['ç»æµ', 'è‚¡å¸‚', 'åŸºé‡‘', 'å€ºåˆ¸', 'ç¾å…ƒ', 'äººæ°‘å¸', 'é»„é‡‘', 'çŸ³æ²¹', 'æ¯”ç‰¹å¸', 'ai', 'ç§‘æŠ€', 'å…¬å¸', 'è´¢æŠ¥']
    china_keywords = ['ä¸­å›½', 'chinese', 'beijing', 'shanghai', 'hk', 'æ¸¯', 'aè‚¡', 'äººæ°‘å¸', 'cny', 'è´¸æ˜“', 'ä¸­ç¾', 'ç¾ä¸­']
    
    # æ£€æŸ¥ç´§æ€¥å…³é”®è¯
    for keyword in urgency_keywords:
        if keyword in title_lower:
            score += 1.0  # æ¯ä¸ªç´§æ€¥å…³é”®è¯+1åˆ†
    
    # æ£€æŸ¥é‡‘èå…³é”®è¯
    for keyword in financial_keywords:
        if keyword in title_lower:
            score += 0.5  # æ¯ä¸ªé‡‘èå…³é”®è¯+0.5åˆ†
    
    # æ£€æŸ¥ä¸­å›½ç›¸å…³å…³é”®è¯
    for keyword in china_keywords:
        if keyword in title_lower:
            score += 1.0  # æ¯ä¸ªä¸­å›½ç›¸å…³å…³é”®è¯+1åˆ†
    
    # 3. å†…å®¹é•¿åº¦ (æ›´é•¿çš„å†…å®¹å¯èƒ½æ›´é‡è¦)
    content_length = len(title) + len(summary)
    if content_length > 200:
        score += 1.0
    elif content_length > 100:
        score += 0.5
    
    # 4. æ—¶é—´å› ç´  (å¦‚æœæ˜¯ä»Šå¤©å‘å¸ƒçš„æ–°é—»ï¼Œå¢åŠ åˆ†æ•°)
    import datetime
    if published_time:
        pub_date = datetime.datetime(*published_time[:6])
        now = datetime.datetime.now()
        hours_diff = (now - pub_date).total_seconds() / 3600
        if hours_diff <= 24:  # 24å°æ—¶å†…å‘å¸ƒçš„æ–°é—»
            score += 1.0
        elif hours_diff <= 48:  # 48å°æ—¶å†…å‘å¸ƒçš„æ–°é—»
            score += 0.5
    
    # 5. æ ‡é¢˜é•¿åº¦å’Œç‰¹å¾ (æ ‡é¢˜é•¿åº¦é€‚ä¸­ä¸”åŒ…å«æ•°å­—æˆ–ç¬¦å·å¯èƒ½æ›´é‡è¦)
    if 30 < len(title) < 100:  # æ ‡é¢˜é•¿åº¦é€‚ä¸­
        score += 0.5
    if ':' in title or '-' in title:  # åŒ…å«åˆ†éš”ç¬¦
        score += 0.3
    if any(char.isdigit() for char in title):  # åŒ…å«æ•°å­—
        score += 0.2
    
    return min(score, 10.0)  # é™åˆ¶æœ€å¤§åˆ†æ•°ä¸º10

def parse_sentiment_score(message: str) -> float:
    """
    ä»æ¶ˆæ¯ä¸­è§£ææ•´ä½“æƒ…ç»ªå¾—åˆ†
    """
    import re
    # æŸ¥æ‰¾ç±»ä¼¼ [ğŸ”¥+8] æˆ– [â„ï¸-8] çš„æ¨¡å¼
    pattern = r'\[(?:ğŸ”¥|â„ï¸|âš¡|ğŸ“‰|ğŸ“ˆ)\s*([+-]?\d+)\]'
    matches = re.findall(pattern, message)
    if matches:
        scores = [int(score) for score in matches]
        # è¿”å›å¹³å‡å€¼ä½œä¸ºæ•´ä½“æƒ…ç»ªå¾—åˆ†
        return sum(scores) / len(scores) if scores else 0
    return 0

def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„æ–°é—»åˆ†ææµç¨‹
    """
    logger.info("ğŸš€ å¯åŠ¨æ¯æ—¥æ–°é—»æœºå™¨äºº...")
    try:
        # 1. æŠ“å–æ–°é—»
        news_items = extract_news_items()
        if not news_items:
            logger.warning("æœªè·å–åˆ°ä»»ä½•æ–°é—»ï¼Œè·³è¿‡åˆ†æ")
            return
        
        # 2. LLMæ·±åº¦åˆ†æ
        analysis_result = analyze_news_with_llm(news_items)
        
        # 3. å‘é€åˆ°é£ä¹¦
        success = send_to_feishu(analysis_result)
        
        if success:
            logger.info("ğŸ‰ æ¯æ—¥æ–°é—»åˆ†æä»»åŠ¡å®Œæˆï¼")
        else:
            logger.error("âŒ æ¯æ—¥æ–°é—»åˆ†æä»»åŠ¡å¤±è´¥")
            send_error_alert("æ—¥æŠ¥å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é£ä¹¦åº”ç”¨é…ç½®")
            
    except Exception as e:
        logger.exception(f"ä¸»å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}")
        send_error_alert(f"æœºå™¨äººæ•…éšœï¼š{str(e)}ï¼Œè¯·ä¸»äººæ£€æŸ¥ï¼")

def send_error_alert(error_message: str, max_retries: int = MAX_RETRIES):
    """
    å‘é€é”™è¯¯è­¦æŠ¥åˆ°é£ä¹¦ï¼ˆä½¿ç”¨webhookæ–¹å¼ï¼‰
    """
    # æ„å»ºé”™è¯¯è­¦æŠ¥æ¶ˆæ¯
    alert_msg = f"ğŸš¨ æœºå™¨äººæ•…éšœè­¦æŠ¥\n\né”™è¯¯è¯¦æƒ…ï¼š{error_message}\n\nè¯·åŠæ—¶æ£€æŸ¥æœºå™¨äººçŠ¶æ€ï¼\n\nDeepSeek-V3 ç›‘æ§ç³»ç»Ÿ"
    
    # ä½¿ç”¨webhookå‘é€é”™è¯¯è­¦æŠ¥
    return send_to_feishu_webhook(alert_msg, max_retries)

# ==================== ä¸»å‡½æ•° ====================

if __name__ == "__main__":
    main()
