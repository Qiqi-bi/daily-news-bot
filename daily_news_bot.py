#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Daily News Bot - å…¨çƒæƒ…æŠ¥ä¸é‡‘èåˆ†æè‡ªåŠ¨åŒ–è„šæœ¬

åŠŸèƒ½ï¼š
1. ä»å›½é™…ä¸»æµRSSæºæŠ“å–æ–°é—»ï¼ˆé€šè¿‡ä»£ç†ï¼‰
2. è°ƒç”¨DeepSeekå¤§æ¨¡å‹APIè¿›è¡Œæ·±åº¦åˆ†æ
3. å‘é€åˆ°é£ä¹¦ç¾¤ï¼ˆä½¿ç”¨webhookæ–¹å¼ï¼‰

ä½œè€…ï¼šPythoné«˜çº§å·¥ç¨‹å¸ˆ
"""

import feedparser
import requests
import json
import time
import logging
import random
from typing import List, Dict, Optional
from urllib.parse import urljoin
import os
import ssl
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3
from fake_useragent import UserAgent

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==================== é…ç½®åŒºåŸŸ ====================
# è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ä»¥ä¸‹é…ç½®

# LLM API é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
API_KEY = os.environ.get('DEEPSEEK_API_KEY', '')
BASE_URL = "https://api.deepseek.com"

# æ™ºèƒ½ä»£ç†é…ç½® - æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­
if os.environ.get('GITHUB_ACTIONS'):
    # åœ¨GitHub Actionsä¸­ï¼Œç›´æ¥è¿æ¥
    PROXIES = None
else:
    # æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨ä»£ç†
    PROXIES = {
        'http': 'http://127.0.0.1:7897',
        'https': 'http://127.0.0.1:7897'
    }

# RSSæºåˆ—è¡¨ï¼ˆç»ˆæç‰ˆæœ¬ï¼‰
RSS_SOURCES = [
    # --- 1. å›½é™…é¡¶æµ (BBC/NYT) ---
    "https://feeds.bbci.co.uk/news/world/rss.xml",  # BBC World
    "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",  # NYT World
    
    # --- 2. åå°”è¡—/é‡‘è (CNBC) ---
    "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664",  # CNBC Finance

    # --- 3. ç¡…è°·/ç§‘æŠ€ (TechCrunch) ---
    "https://techcrunch.com/feed/",  # TechCrunch AI & Startup

    # --- 4. é›…è™è´¢ç» (æ–°å¢) ---
    "https://finance.yahoo.com/news/rssindex",  # Yahoo Finance

    # --- 5. åŠ å¯†è´§å¸ (Crypto) ---
    "https://www.coindesk.com/arc/outboundfeeds/rss/",  # CoinDesk
    "https://cointelegraph.com/rss",  # Cointelegraph
    "https://crypto-slate.com/feed/",  # Crypto Slate

    # --- 6. èƒ½æºä¸æˆ˜äº‰ (Energy) ---
    "https://oilprice.com/rss/main",  # OilPrice.com

    # --- 7. ç¤¾äº¤ä¸é»‘å®¢åŠ¨å‘ (æ›¿ä»£ Twitter/GitHub) ---
    # Hacker News (å…¨çƒæå®¢éƒ½åœ¨è®¨è®ºä»€ä¹ˆï¼Œæ˜¯ GitHub æœ€å¥½çš„é£å‘æ ‡)
    "https://news.ycombinator.com/rss",
    # Reddit WorldNews (å…¨çƒç½‘æ°‘æœ€çƒ­è®®çš„çªå‘äº‹ä»¶)
    "https://www.reddit.com/r/worldnews/top/.rss?t=day",
    
    # --- 8. Reddit è§†é¢‘èšåˆ (æ–°å¢) ---
    "https://www.reddit.com/r/videos/top/.rss?t=day",  # Reddit è§†é¢‘èšåˆ - å…¨çƒ24å°æ—¶å†…æœ€çƒ­é—¨çš„è§†é¢‘é›†åˆ
    
    # --- 9. äºšæ´²/ä¸­å›½å•†ä¸š (æ–°å¢) ---
    "https://www.scmp.com/rss/2/feed",  # South China Morning Post (å—åæ—©æŠ¥ - ä¸­å›½å•†ä¸šç‰ˆå—)
    
    # --- 10. å­¦æœ¯/AIç ”ç©¶ (æ–°å¢) ---
    "http://arxiv.org/rss/cs.AI",  # ArXiv AI Paper Daily (å­¦æœ¯æº)
    "https://mittechnologyreview.com/feed/",  # MIT Technology Review (ç§‘æŠ€è¶‹åŠ¿åˆ†æ)
    
    # --- 11. å›½å†…ä¸»æµæ–°é—»æº (æ–°å¢) ---
    "http://news.baidu.com/n?cmd=file&format=rss&tn=rss&sub=0",  # ç™¾åº¦æ–°é—»
    "http://rss.people.com.cn/GB/303140/index.xml",  # äººæ°‘ç½‘
    "http://www.xinhuanet.com/politics/news_politics.xml",  # æ–°åç½‘ - æ—¶æ”¿
    "http://www.chinanews.com/rss/scroll-news.xml",  # ä¸­å›½æ–°é—»ç½‘
    "https://www.thepaper.cn/rss.jsp",  # æ¾æ¹ƒæ–°é—»
    "http://www.ce.cn/cysc/jg/zxbd/rss2.xml",  # ä¸­å›½ç»æµç½‘
    "https://www.cls.cn/v3/highlights?app_id=70301d300f0f95a1&platform=pc",  # è´¢è”ç¤¾ (éœ€è¦é€‚é…)
    
    # --- 12. å›½å†…ç§‘æŠ€æ–°é—» (æ–°å¢) ---
    "https://www.zhihu.com/rss",  # çŸ¥ä¹æ¯æ—¥ç²¾é€‰
    "https://www.36kr.com/feed",  # 36æ°ª
    "https://news.qq.com/rss/channels/finance/rss.xml",  # è…¾è®¯è´¢ç»
    "https://rss.sina.com.cn/news/china/focus15.xml",  # æ–°æµªæ–°é—»-å›½å†…ç„¦ç‚¹

    # --- 13. ä¸»è¦ç§‘æŠ€å…¬å¸å®˜ç½‘ (æ–°å¢) ---
    "https://blog.google/rss/",  # Google Blog
    "https://openai.com/blog/rss/",  # OpenAI Blog
    "https://blogs.microsoft.com/feed/",  # Microsoft Blog
    "https://www.apple.com/newsroom/rss-feed.rss",  # Apple Newsroom
    "https://nvidianews.nvidia.com/rss.xml",  # NVIDIA Newsroom
    "https://about.meta.com/rss/feed/",  # Meta Newsroom

    # --- 14. ä¸»æµè´¢ç»åª’ä½“ (æ–°å¢) ---
    "https://feeds.reuters.com/reuters/topNews",  # Reuters Top News
    "https://feeds.reuters.com/reuters/businessNews",  # Reuters Business
    "https://feeds.reuters.com/reuters/technologyNews",  # Reuters Technology
    "https://bloomberg.com/feed",  # Bloomberg (å¯èƒ½éœ€è¦é€‚é…)
    "https://www.wsj.com/xml/rss/3_7085.xml",  # Wall Street Journal (å¯èƒ½éœ€è¦é€‚é…)

    # --- 15. ç§‘æŠ€åª’ä½“ (æ–°å¢) ---
    "https://www.theverge.com/rss/index.xml",  # The Verge
    "https://arstechnica.com/feed/",  # Ars Technica

    # --- 16. æŠ•èµ„æœºæ„å’Œæ•°æ®åº“ (æ–°å¢) ---
    "https://www.cbinsights.com/blog/feed/",  # CB Insights
    "https://techcrunch.com/startups/",  # TechCrunch Startups
    "https://www.crunchbase.com/feed",  # Crunchbase (å¯èƒ½éœ€è¦é€‚é…)

    # --- 17. AIç ”ç©¶æœºæ„ (æ–°å¢) ---
    "https://stability.ai/rss",  # Stability AI
    "https://huggingface.co/blog/feed.xml",  # Hugging Face Blog

    # --- 18. å•†ä¸šé¢†è¢–å’Œä¼ä¸šé«˜ç®¡ (æ–°å¢) ---
    "https://www.tesla.com/blog/rss",  # Tesla Blog
    "https://about.twitter.com/content/dam/about-twitter/company/news/rss-feeds/official-company-blog-rss.xml",  # Twitter Blog (X)
    "https://www.spacex.com/static/releases/feed.xml",  # SpaceX Releases

    # --- 19. åŠ å¯†è´§å¸å’ŒåŒºå—é“¾ (æ–°å¢) ---
    "https://cointelegraph.com/feed",  # Cointelegraph
    "https://decrypt.co/feed",  # Decrypt
    "https://messari.io/feed.xml",  # Messari
    "https://theblock.co/rss",  # The Block

    # --- 20. äº¤æ˜“å’ŒæŠ•èµ„å¹³å° (æ–°å¢) ---
    "https://www.binance.com/en/blog/rss",  # Binance Blog
    "https://blog.coinbase.com/feed",  # Coinbase Blog

    # --- 21. åŒºå—é“¾åè®® (æ–°å¢) ---
    "https://blog.ethereum.org/feed.xml",  # Ethereum Blog
    "https://polkadot.network/feed/",  # Polkadot Blog

    # --- 22. é‡‘èå’ŒæŠ•èµ„ (æ–°å¢) ---
    "https://seekingalpha.com/feed.xml",  # Seeking Alpha
    "https://www.ft.com/?format=rss",  # Financial Times (å¯èƒ½éœ€è¦é€‚é…)

    # --- 23. äºšé©¬é€Šç›¸å…³ (æ–°å¢) ---
    "https://www.aboutamazon.com/news/rss-feed.xml",  # Amazon Newsroom

    # --- 24. é©¬æ–¯å…‹ç›¸å…³ (æ–°å¢) ---
    "https://www.neuralink.com/blog.rss",  # Neuralink Blog
    "https://www.boringcompany.com/blog",  # The Boring Company Blog (å¯èƒ½éœ€è¦é€‚é…)

    # --- 25. å…¶ä»–AIå…¬å¸ (æ–°å¢) ---
    "https://www.anthropic.com/rss",  # Anthropic Blog
    "https://deepmind.google/rss/",  # DeepMind Blog
    "https://aws.amazon.com/blogs/aws/feed/",  # AWS Blog
    "https://www.amd.com/en/press-room/press-releases.rss",  # AMD Press Releases
]

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== ç¼“å­˜ç®¡ç† ====================

def load_cache() -> set:
    """
    ä»history.jsonåŠ è½½å·²å¤„ç†çš„URLç¼“å­˜
    """
    if os.path.exists('history.json'):
        try:
            with open('history.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('processed_urls', []))
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return set()
    else:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ç¼“å­˜æ–‡ä»¶
        save_cache(set())
        return set()

def save_cache(processed_urls: set):
    """
    ä¿å­˜å·²å¤„ç†çš„URLåˆ°history.json
    """
    try:
        with open('history.json', 'w', encoding='utf-8') as f:
            json.dump({'processed_urls': list(processed_urls)}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def send_to_feishu(message: str, max_retries: int = MAX_RETRIES) -> bool:
    """
    ä½¿ç”¨é£ä¹¦webhookå‘é€æ¶ˆæ¯åˆ°ç¾¤ç»„
    
    Args:
        message: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        å‘é€æ˜¯å¦æˆåŠŸ
    """
    # ç›´æ¥ä½¿ç”¨webhookæ–¹å¼å‘é€
    return send_to_feishu_webhook(message, max_retries)


def send_to_feishu_webhook(message: str, max_retries: int = MAX_RETRIES) -> bool:
    """
    ä½¿ç”¨é£ä¹¦webhookå‘é€æ¶ˆæ¯åˆ°ç¾¤ç»„ï¼ˆå¯Œæ–‡æœ¬æ ¼å¼ï¼‰
    
    Args:
        message: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        å‘é€æ˜¯å¦æˆåŠŸ
    """
    # ä»ç¯å¢ƒå˜é‡è·å–webhook URLï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å ä½ç¬¦
    webhook_url = os.environ.get('FEISHU_WEBHOOK_URL', '')
    # å‡†å¤‡æ¶ˆæ¯å†…å®¹ï¼ˆè½¬æ¢ä¸ºé€‚åˆå¯Œæ–‡æœ¬çš„æ ¼å¼ï¼‰
    # ç§»é™¤å¯èƒ½å¼•èµ·é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦å’Œæ ¼å¼ï¼Œä¼˜åŒ–æ’ç‰ˆ
    clean_message = message.replace('\ud83d', '').replace('\ude0a', '')  # ç§»é™¤æŸäº›emoji
    clean_message = clean_message.replace('---', '\nâ”€â”€â”€â”€â”€â”€\n')  # åªä¿ç•™ä¸€æ¡ç®€æ´çš„åˆ†éš”çº¿
    clean_message = clean_message.replace('####', '###')  # ç»Ÿä¸€æ ‡é¢˜å±‚çº§
    clean_message = clean_message.replace('###', '\nâ— ')  # å°†ä¸‰çº§æ ‡é¢˜æ”¹ä¸ºåœ†ç‚¹
    clean_message = clean_message.replace('##', '\nâ—† ')  # å°†äºŒçº§æ ‡é¢˜æ”¹ä¸ºè±å½¢ç¬¦å·
    clean_message = clean_message.replace('#', '\nâ˜… ')  # å°†ä¸€çº§æ ‡é¢˜æ”¹ä¸ºæ˜Ÿå·

    # æ„å»ºå¯Œæ–‡æœ¬æ¶ˆæ¯ï¼ˆä½¿ç”¨interactiveç±»å‹å®ç°å¡ç‰‡æ•ˆæœï¼‰
    data = {
        "msg_type": "interactive",
        "card": {
            "config": {
                "wide_screen_mode": True
            },
            "header": {
                "template": "blue",
                "title": {
                    "content": "ğŸŒ å…¨çƒæƒ…æŠ¥ä¸é‡‘èåˆ†ææ—¥æŠ¥",
                    "tag": "plain_text"
                }
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": clean_message
                },
                {
                    "tag": "note",
                    "elements": [
                        {
                            "tag": "plain_text",
                            "content": "ğŸ¤– DeepSeek-V3 æ™ºèƒ½åˆ†æç³»ç»Ÿ | ğŸ“… " + time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                    ]
                }
            ]
        }
    }

    headers = {
        "Content-Type": "application/json; charset=utf-8"
    }

    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨å‘é€æ¶ˆæ¯åˆ°é£ä¹¦webhook (å°è¯• {attempt + 1}/{max_retries})")
            response = requests.post(webhook_url, headers=headers, json=data, timeout=10)
            if response.status_code == 200:
                result = response.json()
                if result.get('StatusCode') == 0 or result.get('code') == 0:
                    logger.info("âœ… æ¶ˆæ¯æˆåŠŸå‘é€åˆ°é£ä¹¦ï¼")
                    return True
                else:
                    logger.error(f"é£ä¹¦webhookè¿”å›é”™è¯¯: {result.get('msg') or result.get('message')}")
            else:
                logger.error(f"HTTPé”™è¯¯: {response.status_code} - {response.text}")
        except Exception as e:
            logger.error(f"å‘é€é£ä¹¦webhookæ¶ˆæ¯å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY)
    
    logger.error("âŒ æ¶ˆæ¯å‘é€æœ€ç»ˆå¤±è´¥")
    return False


# ==================== æ ¸å¿ƒåŠŸèƒ½æ¨¡å— ====================

def fetch_rss_feed(url: str, max_retries: int = MAX_RETRIES) -> Optional[feedparser.FeedParserDict]:
    """
    ä»RSSæºæŠ“å–æ–°é—»ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œä»£ç†æ”¯æŒ
    
    Args:
        url: RSSæºURL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        feedparserè§£æç»“æœæˆ–None
    """
    from urllib.parse import urlparse
    domain = urlparse(url).netloc
    
    # æ™ºèƒ½å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
    time.sleep(random.uniform(1.0, 3.0))
    
    # åˆå§‹åŒ–UserAgent
    ua = UserAgent()
    
    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨æŠ“å–RSSæº: {url} (å°è¯• {attempt + 1}/{max_retries})")
            
            # ä½¿ç”¨æ›´çœŸå®çš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º
            headers = {
                'User-Agent': ua.random,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            # ä½¿ç”¨ä»£ç†æŠ“å–RSSï¼ˆå¦‚æœå¯ç”¨ï¼‰
            response = requests.get(
                url, 
                proxies=PROXIES, 
                timeout=15,  # å¢åŠ è¶…æ—¶æ—¶é—´
                headers=headers
            )
            
            # æ ¹æ®HTTPçŠ¶æ€ç è¿›è¡Œä¸åŒå¤„ç†
            if response.status_code == 200:
                # æˆåŠŸï¼Œè§£æRSS
                feed = feedparser.parse(response.content)
                logger.info(f"æˆåŠŸæŠ“å– {len(feed.entries)} æ¡æ–°é—»")
                return feed
            elif response.status_code == 403:
                # 403é”™è¯¯ï¼šæœåŠ¡å™¨æ‹’ç»è®¿é—®ï¼Œå¯èƒ½æ˜¯åçˆ¬è™«æœºåˆ¶
                logger.warning(f"403é”™è¯¯ - è®¿é—®è¢«æ‹’ç»: {url}")
                # æ›´æ¢User-Agentå†è¯•
                headers['User-Agent'] = ua.random
                if attempt < max_retries - 1:
                    time.sleep(5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                continue
            elif response.status_code == 404:
                # 404é”™è¯¯ï¼šèµ„æºä¸å­˜åœ¨
                logger.error(f"404é”™è¯¯ - RSSæºä¸å­˜åœ¨: {url}")
                return None  # ä¸é‡è¯•ï¼Œç›´æ¥è¿”å›
            elif response.status_code == 429:
                # 429é”™è¯¯ï¼šè¯·æ±‚è¿‡å¤š
                logger.warning(f"429é”™è¯¯ - è¯·æ±‚é¢‘ç‡è¿‡é«˜: {url}")
                # è·å–é‡è¯•æ—¶é—´ï¼ˆå¦‚æœæœ‰Retry-Afterå¤´ï¼‰
                retry_after = response.headers.get('Retry-After', 60)
                try:
                    delay = int(retry_after)
                except ValueError:
                    delay = 60  # é»˜è®¤ç­‰å¾…60ç§’
                if attempt < max_retries - 1:
                    time.sleep(delay * (attempt + 1))
                continue
            else:
                # å…¶ä»–é”™è¯¯
                logger.warning(f"HTTPé”™è¯¯ {response.status_code}: {url}")
                if attempt < max_retries - 1:
                    time.sleep(3 * (attempt + 1))
                    
        except requests.exceptions.SSLError as e:
            logger.warning(f"SSLé”™è¯¯ (å°è¯• {attempt + 1}): {e}")
            # å°è¯•å¿½ç•¥SSLéªŒè¯
            if attempt < max_retries - 1:
                try:
                    response = requests.get(
                        url, 
                        proxies=PROXIES, 
                        timeout=15,  # å¢åŠ è¶…æ—¶æ—¶é—´
                        headers=headers,
                        verify=False  # å¿½ç•¥SSLéªŒè¯
                    )
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        logger.info(f"æˆåŠŸæŠ“å– {len(feed.entries)} æ¡æ–°é—» (å¿½ç•¥SSLéªŒè¯)")
                        return feed
                except Exception:
                    pass  # ç»§ç»­é‡è¯•
                time.sleep(5 * (attempt + 1))
        except requests.exceptions.ConnectionError:
            logger.warning(f"è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}): {url}")
            if attempt < max_retries - 1:
                time.sleep(5 * (attempt + 1))
        except requests.exceptions.Timeout:
            logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}): {url}")
            if attempt < max_retries - 1:
                time.sleep(10 * (attempt + 1))
        except Exception as e:
            logger.warning(f"å…¶ä»–é”™è¯¯ (å°è¯• {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(3 * (attempt + 1))
    
    logger.error(f"RSSæŠ“å–æœ€ç»ˆå¤±è´¥: {url}")
    return None

def extract_news_items() -> List[Dict[str, str]]:
    """
    ä»å¤šä¸ªRSSæºæå–æ–°é—»æ¡ç›®ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«title, summary, link, importance_score
    """
    all_news = []
    seen_titles = set()  # ç”¨äºå»é‡
    processed_urls = load_cache()  # åŠ è½½å·²å¤„ç†çš„URLç¼“å­˜
    
    # å®šä¹‰RSSæºæƒé‡ï¼Œæƒå¨æ€§è¶Šé«˜çš„æºæƒé‡è¶Šå¤§
    source_weights = {
        "https://feeds.bbci.co.uk/news/world/rss.xml": 1.0,  # BBC World
        "https://rss.nytimes.com/services/xml/rss/nyt/World.xml": 1.0,  # NYT World
        "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664": 0.9,  # CNBC Finance
        "https://techcrunch.com/feed/": 0.8,  # TechCrunch AI & Startup
        "https://finance.yahoo.com/news/rssindex": 0.8,  # Yahoo Finance
        "https://www.coindesk.com/arc/outboundfeeds/rss/": 0.7,  # CoinDesk
        "https://oilprice.com/rss/main": 0.7,  # OilPrice.com
        "https://news.ycombinator.com/rss": 0.7,  # Hacker News
        "https://www.reddit.com/r/worldnews/top/.rss?t=day": 0.6,  # Reddit WorldNews
        "https://www.reddit.com/r/videos/top/.rss?t=day": 0.5,  # Reddit è§†é¢‘èšåˆ
        "https://www.scmp.com/rss/2/feed": 0.8,  # South China Morning Post
        "http://arxiv.org/rss/cs.AI": 0.6,  # ArXiv AI Paper Daily
        "http://news.baidu.com/n?cmd=file&format=rss&tn=rss&sub=0": 0.7,  # ç™¾åº¦æ–°é—»
        "http://rss.people.com.cn/GB/303140/index.xml": 0.9,  # äººæ°‘ç½‘
        "http://www.xinhuanet.com/politics/news_politics.xml": 0.9,  # æ–°åç½‘ - æ—¶æ”¿
        "http://www.chinanews.com/rss/scroll-news.xml": 0.7,  # ä¸­å›½æ–°é—»ç½‘
        "https://www.thepaper.cn/rss.jsp": 0.6,  # æ¾æ¹ƒæ–°é—»
        "https://www.cls.cn/v3/highlights?app_id=70301d300f0f95a1&platform=pc": 0.7,  # è´¢è”ç¤¾
        "https://www.zhihu.com/rss": 0.5,  # çŸ¥ä¹æ¯æ—¥ç²¾é€‰
        "https://www.36kr.com/feed": 0.6,  # 36æ°ª
        "https://news.qq.com/rss/channels/finance/rss.xml": 0.7,  # è…¾è®¯è´¢ç»
        "https://rss.sina.com.cn/news/china/focus15.xml": 0.7,  # æ–°æµªæ–°é—»-å›½å†…ç„¦ç‚¹
    }
    
    for rss_url in RSS_SOURCES:
        feed = fetch_rss_feed(rss_url)
        if not feed or not feed.entries:
            continue
            
        for entry in feed.entries[:5]:  # æ¯ä¸ªæºæœ€å¤šå–5æ¡
            title = entry.get('title', '').strip()
            summary = entry.get('summary', '').strip()
            link = entry.get('link', '')
            published_time = entry.get('published_parsed', None)
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡
            if link in processed_urls:
                continue
            
            # å»é‡æ£€æŸ¥
            if not title or title in seen_titles:
                continue
                
            seen_titles.add(title)
            
            # æ¸…ç†summaryä¸­çš„HTMLæ ‡ç­¾
            import re
            summary = re.sub(r'<[^>]+>', '', summary)
            summary = summary[:200] + '...' if len(summary) > 200 else summary
            
            # è®¡ç®—æ–°é—»é‡è¦æ€§åˆ†æ•°
            importance_score = calculate_importance_score(title, summary, rss_url, published_time, source_weights)
            
            all_news.append({
                'title': title,
                'summary': summary,
                'link': link,
                'importance_score': importance_score
            })
    
    # æŒ‰é‡è¦æ€§åˆ†æ•°é™åºæ’åº
    all_news.sort(key=lambda x: x['importance_score'], reverse=True)
    
    # æ›´æ–°ç¼“å­˜ï¼Œæ·»åŠ æ–°å¤„ç†çš„URL
    for item in all_news:
        processed_urls.add(item['link'])
    save_cache(processed_urls)
    
    logger.info(f"æ€»å…±æå–åˆ° {len(all_news)} æ¡å”¯ä¸€æ–°é—»ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº")
    return all_news[:10]  # æœ€å¤šå¤„ç†10æ¡æ–°é—»

def get_asset_price(asset_name: str) -> Optional[str]:
    """
    è·å–æŒ‡å®šèµ„äº§çš„å®æ—¶ä»·æ ¼ï¼ˆä½¿ç”¨å…è´¹APIï¼‰
    æ”¯æŒæ¯”ç‰¹å¸ã€é»„é‡‘ã€è‹±ä¼Ÿè¾¾è‚¡ç¥¨ç­‰
    """
    try:
        # æ ¹æ®èµ„äº§åç§°é€‰æ‹©ä¸åŒçš„API
        if asset_name.lower() in ['bitcoin', 'btc', 'æ¯”ç‰¹å¸']:
            # ä½¿ç”¨CoinGecko APIè·å–æ¯”ç‰¹å¸ä»·æ ¼
            response = requests.get("https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['bitcoin']['usd']
                return f"${price:,}"
        elif asset_name.lower() in ['ethereum', 'eth', 'ä»¥å¤ªåŠ']:
            # ä½¿ç”¨CoinGecko APIè·å–ä»¥å¤ªåŠä»·æ ¼
            response = requests.get("https://api.coingecko.com/api/v3/simple/price?ids=ethereum&vs_currencies=usd", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['ethereum']['usd']
                return f"${price:,}"
        elif asset_name.lower() in ['gold', 'é»„é‡‘']:
            # ä½¿ç”¨è´µé‡‘å±APIè·å–é»„é‡‘ä»·æ ¼ï¼ˆUSD/ç›å¸ï¼‰
            response = requests.get("https://api.metals.live/v1/spot/gold", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['value']
                return f"${price:.2f}/oz"
        elif asset_name.lower() in ['nvidia', 'nvda', 'è‹±ä¼Ÿè¾¾']:
            # ä½¿ç”¨Yahoo Finance APIè·å–è‹±ä¼Ÿè¾¾è‚¡ç¥¨ä»·æ ¼
            response = requests.get(f"https://query1.finance.yahoo.com/v8/finance/chart/NVDA", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['chart']['result'][0]['meta']['regularMarketPrice']
                return f"${price:.2f}"
        elif asset_name.lower() in ['apple', 'aapl', 'è‹¹æœ']:
            # ä½¿ç”¨Yahoo Finance APIè·å–è‹¹æœè‚¡ç¥¨ä»·æ ¼
            response = requests.get(f"https://query1.finance.yahoo.com/v8/finance/chart/AAPL", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['chart']['result'][0]['meta']['regularMarketPrice']
                return f"${price:.2f}"
        elif asset_name.lower() in ['s&p 500', 'sp500', 'æ ‡æ™®500']:
            # ä½¿ç”¨Yahoo Finance APIè·å–æ ‡æ™®500ä»·æ ¼
            response = requests.get(f"https://query1.finance.yahoo.com/v8/finance/chart/SPY", timeout=10)
            if response.status_code == 200:
                data = response.json()
                price = data['chart']['result'][0]['meta']['regularMarketPrice']
                return f"${price:.2f}"
    except Exception as e:
        logger.warning(f"è·å–{asset_name}ä»·æ ¼å¤±è´¥: {e}")
        return None

def analyze_news_with_llm(news_items: List[Dict[str, str]], report_type: str = 'daily') -> str:
    """
    è°ƒç”¨LLM APIå¯¹æ–°é—»è¿›è¡Œæ·±åº¦åˆ†æï¼ˆå¸¦å»é‡é€»è¾‘ã€æƒ…ç»ªè¯„åˆ†ã€ä»·æ ¼æ³¨å…¥å’Œå›¾ç‰‡ä¿¡æ¯ï¼‰
    ä½¿ç”¨åˆ†æ‰¹å¤„ç†æœºåˆ¶ï¼Œé¿å…ä¸€æ¬¡æ€§å‘é€è¿‡å¤šå†…å®¹å¯¼è‡´è¶…æ—¶
    
    Args:
        news_items: æ–°é—»æ¡ç›®åˆ—è¡¨
        report_type: æŠ¥å‘Šç±»å‹ ('morning', 'noon', 'evening', 'summary', 'daily')
        
    Returns:
        LLMç”Ÿæˆçš„Markdownæ ¼å¼åˆ†ææŠ¥å‘Š
    """
    if not news_items:
        return "ä»Šæ—¥æ— é‡è¦æ–°é—»æ›´æ–°ã€‚"
    
    # å°†æ–°é—»åˆ—è¡¨æŒ‰æ¯30æ¡ä¸€ç»„è¿›è¡Œæ‹†åˆ†
    batch_size = 30
    batches = [news_items[i:i + batch_size] for i in range(0, len(news_items), batch_size)]
    
    # ä¸ºæ¯ä¸ªæ‰¹æ¬¡æ„å»ºåˆ†æå†…å®¹
    batch_analyses = []
    for batch_idx, batch in enumerate(batches):
        logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {batch_idx + 1}/{len(batches)} æ‰¹æ¬¡æ–°é—» (å…± {len(batch)} æ¡)")
        
        # æ„å»ºå•ä¸ªæ‰¹æ¬¡çš„æ–°é—»å†…å®¹
        news_content = ""
        for i, item in enumerate(batch):
            # æ£€æŸ¥æ–°é—»ä¸­æ˜¯å¦åŒ…å«éœ€è¦ä»·æ ¼æ³¨å…¥çš„å…³é”®è¯
            title_lower = item['title'].lower()
            summary_lower = item['summary'].lower()
            price_info = ""
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³èµ„äº§å…³é”®è¯
            assets_to_check = ['bitcoin', 'btc', 'ethereum', 'eth', 'gold', 'nvidia', 'nvda', 'apple', 'aapl', 's&p 500', 'sp500']
            for asset in assets_to_check:
                if asset in title_lower or asset in summary_lower:
                    price = get_asset_price(asset)
                    if price:
                        price_info = f" (å½“å‰ä»·æ ¼ï¼š{price})"
                    break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±åœæ­¢
            
            news_content += f"**ID**: {i+1}\n**æ ‡é¢˜**: {item['title']}{price_info}\n**æ‘˜è¦**: {item['summary']}\n**é“¾æ¥**: {item['link']}\n\n"
        
        # ä¸ºå•ä¸ªæ‰¹æ¬¡ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
        SYSTEM_PROMPT = """ä½ æ˜¯ä¸€åé¡¶çº§æ¸¸èµ„æ“ç›˜æ‰‹å’Œå®è§‚ç­–ç•¥å¸ˆã€‚ä½ çš„è¯»è€…æ˜¯æ—¶é—´å®è´µçš„ä¸­å›½æŠ•èµ„è€…/æ‰“å·¥äººã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼š**é€è¿‡æ–°é—»è¡¨è±¡ï¼Œç›´æ¥æ‹†è§£åˆ©ç›Šé“¾æ¡ï¼Œç»™å‡ºæœ€å†·è¡€çš„åˆ¤æ–­ã€‚**

# Constraints
1. **è¯¦ç»†åˆ†æ**ï¼šå…¨ç¯‡æ§åˆ¶åœ¨ 600 å­—å·¦å³ï¼Œæä¾›æ·±å…¥çš„åˆ†æå’Œè§è§£ã€‚
2. **é€šä¿—æ˜“æ‡‚**ï¼šä½¿ç”¨ç®€å•æ˜äº†çš„è¯­è¨€ï¼Œé¿å…å¤æ‚çš„ç®­å¤´ç¬¦å·ï¼Œè®©æ™®é€šç”¨æˆ·ä¹Ÿèƒ½ç†è§£ã€‚
3. **æ ¼å¼ä¸¥æ ¼**ï¼šå¿…é¡»éµå®ˆä¸‹æ–¹çš„ Markdown æ ¼å¼ã€‚
4. **ä¸­å›½è§†è§’**ï¼šæ‰€æœ‰å½±å“åˆ†æå¿…é¡»ç´§æ‰£ä¸­å›½å›½è¿ã€Aè‚¡/æ¸¯è‚¡å’Œæ‰“å·¥äººçš„é’±è¢‹å­ã€‚
5. **ä¸¥ç¦æè¿°å›¾ç‰‡**ï¼šä¸è¦è¾“å‡ºä»»ä½•å›¾ç‰‡æè¿°ã€‚

# Analysis Framework (Markdown Output)
è¯·å¯¹ç­›é€‰å‡ºçš„ Top æ–°é—»æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼š

### [æƒ…ç»ªåˆ† | åˆ†æ•°] æ–°é—»æ ‡é¢˜ (ä¸­æ–‡ï¼ŒåŠ ç²—)

> [ğŸ”— ç‚¹å‡»ç›´è¾¾åŸæ–°é—»](æ–°é—»URL) | æ¥æºï¼šæ–°é—»Source

**æ ¸å¿ƒè¦ç‚¹**ï¼šä¸€å¥è¯æ¦‚æ‹¬æ–°é—»çš„æ ¸å¿ƒå†…å®¹

**äº‹ä»¶è¯¦æƒ…**ï¼šè¯¦ç»†ä»‹ç»å‘ç”Ÿäº†ä»€ä¹ˆäº‹æƒ…ï¼Œæ¶‰åŠå“ªäº›å…³é”®äººç‰©ã€å…¬å¸æˆ–ç»„ç»‡ï¼Œä»¥åŠå…·ä½“çš„æ—¶é—´ã€åœ°ç‚¹ã€æ•°æ®ç­‰ã€‚

**æ·±å±‚è§£è¯»**ï¼šæ·±å…¥åˆ†æè¿™åˆ™æ–°é—»èƒŒåçš„åŠ¨æœºå’ŒåŸå› ã€‚ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µï¼Ÿæ˜¯å‡ºäºå•†ä¸šè€ƒè™‘ã€æ”¿ç­–é©±åŠ¨ã€å¸‚åœºç«äº‰è¿˜æ˜¯æŠ€æœ¯çªç ´ï¼Ÿè§£é‡Šæ¸…æ¥šäº‹ä»¶å‘ç”Ÿçš„æ ¹æœ¬åŸå› ã€‚

**å¯¹ä¸­å›½çš„æ½œåœ¨å½±å“**ï¼š
- **çŸ­æœŸå½±å“**ï¼šå¯¹ä¸­å›½ç»æµã€é‡‘èå¸‚åœºã€ç›¸å…³è¡Œä¸šæˆ–æ¶ˆè´¹è€…çš„ç›´æ¥å½±å“
- **é•¿æœŸå½±å“**ï¼šå¯¹æœªæ¥å‘å±•è¶‹åŠ¿ã€äº§ä¸šå¸ƒå±€ã€å›½é™…åœ°ä½ç­‰æ–¹é¢çš„æ·±è¿œå½±å“

**å¯¹è‚¡å¸‚å’ŒæŠ•èµ„çš„å½±å“**ï¼š
- **å¯èƒ½å—ç›Šçš„æ¿å—æˆ–è‚¡ç¥¨**ï¼šåˆ—å‡ºå¯èƒ½å› æ­¤å—ç›Šçš„è¡Œä¸šã€å…¬å¸æˆ–æŠ•èµ„æ ‡çš„
- **å¯èƒ½å—æŸçš„æ¿å—æˆ–è‚¡ç¥¨**ï¼šæŒ‡å‡ºå¯èƒ½é¢ä¸´è´Ÿé¢å½±å“çš„é¢†åŸŸ
- **æŠ•èµ„ç­–ç•¥å»ºè®®**ï¼šåŸºäºæ­¤æ–°é—»ï¼ŒæŠ•èµ„è€…åº”è¯¥å¦‚ä½•è°ƒæ•´ç­–ç•¥

**æœªæ¥å±•æœ›**ï¼šé¢„æµ‹è¿™ä¸€äº‹ä»¶å¯èƒ½å¸¦æ¥çš„åç»­å‘å±•ï¼Œä»¥åŠæˆ‘ä»¬åº”è¯¥å¦‚ä½•åº”å¯¹ã€‚

**å…³è”ä¿¡æ¯**ï¼šå¦‚æœè¿™åˆ™æ–°é—»ä¸å…¶ä»–äº‹ä»¶æœ‰å…³è”ï¼Œè¯´æ˜å®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚"""

        system_prompt = SYSTEM_PROMPT

        # ç”¨æˆ·æ¶ˆæ¯
        user_message = f"è¯·åˆ†æä»¥ä¸‹æ–°é—»ï¼ˆå…±{len(batch)}æ¡ï¼‰ï¼Œå¹¶å¯¹é‡å¤è¯é¢˜è¿›è¡Œåˆå¹¶ï¼Œä¸ºæ¯æ¡æ–°é—»æ·»åŠ æƒ…ç»ªè¯„åˆ†å’Œä»·æ ¼ä¿¡æ¯ï¼š\n\n{news_content}"
        
        # è°ƒç”¨DeepSeek APIï¼ˆä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼ï¼‰
        headers = {
            'Authorization': f'Bearer {API_KEY}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            "model": "deepseek-chat",  # DeepSeek V3.2çš„æ¨¡å‹åç§°
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            "temperature": 0.7,
            "max_tokens": 4000  # å¢åŠ tokené™åˆ¶ä»¥å¤„ç†å¤šæ¡æ–°é—»
        }
        
        for attempt in range(MAX_RETRIES):
            try:
                logger.info(f"æ­£åœ¨è°ƒç”¨DeepSeek APIè¿›è¡Œç¬¬ {batch_idx + 1} æ‰¹æ¬¡æ–°é—»åˆ†æ (å°è¯• {attempt + 1}/{MAX_RETRIES})")
                
                # DeepSeek APIåœ¨ä¸­å›½å¢ƒå†…ï¼Œä¸éœ€è¦ä»£ç†
                response = requests.post(
                    f"{BASE_URL}/chat/completions",
                    headers=headers,
                    json=payload,
                    proxies=None,  # ä¸ä½¿ç”¨ä»£ç†è®¿é—®DeepSeek API
                    timeout=(5, 120)  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’ï¼Œè¿æ¥5ç§’ï¼Œè¯»å–120ç§’
                )
                
                if response.status_code == 200:
                    result = response.json()
                    analysis = result['choices'][0]['message']['content']
                    logger.info(f"ç¬¬ {batch_idx + 1} æ‰¹æ¬¡LLMåˆ†æå®Œæˆ")
                    batch_analyses.append(analysis)
                    break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯
                else:
                    logger.warning(f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                    
            except Exception as e:
                logger.warning(f"LLM APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
                
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
        else:
            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæ·»åŠ ç®€åŒ–ç‰ˆæœ¬
            logger.error(f"ç¬¬ {batch_idx + 1} æ‰¹æ¬¡LLMåˆ†æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬")
            fallback_analysis = ""
            for i, item in enumerate(batch[:3], 1):
                fallback_analysis += f"### {i}. [ç‚¹å‡»ç›´è¾¾ï¼š{item['title']}]({item['link']})\n"
                fallback_analysis += "- **ğŸ“… æ¥æº**: å›½é™…åª’ä½“\n"
                fallback_analysis += f"- **ğŸ“ æ ¸å¿ƒäº‹å®**: {item['summary'][:30]}...\n\n"
                fallback_analysis += "#### ğŸ“Š æ·±åº¦ç ”æŠ¥\n"
                fallback_analysis += "* **ğŸ‡¨ğŸ‡³ å¯¹ä¸­å›½çŸ­æœŸå½±å“**: å¾…åˆ†æ\n"
                fallback_analysis += "* **ğŸ”® å¯¹ä¸­å›½é•¿æœŸå½±å“**: å¾…åˆ†æ\n"
                fallback_analysis += "* **ğŸ“ˆ è‚¡å¸‚å½±å“ (Aè‚¡/æ¸¯è‚¡/ç¾è‚¡)**:\n"
                fallback_analysis += "    * *åˆ©å¥½/åˆ©ç©ºæ¿å—*: å¾…åˆ†æ\n"
                fallback_analysis += "    * *åº•å±‚é€»è¾‘*: å¾…åˆ†æ\n\n"
                fallback_analysis += "---\n"
            batch_analyses.append(fallback_analysis)
    
    # å¦‚æœæœ‰å¤šæ‰¹æ¬¡ï¼Œéœ€è¦å°†å„æ‰¹æ¬¡ç»“æœè¿›è¡Œç»¼åˆæ±‡æ€»
    if len(batch_analyses) > 1:
        logger.info(f"æ­£åœ¨è¿›è¡Œè·¨æ‰¹æ¬¡ç»¼åˆæ±‡æ€» (å…± {len(batch_analyses)} ä¸ªæ‰¹æ¬¡)")
        combined_analysis = "\n".join(batch_analyses)
        summary_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆã€‚è¯·å°†ä»¥ä¸‹æ¥è‡ªä¸åŒæ‰¹æ¬¡çš„æ–°é—»åˆ†æç»“æœè¿›è¡Œæ•´åˆï¼Œå»é™¤é‡å¤å†…å®¹ï¼Œå½¢æˆä¸€ä»½è¿è´¯çš„æŠ¥å‘Šã€‚è¦æ±‚ä¿æŒåŸæœ‰çš„æ ¼å¼å’Œç»“æ„ã€‚

{combined_analysis}"""

        headers = {
            'Authorization': f'Bearer {API_KEY}',
            'Content-Type': 'application/json'
        }

        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆï¼Œè´Ÿè´£æ•´åˆå¤šä»½æ–°é—»åˆ†ææŠ¥å‘Šã€‚"},
                {"role": "user", "content": summary_prompt}
            ],
            "temperature": 0.5,
            "max_tokens": 4000
        }

        for attempt in range(MAX_RETRIES):
            try:
                logger.info(f"æ­£åœ¨è°ƒç”¨DeepSeek APIè¿›è¡Œè·¨æ‰¹æ¬¡ç»¼åˆæ±‡æ€» (å°è¯• {attempt + 1}/{MAX_RETRIES})")
                
                response = requests.post(
                    f"{BASE_URL}/chat/completions",
                    headers=headers,
                    json=payload,
                    proxies=None,
                    timeout=(5, 120)  # å¢åŠ è¶…æ—¶æ—¶é—´
                )
                
                if response.status_code == 200:
                    result = response.json()
                    final_analysis = result['choices'][0]['message']['content']
                    logger.info("è·¨æ‰¹æ¬¡ç»¼åˆæ±‡æ€»å®Œæˆ")
                    return final_analysis
                else:
                    logger.warning(f"è·¨æ‰¹æ¬¡æ±‡æ€»APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            except Exception as e:
                logger.warning(f"è·¨æ‰¹æ¬¡æ±‡æ€»APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
                
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
        else:
            logger.error("è·¨æ‰¹æ¬¡ç»¼åˆæ±‡æ€»å¤±è´¥ï¼Œè¿”å›åŸå§‹æ‰¹æ¬¡åˆ†æç»“æœ")
            return combined_analysis

    # å¦‚æœåªæœ‰ä¸€ä¸ªæ‰¹æ¬¡ï¼Œç›´æ¥è¿”å›
    return batch_analyses[0] if batch_analyses else "ä»Šæ—¥æ— é‡è¦æ–°é—»æ›´æ–°ã€‚"

def calculate_importance_score(title: str, summary: str, source_url: str, published_time, source_weights: dict) -> float:
    """
    è®¡ç®—æ–°é—»é‡è¦æ€§åˆ†æ•°
    
    Args:
        title: æ–°é—»æ ‡é¢˜
        summary: æ–°é—»æ‘˜è¦
        source_url: æ–°é—»æ¥æºURL
        published_time: å‘å¸ƒæ—¶é—´
        source_weights: æ¥æºæƒé‡å­—å…¸
    
    Returns:
        é‡è¦æ€§åˆ†æ•° (0-10)
    """
    score = 0.0
    
    # 1. æ¥æºæƒé‡ (åŸºç¡€åˆ†æ•°)
    base_weight = source_weights.get(source_url, 0.5)  # é»˜è®¤æƒé‡0.5
    score += base_weight * 4  # æƒé‡å æ¯”40%
    
    # 2. æ ‡é¢˜å…³é”®è¯åˆ†æ (æ—¶æ•ˆæ€§ã€ç´§æ€¥æ€§ã€å½±å“åŠ›)
    title_lower = title.lower()
    urgency_keywords = ['çªå‘', 'ç´§æ€¥', 'è­¦å‘Š', 'å±æœº', 'æš´è·Œ', 'æš´æ¶¨', 'æˆ˜', 'å†²çª', 'åˆ¶è£', 'æ”¿ç­–', 'å¤®è¡Œ', 'åˆ©ç‡', 'gdp', 'å°±ä¸š', 'é€šèƒ€']
    financial_keywords = ['ç»æµ', 'è‚¡å¸‚', 'åŸºé‡‘', 'å€ºåˆ¸', 'ç¾å…ƒ', 'äººæ°‘å¸', 'é»„é‡‘', 'çŸ³æ²¹', 'æ¯”ç‰¹å¸', 'ai', 'ç§‘æŠ€', 'å…¬å¸', 'è´¢æŠ¥']
    china_keywords = ['ä¸­å›½', 'chinese', 'beijing', 'shanghai', 'hk', 'æ¸¯', 'aè‚¡', 'äººæ°‘å¸', 'cny', 'è´¸æ˜“', 'ä¸­ç¾', 'ç¾ä¸­']
    
    # æ£€æŸ¥ç´§æ€¥å…³é”®è¯
    for keyword in urgency_keywords:
        if keyword in title_lower:
            score += 1.0  # æ¯ä¸ªç´§æ€¥å…³é”®è¯+1åˆ†
    
    # æ£€æŸ¥é‡‘èå…³é”®è¯
    for keyword in financial_keywords:
        if keyword in title_lower:
            score += 0.5  # æ¯ä¸ªé‡‘èå…³é”®è¯+0.5åˆ†
    
    # æ£€æŸ¥ä¸­å›½ç›¸å…³å…³é”®è¯
    for keyword in china_keywords:
        if keyword in title_lower:
            score += 1.0  # æ¯ä¸ªä¸­å›½ç›¸å…³å…³é”®è¯+1åˆ†
    
    # 3. å†…å®¹é•¿åº¦ (æ›´é•¿çš„å†…å®¹å¯èƒ½æ›´é‡è¦)
    content_length = len(title) + len(summary)
    if content_length > 200:
        score += 1.0
    elif content_length > 100:
        score += 0.5
    
    # 4. æ—¶é—´å› ç´  (å¦‚æœæ˜¯ä»Šå¤©å‘å¸ƒçš„æ–°é—»ï¼Œå¢åŠ åˆ†æ•°)
    import datetime
    if published_time:
        pub_date = datetime.datetime(*published_time[:6])
        now = datetime.datetime.now()
        hours_diff = (now - pub_date).total_seconds() / 3600
        if hours_diff <= 24:  # 24å°æ—¶å†…å‘å¸ƒçš„æ–°é—»
            score += 1.0
        elif hours_diff <= 48:  # 48å°æ—¶å†…å‘å¸ƒçš„æ–°é—»
            score += 0.5
    
    # 5. æ ‡é¢˜é•¿åº¦å’Œç‰¹å¾ (æ ‡é¢˜é•¿åº¦é€‚ä¸­ä¸”åŒ…å«æ•°å­—æˆ–ç¬¦å·å¯èƒ½æ›´é‡è¦)
    if 30 < len(title) < 100:  # æ ‡é¢˜é•¿åº¦é€‚ä¸­
        score += 0.5
    if ':' in title or '-' in title:  # åŒ…å«åˆ†éš”ç¬¦
        score += 0.3
    if any(char.isdigit() for char in title):  # åŒ…å«æ•°å­—
        score += 0.2
    
    return min(score, 10.0)  # é™åˆ¶æœ€å¤§åˆ†æ•°ä¸º10

def parse_sentiment_score(message: str) -> float:
    """
    ä»æ¶ˆæ¯ä¸­è§£ææ•´ä½“æƒ…ç»ªå¾—åˆ†
    """
    import re
    # æŸ¥æ‰¾ç±»ä¼¼ [ğŸ”¥+8] æˆ– [â„ï¸-8] çš„æ¨¡å¼
    pattern = r'\[(?:ğŸ”¥|â„ï¸|âš¡|ğŸ“‰|ğŸ“ˆ)\s*([+-]?\d+)\]'
    matches = re.findall(pattern, message)
    if matches:
        scores = [int(score) for score in matches]
        # è¿”å›å¹³å‡å€¼ä½œä¸ºæ•´ä½“æƒ…ç»ªå¾—åˆ†
        return sum(scores) / len(scores) if scores else 0
    return 0

def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„æ–°é—»åˆ†ææµç¨‹
    """
    logger.info("ğŸš€ å¯åŠ¨æ¯æ—¥æ–°é—»æœºå™¨äºº...")
    try:
        # 1. æŠ“å–æ–°é—»
        news_items = extract_news_items()
        if not news_items:
            logger.warning("æœªè·å–åˆ°ä»»ä½•æ–°é—»ï¼Œè·³è¿‡åˆ†æ")
            # å³ä½¿æ²¡æœ‰æ–°é—»ä¹Ÿè¦è®°å½•æ—¥å¿—
            logger.info("ğŸ“Š æŠ“å–ç»Ÿè®¡: æˆåŠŸ 0 æ¡, å¤±è´¥ 0 æ¡")
            return
        
        logger.info(f"ğŸ“Š æŠ“å–ç»Ÿè®¡: æˆåŠŸ {len(news_items)} æ¡, å¤±è´¥ 0 æ¡")
        
        # 2. LLMæ·±åº¦åˆ†æ
        analysis_result = analyze_news_with_llm(news_items)
        sentiment_score = parse_sentiment_score(analysis_result)
        logger.info(f"ğŸ“Š LLMè¯„åˆ†æ˜ç»†: æƒ…ç»ªåˆ† {sentiment_score}, æ–°é—»æ•°é‡ {len(news_items)}")
        
        # 3. å‘é€åˆ°é£ä¹¦
        success = send_to_feishu(analysis_result)
        
        if success:
            logger.info("ğŸ‰ æ¯æ—¥æ–°é—»åˆ†æä»»åŠ¡å®Œæˆï¼")
        else:
            logger.error("âŒ æ¯æ—¥æ–°é—»åˆ†æä»»åŠ¡å¤±è´¥")
            send_error_alert("æ—¥æŠ¥å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é£ä¹¦åº”ç”¨é…ç½®")
            
    except Exception as e:
        logger.exception(f"ä¸»å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}")
        send_error_alert(f"æœºå™¨äººæ•…éšœï¼š{str(e)}ï¼Œè¯·ä¸»äººæ£€æŸ¥ï¼")

def send_error_alert(error_message: str, max_retries: int = MAX_RETRIES):
    """
    å‘é€é”™è¯¯è­¦æŠ¥åˆ°é£ä¹¦ï¼ˆä½¿ç”¨webhookæ–¹å¼ï¼‰
    """
    # æ„å»ºé”™è¯¯è­¦æŠ¥æ¶ˆæ¯
    alert_msg = f"ğŸš¨ æœºå™¨äººæ•…éšœè­¦æŠ¥\n\né”™è¯¯è¯¦æƒ…ï¼š{error_message}\n\nè¯·åŠæ—¶æ£€æŸ¥æœºå™¨äººçŠ¶æ€ï¼\n\nDeepSeek-V3 ç›‘æ§ç³»ç»Ÿ"
    
    # ä½¿ç”¨webhookå‘é€é”™è¯¯è­¦æŠ¥
    return send_to_feishu_webhook(alert_msg, max_retries)

# ==================== ä¸»å‡½æ•° ====================

if __name__ == "__main__":
    main()


