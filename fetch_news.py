import requests
import json
from datetime import datetime

class NewsAnalyzer:
    def __init__(self):
        # ä½¿ç”¨æ–°çš„åº”ç”¨è®¤è¯æ–¹å¼ï¼Œä¸å†éœ€è¦webhook URL
        pass
        
    def analyze_news_impact(self, title, description, source_type="general"):
        """
        åˆ†ææ–°é—»å¯¹ä¸­å›½å’Œè‚¡å¸‚çš„å½±å“
        """
        # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„AIåˆ†æé€»è¾‘
        # ç›®å‰ä½¿ç”¨åŸºäºå…³é”®è¯çš„ç®€å•åˆ†æ
        
        impact_analysis = {
            "cause": "äº‹ä»¶èµ·å› å¾…åˆ†æ",
            "short_term_china": "æš‚æ— æ˜¾è‘—çŸ­æœŸå½±å“",
            "long_term_china": "é•¿æœŸå½±å“éœ€è¿›ä¸€æ­¥è§‚å¯Ÿ",
            "stock_market": "å¯¹è‚¡å¸‚å½±å“æœ‰é™"
        }
        
        # å…³é”®è¯åˆ†æ
        title_lower = title.lower()
        desc_lower = description.lower() if description else ""
        combined_text = title_lower + " " + desc_lower
        
        # AI/ç§‘æŠ€ç›¸å…³
        if any(keyword in combined_text for keyword in ["ai", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç®—æ³•"]):
            impact_analysis["cause"] = "æŠ€æœ¯è¿›æ­¥å’Œå¸‚åœºéœ€æ±‚é©±åŠ¨AIæŠ€æœ¯å¿«é€Ÿå‘å±•"
            impact_analysis["short_term_china"] = "æ¨åŠ¨AIäº§ä¸šå‘å±•ï¼Œä¿ƒè¿›æŠ€æœ¯åˆ›æ–°"
            impact_analysis["long_term_china"] = "æå‡å›½å®¶ç§‘æŠ€ç«äº‰åŠ›ï¼ŒåŠ é€Ÿæ•°å­—åŒ–è½¬å‹"
            impact_analysis["stock_market"] = "åˆ©å¥½Aè‚¡AIç›¸å…³è¡Œä¸šï¼Œå¦‚è®¡ç®—æœºï¼ˆç§‘å¤§è®¯é£ã€æµªæ½®ä¿¡æ¯ï¼‰å’Œç”µå­ï¼ˆéŸ¦å°”è‚¡ä»½ã€å…†æ˜“åˆ›æ–°ï¼‰ç­‰æ¿å—"
            
        # ç»æµ/æ”¿ç­–ç›¸å…³
        elif any(keyword in combined_text for keyword in ["ç»æµ", "æ”¿ç­–", "è´¢æ”¿", "é‡‘è", "åˆ©ç‡", "é€šèƒ€"]):
            impact_analysis["cause"] = "å®è§‚ç»æµç¯å¢ƒå˜åŒ–æˆ–æ”¿ç­–è°ƒæ•´"
            impact_analysis["short_term_china"] = "å½±å“å¸‚åœºä¿¡å¿ƒå’ŒæŠ•èµ„å†³ç­–"
            impact_analysis["long_term_china"] = "å½±å“ç»æµç»“æ„è°ƒæ•´å’Œäº§ä¸šå‡çº§"
            impact_analysis["stock_market"] = "å¯èƒ½å¼•å‘å¸‚åœºæ³¢åŠ¨ï¼Œé‡‘èï¼ˆæ‹›å•†é“¶è¡Œã€ä¸­å›½å¹³å®‰ï¼‰å’Œåœ°äº§ï¼ˆä¸‡ç§‘Aã€ä¿åˆ©å‘å±•ï¼‰ç­‰æ¿å—å—å½±å“è¾ƒå¤§"
            
        # å›½é™…å…³ç³»ç›¸å…³
        elif any(keyword in combined_text for keyword in ["è´¸æ˜“", "å…³ç¨", "å¤–äº¤", "å›½é™…", "åˆä½œ", "å†²çª"]):
            impact_analysis["cause"] = "å›½é™…æ”¿æ²»ç»æµæ ¼å±€å˜åŒ–æˆ–åœ°ç¼˜æ”¿æ²»å› ç´ "
            impact_analysis["short_term_china"] = "å½±å“å¤–è´¸ä¼ä¸šå’Œå›½é™…åˆä½œ"
            impact_analysis["long_term_china"] = "å½±å“å…¨çƒä¾›åº”é“¾å’Œæˆ˜ç•¥å¸ƒå±€"
            impact_analysis["stock_market"] = "å¤–è´¸ï¼ˆä¸­è¿œæµ·æ§ã€æµ·å°”æ™ºå®¶ï¼‰ã€èˆªè¿ï¼ˆæ‹›å•†è½®èˆ¹ã€ä¸­è¿œæµ·å‘ï¼‰ç­‰æ¿å—å¯èƒ½æ³¢åŠ¨"
            
        # åŒ»ç–—/å¥åº·ç›¸å…³
        elif any(keyword in combined_text for keyword in ["åŒ»ç–—", "å¥åº·", "åŒ»è¯", "ç–«è‹—", "ç”Ÿç‰©ç§‘æŠ€"]):
            impact_analysis["cause"] = "åŒ»ç–—æŠ€æœ¯è¿›æ­¥æˆ–å…¬å…±å«ç”Ÿéœ€æ±‚å¢é•¿"
            impact_analysis["short_term_china"] = "ä¿ƒè¿›åŒ»ç–—å¥åº·äº§ä¸šå‘å±•"
            impact_analysis["long_term_china"] = "æå‡å…¬å…±å«ç”Ÿä½“ç³»å’ŒåŒ»ç–—æŠ€æœ¯æ°´å¹³"
            impact_analysis["stock_market"] = "åˆ©å¥½åŒ»è¯ï¼ˆæ’ç‘åŒ»è¯ã€è¯æ˜åº·å¾·ï¼‰ã€ç”Ÿç‰©ç§‘æŠ€ï¼ˆåå¤§åŸºå› ã€æ™ºé£ç”Ÿç‰©ï¼‰ç­‰æ¿å—"
            
        # æ–°èƒ½æº/ç¯ä¿ç›¸å…³
        elif any(keyword in combined_text for keyword in ["æ–°èƒ½æº", "å…‰ä¼", "é£ç”µ", "ç”µæ± ", "ç¯ä¿", "ç¢³ä¸­å’Œ"]):
            impact_analysis["cause"] = "èƒ½æºè½¬å‹éœ€æ±‚å’Œç¯ä¿æ”¿ç­–æ¨åŠ¨"
            impact_analysis["short_term_china"] = "ä¿ƒè¿›æ–°èƒ½æºäº§ä¸šå‘å±•ï¼Œå¸¦åŠ¨ç›¸å…³äº§ä¸šé“¾"
            impact_analysis["long_term_china"] = "åŠ©åŠ›å®ç°åŒç¢³ç›®æ ‡ï¼Œæ¨åŠ¨èƒ½æºç»“æ„ä¼˜åŒ–"
            impact_analysis["stock_market"] = "åˆ©å¥½æ–°èƒ½æºï¼ˆå®å¾·æ—¶ä»£ã€éš†åŸºç»¿èƒ½ï¼‰ã€ç”µåŠ›è®¾å¤‡ï¼ˆå›½ç”µå—ç‘ã€è®¸ç»§ç”µæ°”ï¼‰ç­‰æ¿å—"
            
        return impact_analysis
    
    def calculate_importance_score(self, title, description):
        """
        è®¡ç®—æ–°é—»é‡è¦æ€§è¯„åˆ†ï¼Œç”¨äºæ’åº
        """
        score = 0
        text = (title + " " + description).lower()
        
        # é«˜é‡è¦æ€§å…³é”®è¯ï¼ˆ+10åˆ†ï¼‰
        high_importance = ["é‡å¤§", "çªç ´", "å‘å¸ƒ", "æ”¿ç­–", "æ³•è§„", "ç›‘ç®¡", "å±æœº", "å†²çª", "åˆä½œ"]
        for keyword in high_importance:
            if keyword in text:
                score += 10
                
        # ä¸­ç­‰é‡è¦æ€§å…³é”®è¯ï¼ˆ+5åˆ†ï¼‰
        medium_importance = ["ai", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "èŠ¯ç‰‡", "æŠ€æœ¯", "åˆ›æ–°", "ç»æµ", "é‡‘è", "åŒ»ç–—", "æ–°èƒ½æº"]
        for keyword in medium_importance:
            if keyword in text:
                score += 5
                
        # ä¸­å›½ç›¸å…³ï¼ˆ+3åˆ†ï¼‰
        china_keywords = ["ä¸­å›½", "å›½å†…", "å›½äº§", "æœ¬åœŸ", "åä¸º", "è…¾è®¯", "é˜¿é‡Œ", "ç™¾åº¦"]
        for keyword in china_keywords:
            if keyword in text:
                score += 3
                
        return score
    
    def create_news_summary(self, articles):
        """
        åˆ›å»ºåŒ…å«å½±å“åˆ†æçš„æ–°é—»æ‘˜è¦ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº
        """
        summary_items = []
        
        # å…ˆè®¡ç®—æ¯æ¡æ–°é—»çš„é‡è¦æ€§è¯„åˆ†
        scored_articles = []
        for article in articles:
            title = article.get('title', 'æ— æ ‡é¢˜')
            description = article.get('description', 'æ— æè¿°')
            url = article.get('url', '#')
            source = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
            
            if not title or not description or title == '[Removed]' or description == '[Removed]':
                continue
                
            importance_score = self.calculate_importance_score(title, description)
            scored_articles.append({
                'article': article,
                'score': importance_score
            })
        
        # æŒ‰é‡è¦æ€§è¯„åˆ†é™åºæ’åº
        scored_articles.sort(key=lambda x: x['score'], reverse=True)
        
        # åªå¤„ç†å‰5æ¡æœ€é‡è¦çš„æ–°é—»
        for item in scored_articles[:5]:
            article = item['article']
            title = article.get('title', 'æ— æ ‡é¢˜')
            description = article.get('description', 'æ— æè¿°')
            url = article.get('url', '#')
            source = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
            
            # åˆ†æå½±å“
            impact = self.analyze_news_impact(title, description)
            
            # åˆ›å»ºå®Œæ•´çš„æ‘˜è¦æ–‡æœ¬ï¼ˆæŒ‰è¦æ±‚çš„é¡ºåºï¼‰
            summary_text = f"ã€äº‹ä»¶ç®€è¦ã€‘\n{description}\n\n"
            summary_text += f"ã€äº‹ä»¶èµ·å› ã€‘\n{impact['cause']}\n\n"
            summary_text += f"ã€å¯¹ä¸­å›½çŸ­æœŸå½±å“ã€‘\n{impact['short_term_china']}\n\n"
            summary_text += f"ã€å¯¹ä¸­å›½é•¿æœŸå½±å“ã€‘\n{impact['long_term_china']}\n\n"
            summary_text += f"ã€å¯¹è‚¡å¸‚å½±å“ã€‘\n{impact['stock_market']}\n\n"
            summary_text += f"æ¥æº: {source}"
            
            summary_items.append({
                'title': title,
                'summary': summary_text,
                'url': url,
                'importance_score': item['score']
            })
        
        return summary_items
    
    def send_to_feishu(self, summary_items):
        """
        ä½¿ç”¨æ–°çš„åº”ç”¨è®¤è¯æ–¹å¼å‘é€æ¶ˆæ¯åˆ°é£ä¹¦
        """
        # å¯¼å…¥daily_news_botä¸­çš„send_to_feishuå‡½æ•°
        from daily_news_bot import send_to_feishu
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        message_parts = []
        for i, item in enumerate(summary_items, 1):
            message_parts.append(f"### {i}. [{item['title']}]({item['url']})")
            message_parts.append(item['summary'])
            message_parts.append("---")  # åˆ†éš”çº¿
        
        full_message = "\n".join(message_parts)
        
        try:
            success = send_to_feishu(full_message)
            if success:
                print("âœ… æˆåŠŸå‘é€æ–°é—»æ‘˜è¦åˆ°é£ä¹¦ç¾¤ï¼")
                return True
            else:
                print("âŒ å‘é€å¤±è´¥")
                return False
        except Exception as e:
            print(f"âŒ å‘é€é£ä¹¦æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return False
    
    def get_sample_news_data(self):
        """
        è·å–ç¤ºä¾‹æ–°é—»æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        """
        sample_articles = [
            {
                "title": "OpenAIå‘å¸ƒæ–°ä¸€ä»£GPTæ¨¡å‹ï¼Œæ€§èƒ½å¤§å¹…æå‡",
                "description": "OpenAIå®£å¸ƒæ¨å‡ºGPT-5æ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡å’Œå¤šæ¨¡æ€è¾“å…¥ã€‚",
                "url": "https://example.com/openai-gpt5",
                "source": {"name": "ç§‘æŠ€åª’ä½“"}
            },
            {
                "title": "ä¸­å›½AIèŠ¯ç‰‡ä¼ä¸šè·å¾—é‡å¤§æŠ€æœ¯çªç ´",
                "description": "å›½å†…æŸAIèŠ¯ç‰‡å…¬å¸å®£å¸ƒåœ¨7nmå·¥è‰ºä¸Šå–å¾—çªç ´ï¼Œå°†å¤§å¹…æå‡å›½äº§AIèŠ¯ç‰‡çš„è®¡ç®—èƒ½åŠ›ã€‚",
                "url": "https://example.com/china-ai-chip",
                "source": {"name": "æ–°åç½‘"}
            },
            {
                "title": "æ¬§ç›Ÿé€šè¿‡æ–°çš„äººå·¥æ™ºèƒ½ç›‘ç®¡æ³•æ¡ˆ",
                "description": "æ¬§ç›Ÿè®®ä¼šé€šè¿‡äº†å…¨é¢çš„äººå·¥æ™ºèƒ½ç›‘ç®¡æ¡†æ¶ï¼Œå¯¹é«˜é£é™©AIç³»ç»Ÿå®æ–½ä¸¥æ ¼ç®¡æ§ã€‚",
                "url": "https://example.com/eu-ai-regulation",
                "source": {"name": "è·¯é€ç¤¾"}
            },
            {
                "title": "AIåŒ»ç–—è¯Šæ–­ç³»ç»Ÿè·FDAæ‰¹å‡†",
                "description": "æ–°å‹AIåŒ»ç–—è¯Šæ–­ç³»ç»Ÿè·å¾—ç¾å›½FDAæ‰¹å‡†ï¼Œå¯ç”¨äºæ—©æœŸç™Œç—‡ç­›æŸ¥ï¼Œå‡†ç¡®ç‡è¾¾åˆ°95%ä»¥ä¸Šã€‚",
                "url": "https://example.com/ai-medical-fda",
                "source": {"name": "BBC"}
            },
            {
                "title": "è‡ªåŠ¨é©¾é©¶å‡ºç§Ÿè½¦åœ¨å¤šä¸ªåŸå¸‚å¼€å§‹è¯•ç‚¹è¿è¥",
                "description": "å¤šå®¶ç§‘æŠ€å…¬å¸å®£å¸ƒåœ¨åŒ—ä¸Šå¹¿æ·±ç­‰åŸå¸‚å¯åŠ¨è‡ªåŠ¨é©¾é©¶å‡ºç§Ÿè½¦è¯•ç‚¹æœåŠ¡ï¼Œç”¨æˆ·å¯é€šè¿‡APPé¢„çº¦ã€‚",
                "url": "https://example.com/autonomous-taxi",
                "source": {"name": "å¤®è§†æ–°é—»"}
            }
        ]
        return sample_articles
    
    def fetch_real_news(self):
        """
        ä»çœŸå®æ–°é—»æºè·å–æ•°æ®ï¼ˆéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥ï¼‰
        """
        # è¿™é‡Œå¯ä»¥é›†æˆçœŸå®çš„æ–°é—»API
        # ç”±äºAPIé™åˆ¶ï¼Œç›®å‰è¿”å›ç¤ºä¾‹æ•°æ®
        return self.get_sample_news_data()
    
    def run(self):
        """
        ä¸»æ‰§è¡Œå‡½æ•°
        """
        print("ğŸš€ å¼€å§‹è·å–å’Œåˆ†ææ–°é—»æ•°æ®...")
        
        # è·å–æ–°é—»æ•°æ®
        try:
            articles = self.fetch_real_news()
            print(f"è·å–åˆ° {len(articles)} æ¡æ–°é—»")
        except Exception as e:
            print(f"è·å–æ–°é—»å¤±è´¥ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®: {e}")
            articles = self.get_sample_news_data()
        
        if not articles:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            return False
        
        # ç”ŸæˆåŒ…å«å½±å“åˆ†æçš„æ‘˜è¦
        print("ğŸ“Š æ­£åœ¨åˆ†ææ–°é—»å½±å“...")
        summary_items = self.create_news_summary(articles)
        print(f"ç”Ÿæˆ {len(summary_items)} æ¡æ–°é—»æ‘˜è¦")
        
        if not summary_items:
            print("âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„æ–°é—»æ‘˜è¦")
            return False
        
        # å‘é€åˆ°é£ä¹¦
        print("ğŸ“¤ æ­£åœ¨å‘é€åˆ°é£ä¹¦ç¾¤...")
        success = self.send_to_feishu(summary_items)
        
        if success:
            print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼è¯·æ£€æŸ¥æ‚¨çš„é£ä¹¦ç¾¤ã€‚")
        else:
            print("âŒ ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
            
        return success

def main():
    analyzer = NewsAnalyzer()
    analyzer.run()

if __name__ == "__main__":
    main()