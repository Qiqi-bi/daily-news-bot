#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæ˜¾ç¤ºä»Šå¤©å·²å¤„ç†çš„æ–°é—»é“¾æ¥
"""

import json
import datetime
from urllib.parse import urlparse
from collections import defaultdict

def show_processed_news():
    """æ˜¾ç¤ºå·²å¤„ç†çš„æ–°é—»é“¾æ¥"""
    
    # è·å–ä»Šå¤©çš„æ—¥æœŸ
    today = datetime.date.today()
    today_str = today.strftime('%Y-%m-%d')
    
    print(f"ğŸ“° ä»Šæ—¥å·²å¤„ç†çš„æ–°é—»é“¾æ¥æ¦‚è§ˆ")
    print("=" * 60)
    print(f"ğŸ“… æ—¥æœŸ: {today_str}")
    print()
    
    # è¯»å–å†å²è®°å½•
    history_file = "history.json"
    try:
        with open(history_file, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
        
        if 'processed_urls' in history_data:
            processed_urls = history_data['processed_urls']
            
            print(f"ğŸ“Š æ€»è®¡å·²å¤„ç†æ–°é—»é“¾æ¥: {len(processed_urls)}")
            print()
            
            # æŒ‰åŸŸååˆ†ç»„URL
            domain_groups = defaultdict(list)
            for url in processed_urls:
                try:
                    parsed = urlparse(url)
                    domain = parsed.netloc
                    domain_groups[domain].append(url)
                except:
                    continue
            
            # æ˜¾ç¤ºæ¯ä¸ªåŸŸåçš„é“¾æ¥æ•°é‡
            print("ğŸ“ˆ æŒ‰æ¥æºåˆ†ç±»:")
            for domain, urls in sorted(domain_groups.items(), key=lambda x: len(x[1]), reverse=True):
                print(f"   â€¢ {domain}: {len(urls)} æ¡")
            
            print()
            print("ğŸ”— æœ€è¿‘å¤„ç†çš„30ä¸ªé“¾æ¥:")
            print("-" * 50)
            
            # æ˜¾ç¤ºæœ€è¿‘çš„é“¾æ¥
            recent_urls = processed_urls[-30:] if len(processed_urls) > 30 else processed_urls
            
            for i, url in enumerate(recent_urls, 1):
                print(f"{i:2d}. {url}")
            
            print()
            print("ğŸ’¡ æç¤º: è¿™äº›æ˜¯ç³»ç»Ÿä»Šå¤©å·²å¤„ç†çš„æ–°é—»é“¾æ¥ï¼ŒæŒ‰å¤„ç†é¡ºåºæ’åˆ—")
            print("   ç”±äºå†å²è®°å½•ä¸­æ²¡æœ‰æ—¶é—´æˆ³ï¼Œæ— æ³•ç²¾ç¡®ç¡®å®šæ¯ä¸ªé“¾æ¥çš„å¤„ç†æ—¶é—´")
            
        else:
            print("âš ï¸  å†å²è®°å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å·²å¤„ç†çš„URLåˆ—è¡¨")
    
    except FileNotFoundError:
        print(f"âš ï¸  æœªæ‰¾åˆ°å†å²è®°å½•æ–‡ä»¶: {history_file}")
        print("ğŸ’¡ æç¤º: ç³»ç»Ÿå¯èƒ½å°šæœªè¿è¡Œè¿‡æ–°é—»æŠ“å–ç¨‹åº")
    except json.JSONDecodeError:
        print(f"âš ï¸  å†å²è®°å½•æ–‡ä»¶æ ¼å¼é”™è¯¯: {history_file}")
    except Exception as e:
        print(f"âš ï¸  è¯»å–å†å²è®°å½•æ—¶å‡ºé”™: {e}")

def show_sample_news_analysis():
    """æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹æ–°é—»åˆ†ææ ¼å¼"""
    print("\nğŸ“‹ ç¤ºä¾‹æ–°é—»åˆ†ææ ¼å¼:")
    print("-" * 30)
    print("æ ‡é¢˜: [æ–°é—»æ ‡é¢˜]")
    print("æ¥æº: [æ–°é—»ç½‘ç«™]")
    print("æ—¶é—´: [å‘å¸ƒæ—¥æœŸ]")
    print("æ‘˜è¦: [AIç”Ÿæˆçš„æ‘˜è¦]")
    print("æƒ…ç»ª: [æ­£é¢/è´Ÿé¢/ä¸­æ€§]")
    print("é‡è¦åº¦: [1-10åˆ†]")
    print("æ ‡ç­¾: [å…³é”®è¯æ ‡ç­¾]")

if __name__ == "__main__":
    show_processed_news()
    show_sample_news_analysis()