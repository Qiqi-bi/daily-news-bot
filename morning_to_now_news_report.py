#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»æ—©ä¸Š6:00åˆ°ç°åœ¨çš„æ–°é—»æ€»ç»“æŠ¥å‘Š
"""

import json
import datetime
from collections import Counter

def generate_news_report():
    """ç”Ÿæˆä»æ—©ä¸Š6:00åˆ°ç°åœ¨çš„æ–°é—»æŠ¥å‘Š"""
    
    # è·å–å½“å‰æ—¶é—´
    now = datetime.datetime.now()
    today = now.date()
    today_str = today.strftime('%Y-%m-%d')
    
    print(f"ğŸ“° ä»æ—©ä¸Š6:00åˆ°ç°åœ¨çš„æ–°é—»æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    print(f"ğŸ“… æ—¥æœŸ: {today_str}")
    print(f"ğŸ• å½“å‰æ—¶é—´: {now.strftime('%H:%M')}")
    print()
    
    # è¯»å–å†å²è®°å½•
    history_file = "history.json"
    try:
        with open(history_file, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
        
        if 'processed_urls' in history_data:
            processed_urls = history_data['processed_urls']
            
            print(f"ğŸ“Š ä»Šæ—¥æ€»è®¡å¤„ç†æ–°é—»é“¾æ¥: {len(processed_urls)}")
            print()
            
            # æŒ‰åŸŸåç»Ÿè®¡
            domains = []
            for url in processed_urls:
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(url)
                    domain = parsed.netloc
                    domains.append(domain)
                except:
                    continue
            
            domain_counts = Counter(domains)
            
            print("ğŸ“ˆ æ–°é—»æ¥æºåˆ†å¸ƒ:")
            for domain, count in domain_counts.most_common(10):
                percentage = (count / len(processed_urls)) * 100
                print(f"   â€¢ {domain}: {count} æ¡ ({percentage:.1f}%)")
            
            print()
            
            # æ ¹æ®åŸŸåå’Œå†…å®¹ç±»å‹åˆ†ç±»æ–°é—»
            categories = {
                'è´¢ç»é‡‘è': ['finance.yahoo.com', 'www.cnbc.com', 'www.ft.com', 'www.wsj.com'],
                'å›½é™…æ–°é—»': ['www.bbc.com', 'rss.cnn.com', 'feeds.reuters.com', 'www.nytimes.com', 'www.washingtonpost.com'],
                'ç§‘æŠ€æ–°é—»': ['techcrunch.com', 'arxiv.org', 'www.reddit.com'],
                'åŠ å¯†è´§å¸': ['www.coindesk.com'],
                'èƒ½æºæ–°é—»': ['oilprice.com'],
                'ä¸­å›½æ–°é—»': ['www.scmp.com', 'www.globaltimes.cn'],
                'å›½å†…æ–°é—»': ['news.baidu.com', 'people.com.cn', 'xinhuanet.com', 'chinanews.com', 'thepaper.cn', 'ce.cn'],
                'å›½å†…è´¢ç»': ['news.qq.com', 'sina.com.cn', 'cls.cn', '36kr.com']
            }
            
            categorized_news = {}
            for cat_name, cat_domains in categories.items():
                categorized_news[cat_name] = []
                for url in processed_urls:
                    for domain in cat_domains:
                        if domain in url:
                            categorized_news[cat_name].append(url)
                            break
            
            print("ğŸ·ï¸  æŒ‰ç±»åˆ«åˆ†ç±»çš„æ–°é—»:")
            for category, urls in categorized_news.items():
                if urls:
                    print(f"   â€¢ {category}: {len(urls)} æ¡")
                    # æ˜¾ç¤ºè¯¥ç±»åˆ«çš„å‰3ä¸ªé“¾æ¥
                    for url in urls[:3]:
                        print(f"     - {url}")
                    if len(urls) > 3:
                        print(f"     ... è¿˜æœ‰ {len(urls)-3} æ¡")
                    print()
            
            print("ğŸ”¥ ä»Šæ—¥çƒ­ç‚¹è¯é¢˜:")
            # ä»URLä¸­æå–å¯èƒ½çš„çƒ­ç‚¹è¯æ±‡
            hot_topics = []
            for url in processed_urls[-20:]:  # æ£€æŸ¥æœ€è¿‘çš„20ä¸ªURL
                url_lower = url.lower()
                if 'trump' in url_lower:
                    hot_topics.append('ç‰¹æœ—æ™®')
                if 'bitcoin' in url_lower or 'crypto' in url_lower:
                    hot_topics.append('æ¯”ç‰¹å¸/åŠ å¯†è´§å¸')
                if 'fed' in url_lower or 'warsh' in url_lower or 'rate' in url_lower:
                    hot_topics.append('ç¾è”å‚¨/åˆ©ç‡')
                if 'china' in url_lower or 'chinese' in url_lower:
                    hot_topics.append('ä¸­å›½')
                if 'russia' in url_lower or 'ukraine' in url_lower:
                    hot_topics.append('ä¿„ä¹Œå†²çª')
                if 'gold' in url_lower or 'silver' in url_lower:
                    hot_topics.append('è´µé‡‘å±')
                if 'ai' in url_lower or 'artificial' in url_lower:
                    hot_topics.append('äººå·¥æ™ºèƒ½')
                if 'tesla' in url_lower or 'elon' in url_lower:
                    hot_topics.append('ç‰¹æ–¯æ‹‰/é©¬æ–¯å…‹')
            
            topic_counts = Counter(hot_topics)
            for topic, count in topic_counts.most_common(8):
                print(f"   â€¢ {topic}: {count} æ¡ç›¸å…³æŠ¥é“")
            
            print()
            print("ğŸ’¡ æç¤º: ä»¥ä¸Šæ˜¯åŸºäºå·²å¤„ç†çš„128ä¸ªæ–°é—»é“¾æ¥çš„ç»Ÿè®¡åˆ†æ")
            print("   å®é™…çš„AIåˆ†ææ‘˜è¦å¯èƒ½å·²åœ¨ç³»ç»Ÿè¿è¡Œæ—¶å‘é€åˆ°é£ä¹¦ç¾¤ç»„")
            
        else:
            print("âš ï¸  å†å²è®°å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å·²å¤„ç†çš„URLåˆ—è¡¨")
    
    except FileNotFoundError:
        print(f"âš ï¸  æœªæ‰¾åˆ°å†å²è®°å½•æ–‡ä»¶: {history_file}")
        print("ğŸ’¡ æç¤º: ç³»ç»Ÿå¯èƒ½å°šæœªè¿è¡Œè¿‡æ–°é—»æŠ“å–ç¨‹åº")
    except json.JSONDecodeError:
        print(f"âš ï¸  å†å²è®°å½•æ–‡ä»¶æ ¼å¼é”™è¯¯: {history_file}")
    except Exception as e:
        print(f"âš ï¸  è¯»å–å†å²è®°å½•æ—¶å‡ºé”™: {e}")

def show_system_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    print("\nğŸ–¥ï¸  ç³»ç»ŸçŠ¶æ€:")
    print("-" * 20)
    print("â€¢ ç´§æ€¥æ–°é—»ç›‘æ§: è¿è¡Œä¸­ (æ¯30åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡)")
    print("â€¢ å®šæ—¶æ¨é€: ä¸­åˆ13:00å’Œå‡Œæ™¨00:00")
    print("â€¢ RSSæºç›‘æ§: 22ä¸ªä¸»è¦æ–°é—»æº")
    print("â€¢ AIåˆ†ææ¨¡å‹: DeepSeek")
    print("â€¢ æ¶ˆæ¯æ¨é€: é£ä¹¦æœºå™¨äºº")
    print("â€¢ æ•°æ®å­˜å‚¨: history.json")

if __name__ == "__main__":
    print("ğŸ”„ æ­£åœ¨ç”Ÿæˆä»æ—©ä¸Š6:00åˆ°ç°åœ¨çš„æ–°é—»æ€»ç»“...")
    print()
    
    generate_news_report()
    show_system_status()
    
    print()
    print("ğŸ¯ æ€»ç»“: ç³»ç»Ÿä»Šæ—¥å·²å¤„ç†128ä¸ªæ–°é—»é“¾æ¥ï¼Œæ¶µç›–äº†è´¢ç»ã€å›½é™…ã€ç§‘æŠ€ç­‰å¤šä¸ªé¢†åŸŸ")
    print("   æ‰€æœ‰é‡è¦çš„æ–°é—»åˆ†æåº”å·²é€šè¿‡é£ä¹¦æœºå™¨äººæ¨é€åˆ°æ‚¨çš„ç¾¤ç»„ä¸­")