#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è·å–ä»Šå¤©ä»æ—©ä¸Š6:00å¼€å§‹çš„æ–°é—»æ‘˜è¦
ç”±äºå†å²è®°å½•ä¸­æ²¡æœ‰æ—¶é—´æˆ³ï¼Œæ­¤è„šæœ¬å°†ä»RSSæºè·å–æœ€æ–°çš„æ–°é—»
"""

import feedparser
import json
import datetime
import requests
from urllib.parse import urlparse
import time
import os

# RSSæºåˆ—è¡¨ï¼ˆä»fetch_news.pyä¸­æå–ï¼‰
RSS_SOURCES = [
    ("https://feeds.bbci.co.uk/news/world/rss.xml", "BBC World News"),
    ("https://rss.cnn.com/rss/edition.rss", "CNN International"),
    ("https://feeds.reuters.com/reuters/topNews", "Reuters Top News"),
    ("https://www.ft.com/rss/world", "Financial Times"),
    ("https://feeds.a.dj.com/rss/RSSWorldNews.xml", "Wall Street Journal"),
    ("https://rss.nytimes.com/services/xml/rss/nyt/InternationalHome.xml", "New York Times"),
    ("https://www.washingtonpost.com/rss/world/index.xml", "Washington Post"),
    ("https://feeds.skynews.com/feeds/rss/world.xml", "Sky News"),
    ("https://www.aljazeera.com/xml/rss/all.xml", "Al Jazeera"),
    ("https://rss.dw.com/xml/rss-en-all", "Deutsche Welle"),
    ("https://www.france24.com/en/rss", "France 24"),
    ("https://www.globaltimes.cn/rss_en/global.xml", "Global Times"),
    ("https://www.scmp.com/rss/9/feed", "South China Morning Post"),
    ("https://rss.app/feeds/8LoPANQoGAbJ3kQR.xml", "TechCrunch"),
    ("https://feeds.arxiv.org/list/cs.AI/recent", "ArXiv AI Papers"),
    ("https://www.coindesk.com/feed/", "CoinDesk"),
    ("https://oilprice.com/rss/main", "OilPrice.com"),
    ("https://www.reddit.com/r/worldnews/.rss", "Reddit World News"),
    ("https://www.reddit.com/r/videos/.rss", "Reddit Popular Videos"),
    ("https://www.hackernews.cc/rss", "Hacker News"),
    ("https://www.ycombinator.com/news/rss", "Y Combinator News"),
    ("https://rss.app/feeds/q1RDOaVg5zmjqboN.xml", "Yahoo Finance"),
    # å›½å†…ä¸»æµæ–°é—»æº
    ("http://news.baidu.com/n?cmd=file&format=rss&tn=rss&sub=0", "ç™¾åº¦æ–°é—»"),
    ("http://rss.people.com.cn/GB/303140/index.xml", "äººæ°‘ç½‘"),
    ("http://www.xinhuanet.com/politics/news_politics.xml", "æ–°åç½‘"),
    ("http://www.chinanews.com/rss/scroll-news.xml", "ä¸­å›½æ–°é—»ç½‘"),
    ("https://www.thepaper.cn/rss.jsp", "æ¾æ¹ƒæ–°é—»"),
    ("http://www.ce.cn/cysc/jg/zxbd/rss2.xml", "ä¸­å›½ç»æµç½‘"),
    # å›½å†…ç§‘æŠ€æ–°é—»
    ("https://www.zhihu.com/rss", "çŸ¥ä¹æ—¥æŠ¥"),
    ("https://www.36kr.com/feed", "36æ°ª"),
    ("https://news.qq.com/rss/channels/finance/rss.xml", "è…¾è®¯è´¢ç»"),
    ("https://rss.sina.com.cn/news/china/focus15.xml", "æ–°æµªæ–°é—»"),
]

def get_news_since_morning():
    """è·å–ä»ä»Šå¤©æ—©ä¸Š6:00å¼€å§‹çš„æ–°é—»"""
    
    # è·å–å½“å‰æ—¶é—´å’Œä»Šå¤©æ—©ä¸Š6:00çš„æ—¶é—´
    now = datetime.datetime.now()
    today_morning = datetime.datetime.combine(now.date(), datetime.time(6, 0))
    
    print(f"ğŸ“° ä»æ—©ä¸Š6:00åˆ°ç°åœ¨çš„æ–°é—»æ‘˜è¦")
    print("=" * 60)
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {today_morning.strftime('%Y-%m-%d %H:%M')} åˆ° {now.strftime('%Y-%m-%d %H:%M')}")
    print()
    
    all_articles = []
    
    print("ğŸ”„ æ­£åœ¨ä»RSSæºè·å–æ–°é—»...")
    
    for rss_url, source_name in RSS_SOURCES[:5]:  # åªè·å–å‰5ä¸ªä¸»è¦RSSæºä»¥èŠ‚çœæ—¶é—´
        try:
            print(f"   ğŸ“¡ è·å– {source_name}...")
            
            # è§£æRSSæº
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries:
                try:
                    # è§£æå‘å¸ƒæ—¶é—´
                    pub_date = None
                    if hasattr(entry, 'published_parsed'):
                        if entry.published_parsed:
                            pub_date = datetime.datetime(*entry.published_parsed[:6])
                        else:
                            # å¦‚æœæ²¡æœ‰è§£æçš„æ—¥æœŸï¼Œå°è¯•ä»publishedå­—ç¬¦ä¸²è§£æ
                            if hasattr(entry, 'published'):
                                try:
                                    # å°è¯•å‡ ç§å¸¸è§çš„æ—¥æœŸæ ¼å¼
                                    date_str = entry.published
                                    formats = [
                                        '%a, %d %b %Y %H:%M:%S %z',
                                        '%a, %d %b %Y %H:%M:%S',
                                        '%Y-%m-%dT%H:%M:%SZ',
                                        '%Y-%m-%dT%H:%M:%S.%fZ',
                                        '%Y-%m-%d %H:%M:%S'
                                    ]
                                    
                                    for fmt in formats:
                                        try:
                                            pub_date = datetime.datetime.strptime(date_str.split('+')[0].split('-')[0], fmt)
                                            break
                                        except ValueError:
                                            continue
                                except:
                                    pass
                    
                    # å¦‚æœæ— æ³•è§£ææ—¥æœŸï¼Œè·³è¿‡æ­¤æ¡ç›®
                    if pub_date is None:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                    if today_morning <= pub_date <= now:
                        article = {
                            'title': getattr(entry, 'title', 'æ— æ ‡é¢˜'),
                            'link': getattr(entry, 'link', ''),
                            'description': getattr(entry, 'summary', ''),
                            'published': pub_date,
                            'source': source_name
                        }
                        all_articles.append(article)
                
                except Exception as e:
                    print(f"     âš ï¸  å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                    continue
            
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            time.sleep(1)
        
        except Exception as e:
            print(f"   âŒ è·å– {source_name} æ—¶å‡ºé”™: {e}")
            continue
    
    # æŒ‰æ—¶é—´æ’åº
    all_articles.sort(key=lambda x: x['published'], reverse=True)
    
    if all_articles:
        print()
        print(f"ğŸ“Š æ‰¾åˆ° {len(all_articles)} æ¡ç¬¦åˆæ¡ä»¶çš„æ–°é—»:")
        print()
        
        for i, article in enumerate(all_articles, 1):
            print(f"{i}. ğŸ• {article['published'].strftime('%H:%M')}")
            print(f"   ğŸ“ {article['title']}")
            print(f"   ğŸ¢ æ¥æº: {article['source']}")
            if article['link']:
                print(f"   ğŸ”— {article['link']}")
            if article['description']:
                desc = article['description'].replace('<[^<]+?>', '').replace('\n', ' ')[:200] + "..."
                print(f"   ğŸ“„ æ‘˜è¦: {desc}")
            print()
    else:
        print()
        print("ğŸ” åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
        print()
        print("ğŸ’¡ æç¤º: å¯èƒ½æ˜¯å› ä¸ºRSSæºä¸­çš„æ–°é—»å‘å¸ƒæ—¶é—´ä¸åœ¨ä»Šå¤©æ—©ä¸Š6:00ä¹‹å")
        print("   æˆ–è€…RSSæºæš‚æ—¶ä¸å¯ç”¨")
    
    # æ˜¾ç¤ºå†å²è®°å½•ä¸­çš„æ€»é“¾æ¥æ•°
    history_file = "history.json"
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
            
            if 'processed_urls' in history_data:
                total_links = len(history_data['processed_urls'])
                print(f"ğŸ“ˆ ç³»ç»Ÿæ€»è®¡å·²å¤„ç†æ–°é—»é“¾æ¥: {total_links}")
            else:
                print("ğŸ“ˆ æ— æ³•ç»Ÿè®¡å·²å¤„ç†çš„æ–°é—»é“¾æ¥æ•°é‡")
        except:
            print("ğŸ“ˆ æ— æ³•è¯»å–å†å²è®°å½•æ–‡ä»¶")

def get_recent_processed_links():
    """æ˜¾ç¤ºæœ€è¿‘å¤„ç†çš„é“¾æ¥ï¼ˆä»history.jsonï¼‰"""
    history_file = "history.json"
    
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
            
            if 'processed_urls' in history_data:
                processed_urls = history_data['processed_urls']
                
                print(f"\nğŸ“‹ æœ€è¿‘å¤„ç†çš„20ä¸ªé“¾æ¥:")
                print("-" * 40)
                
                # æ˜¾ç¤ºæœ€è¿‘çš„20ä¸ªé“¾æ¥
                recent_urls = processed_urls[-20:] if len(processed_urls) > 20 else processed_urls
                
                for i, url in enumerate(recent_urls, 1):
                    print(f"{i:2d}. {url}")
                
                print(f"\nğŸ’¡ æ€»è®¡å·²å¤„ç† {len(processed_urls)} ä¸ªé“¾æ¥")
        except Exception as e:
            print(f"âš ï¸  è¯»å–å†å²è®°å½•æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    print("ğŸ”„ æ­£åœ¨æœç´¢ä»æ—©ä¸Š6:00åˆ°ç°åœ¨çš„æ–°é—»...")
    print()
    
    get_news_since_morning()
    get_recent_processed_links()
    
    print()
    print("ğŸ’¡ æ³¨æ„: ç”±äºå†å²è®°å½•ä¸­æ²¡æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œæ­¤è„šæœ¬ç›´æ¥ä»RSSæºè·å–æœ€æ–°çš„æ–°é—»")
    print("   ä»¥ç¡®å®šå“ªäº›æ–°é—»æ˜¯åœ¨ä»Šå¤©æ—©ä¸Š6:00ä¹‹åå‘å¸ƒçš„")