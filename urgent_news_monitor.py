#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç´§æ€¥æ–°é—»ç›‘æ§æœºå™¨äºº - é«˜é¢‘ç›‘æ§é‡è¦äº‹ä»¶è„šæœ¬

åŠŸèƒ½ï¼š
1. æ¯30åˆ†é’Ÿä»RSSæºæŠ“å–æœ€æ–°æ–°é—»
2. å¿«é€Ÿè°ƒç”¨DeepSeekå¤§æ¨¡å‹APIè¿›è¡Œæƒ…ç»ªè¯„åˆ†
3. åªæœ‰å½“Abs(æƒ…ç»ªåˆ†) >= 9ï¼ˆæåº¦é‡è¦ï¼‰æ—¶ï¼Œæ‰è§¦å‘æ¨é€
4. å¹³æ—¶ä¸æ‰“æ‰°ï¼Œä¸€æ—¦å“é“ƒï¼Œå¿…æ˜¯å¤§äº‹

ä½œè€…ï¼šPythoné«˜çº§å·¥ç¨‹å¸ˆ
"""

import feedparser
import requests
import json
import time
import logging
from typing import List, Dict, Optional
from urllib.parse import urljoin
import os
import re

# ==================== é…ç½®åŒºåŸŸ ====================
# è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ä»¥ä¸‹é…ç½®

# LLM API é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
API_KEY = os.environ.get('DEEPSEEK_API_KEY', '')
BASE_URL = "https://api.deepseek.com"

# æ™ºèƒ½ä»£ç†é…ç½® - æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­
if os.environ.get('GITHUB_ACTIONS'):
    # åœ¨GitHub Actionsä¸­ï¼Œç›´æ¥è¿æ¥
    PROXIES = None
else:
    # æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨ä»£ç†
    PROXIES = {
        'http': 'http://127.0.0.1:7897',
        'https': 'http://127.0.0.1:7897'
    }

# RSSæºåˆ—è¡¨ï¼ˆç´§æ€¥ç›‘æ§ä¸“ç”¨ï¼Œä½¿ç”¨å®Œæ•´çš„æ–°é—»æºåˆ—è¡¨ï¼‰
URGENT_RSS_SOURCES = [
    # --- 1. å›½é™…é¡¶æµ (BBC/NYT) ---
    "https://feeds.bbci.co.uk/news/world/rss.xml",  # BBC World
    "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",  # NYT World
    
    # --- 2. åå°”è¡—/é‡‘è (CNBC) ---
    "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664",  # CNBC Finance

    # --- 3. ç¡…è°·/ç§‘æŠ€ (TechCrunch) ---
    "https://techcrunch.com/feed/",  # TechCrunch AI & Startup

    # --- 4. é›…è™è´¢ç» (æ–°å¢) ---
    "https://finance.yahoo.com/news/rssindex",  # Yahoo Finance

    # --- 5. åŠ å¯†è´§å¸ (Crypto) ---
    "https://www.coindesk.com/arc/outboundfeeds/rss/",  # CoinDesk
    "https://cointelegraph.com/rss",  # Cointelegraph
    "https://crypto-slate.com/feed/",  # Crypto Slate

    # --- 6. èƒ½æºä¸æˆ˜äº‰ (Energy) ---
    "https://oilprice.com/rss/main",  # OilPrice.com

    # --- 7. ç¤¾äº¤ä¸é»‘å®¢åŠ¨å‘ (æ›¿ä»£ Twitter/GitHub) ---
    # Hacker News (å…¨çƒæå®¢éƒ½åœ¨è®¨è®ºä»€ä¹ˆï¼Œæ˜¯ GitHub æœ€å¥½çš„é£å‘æ ‡)
    "https://news.ycombinator.com/rss",
    # Reddit WorldNews (å…¨çƒç½‘æ°‘æœ€çƒ­è®®çš„çªå‘äº‹ä»¶)
    "https://www.reddit.com/r/worldnews/top/.rss?t=day",
    
    # --- 8. Reddit è§†é¢‘èšåˆ (æ–°å¢) ---
    "https://www.reddit.com/r/videos/top/.rss?t=day",  # Reddit è§†é¢‘èšåˆ - å…¨çƒ24å°æ—¶å†…æœ€çƒ­é—¨çš„è§†é¢‘é›†åˆ
    
    # --- 9. äºšæ´²/ä¸­å›½å•†ä¸š (æ–°å¢) ---
    "https://www.scmp.com/rss/2/feed",  # South China Morning Post (å—åæ—©æŠ¥ - ä¸­å›½å•†ä¸šç‰ˆå—)
    
    # --- 10. å­¦æœ¯/AIç ”ç©¶ (æ–°å¢) ---
    "http://arxiv.org/rss/cs.AI",  # ArXiv AI Paper Daily (å­¦æœ¯æº)
    "https://mittechnologyreview.com/feed/",  # MIT Technology Review (ç§‘æŠ€è¶‹åŠ¿åˆ†æ)
    
    # --- 11. å›½å†…ä¸»æµæ–°é—»æº (æ–°å¢) ---
    "http://news.baidu.com/n?cmd=file&format=rss&tn=rss&sub=0",  # ç™¾åº¦æ–°é—»
    "http://rss.people.com.cn/GB/303140/index.xml",  # äººæ°‘ç½‘
    "http://www.xinhuanet.com/politics/news_politics.xml",  # æ–°åç½‘ - æ—¶æ”¿
    "http://www.chinanews.com/rss/scroll-news.xml",  # ä¸­å›½æ–°é—»ç½‘
    "https://www.thepaper.cn/rss.jsp",  # æ¾æ¹ƒæ–°é—»
    "http://www.ce.cn/cysc/jg/zxbd/rss2.xml",  # ä¸­å›½ç»æµç½‘
    "https://www.cls.cn/v3/highlights?app_id=70301d300f0f95a1&platform=pc",  # è´¢è”ç¤¾ (éœ€è¦é€‚é…)
    
    # --- 12. å›½å†…ç§‘æŠ€æ–°é—» (æ–°å¢) ---
    "https://www.zhihu.com/rss",  # çŸ¥ä¹æ¯æ—¥ç²¾é€‰
    "https://www.36kr.com/feed",  # 36æ°ª
    "https://news.qq.com/rss/channels/finance/rss.xml",  # è…¾è®¯è´¢ç»
    "https://rss.sina.com.cn/news/china/focus15.xml",  # æ–°æµªæ–°é—»-å›½å†…ç„¦ç‚¹

    # --- 13. ä¸»è¦ç§‘æŠ€å…¬å¸å®˜ç½‘ (æ–°å¢) ---
    "https://blog.google/rss/",  # Google Blog
    "https://openai.com/blog/rss/",  # OpenAI Blog
    "https://blogs.microsoft.com/feed/",  # Microsoft Blog
    "https://www.apple.com/newsroom/rss-feed.rss",  # Apple Newsroom
    "https://nvidianews.nvidia.com/rss.xml",  # NVIDIA Newsroom
    "https://about.meta.com/rss/feed/",  # Meta Newsroom

    # --- 14. ä¸»æµè´¢ç»åª’ä½“ (æ–°å¢) ---
    "https://feeds.reuters.com/reuters/topNews",  # Reuters Top News
    "https://feeds.reuters.com/reuters/businessNews",  # Reuters Business
    "https://feeds.reuters.com/reuters/technologyNews",  # Reuters Technology
    "https://bloomberg.com/feed",  # Bloomberg (å¯èƒ½éœ€è¦é€‚é…)
    "https://www.wsj.com/xml/rss/3_7085.xml",  # Wall Street Journal (å¯èƒ½éœ€è¦é€‚é…)

    # --- 15. ç§‘æŠ€åª’ä½“ (æ–°å¢) ---
    "https://www.theverge.com/rss/index.xml",  # The Verge
    "https://arstechnica.com/feed/",  # Ars Technica

    # --- 16. æŠ•èµ„æœºæ„å’Œæ•°æ®åº“ (æ–°å¢) ---
    "https://www.cbinsights.com/blog/feed/",  # CB Insights
    "https://techcrunch.com/startups/",  # TechCrunch Startups
    "https://www.crunchbase.com/feed",  # Crunchbase (å¯èƒ½éœ€è¦é€‚é…)

    # --- 17. AIç ”ç©¶æœºæ„ (æ–°å¢) ---
    "https://stability.ai/rss",  # Stability AI
    "https://huggingface.co/blog/feed.xml",  # Hugging Face Blog

    # --- 18. å•†ä¸šé¢†è¢–å’Œä¼ä¸šé«˜ç®¡ (æ–°å¢) ---
    "https://www.tesla.com/blog/rss",  # Tesla Blog
    "https://about.twitter.com/content/dam/about-twitter/company/news/rss-feeds/official-company-blog-rss.xml",  # Twitter Blog (X)
    "https://www.spacex.com/static/releases/feed.xml",  # SpaceX Releases

    # --- 19. åŠ å¯†è´§å¸å’ŒåŒºå—é“¾ (æ–°å¢) ---
    "https://cointelegraph.com/feed",  # Cointelegraph
    "https://decrypt.co/feed",  # Decrypt
    "https://messari.io/feed.xml",  # Messari
    "https://theblock.co/rss",  # The Block

    # --- 20. äº¤æ˜“å’ŒæŠ•èµ„å¹³å° (æ–°å¢) ---
    "https://www.binance.com/en/blog/rss",  # Binance Blog
    "https://blog.coinbase.com/feed",  # Coinbase Blog

    # --- 21. åŒºå—é“¾åè®® (æ–°å¢) ---
    "https://blog.ethereum.org/feed.xml",  # Ethereum Blog
    "https://polkadot.network/feed/",  # Polkadot Blog

    # --- 22. é‡‘èå’ŒæŠ•èµ„ (æ–°å¢) ---
    "https://seekingalpha.com/feed.xml",  # Seeking Alpha
    "https://www.ft.com/?format=rss",  # Financial Times (å¯èƒ½éœ€è¦é€‚é…)

    # --- 23. äºšé©¬é€Šç›¸å…³ (æ–°å¢) ---
    "https://www.aboutamazon.com/news/rss-feed.xml",  # Amazon Newsroom

    # --- 24. é©¬æ–¯å…‹ç›¸å…³ (æ–°å¢) ---
    "https://www.neuralink.com/blog.rss",  # Neuralink Blog
    "https://www.boringcompany.com/blog",  # The Boring Company Blog (å¯èƒ½éœ€è¦é€‚é…)

    # --- 25. å…¶ä»–AIå…¬å¸ (æ–°å¢) ---
    "https://www.anthropic.com/rss",  # Anthropic Blog
    "https://deepmind.google/rss/",  # DeepMind Blog
    "https://aws.amazon.com/blogs/aws/feed/",  # AWS Blog
    "https://www.amd.com/en/press-room/press-releases.rss",  # AMD Press Releases
]

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== ç¼“å­˜ç®¡ç† ====================

def load_cache() -> set:
    """
    ä»history.jsonåŠ è½½å·²å¤„ç†çš„URLç¼“å­˜
    """
    if os.path.exists('history.json'):
        try:
            with open('history.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('processed_urls', []))
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return set()
    else:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ç¼“å­˜æ–‡ä»¶
        save_cache(set())
        return set()

def save_cache(processed_urls: set):
    """
    ä¿å­˜å·²å¤„ç†çš„URLåˆ°history.json
    """
    try:
        with open('history.json', 'w', encoding='utf-8') as f:
            json.dump({'processed_urls': list(processed_urls)}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def send_to_feishu(message: str, max_retries: int = MAX_RETRIES) -> bool:
    """
    ä½¿ç”¨é£ä¹¦webhookå‘é€æ¶ˆæ¯åˆ°ç¾¤ç»„ï¼ˆå¯Œæ–‡æœ¬æ ¼å¼ï¼‰
    
    Args:
        message: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        å‘é€æ˜¯å¦æˆåŠŸ
    """
    # ä»ç¯å¢ƒå˜é‡è·å–webhook URLï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å ä½ç¬¦
    webhook_url = os.environ.get('FEISHU_WEBHOOK_URL', '')
    # å‡†å¤‡æ¶ˆæ¯å†…å®¹ï¼ˆè½¬æ¢ä¸ºé€‚åˆå¯Œæ–‡æœ¬çš„æ ¼å¼ï¼‰
    # ç§»é™¤å¯èƒ½å¼•èµ·é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦å’Œæ ¼å¼ï¼Œä¼˜åŒ–æ’ç‰ˆ
    clean_message = message.replace('\ud83d', '').replace('\ude0a', '')  # ç§»é™¤æŸäº›emoji
    clean_message = clean_message.replace('---', '\nâ”€â”€â”€â”€â”€â”€\n')  # åªä¿ç•™ä¸€æ¡ç®€æ´çš„åˆ†éš”çº¿
    clean_message = clean_message.replace('####', '###')  # ç»Ÿä¸€æ ‡é¢˜å±‚çº§
    clean_message = clean_message.replace('###', '\nâ— ')  # å°†ä¸‰çº§æ ‡é¢˜æ”¹ä¸ºåœ†ç‚¹
    clean_message = clean_message.replace('##', '\nâ—† ')  # å°†äºŒçº§æ ‡é¢˜æ”¹ä¸ºè±å½¢ç¬¦å·
    clean_message = clean_message.replace('#', '\nâ˜… ')  # å°†ä¸€çº§æ ‡é¢˜æ”¹ä¸ºæ˜Ÿå·

    # æ„å»ºå¯Œæ–‡æœ¬æ¶ˆæ¯ï¼ˆä½¿ç”¨interactiveç±»å‹å®ç°å¡ç‰‡æ•ˆæœï¼‰
    data = {
        "msg_type": "interactive",
        "card": {
            "config": {
                "wide_screen_mode": True
            },
            "header": {
                "template": "red",  # ç´§æ€¥äº‹ä»¶ä½¿ç”¨çº¢è‰²æ ‡é¢˜
                "title": {
                    "content": "ğŸš¨ å…¨çƒç´§æ€¥äº‹ä»¶è­¦æŠ¥",
                    "tag": "plain_text"
                }
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": clean_message
                },
                {
                    "tag": "note",
                    "elements": [
                        {
                            "tag": "plain_text",
                            "content": "ğŸ¤– DeepSeek-V3 æ™ºèƒ½åˆ†æç³»ç»Ÿ | ğŸ“… " + time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                    ]
                }
            ]
        }
    }

    headers = {
        "Content-Type": "application/json; charset=utf-8"
    }

    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨å‘é€ç´§æ€¥æ¶ˆæ¯åˆ°é£ä¹¦webhook (å°è¯• {attempt + 1}/{max_retries})")
            response = requests.post(webhook_url, headers=headers, json=data, timeout=10)
            if response.status_code == 200:
                result = response.json()
                if result.get('StatusCode') == 0 or result.get('code') == 0:
                    logger.info("ğŸš¨ ç´§æ€¥æ¶ˆæ¯æˆåŠŸå‘é€åˆ°é£ä¹¦ï¼")
                    return True
                else:
                    logger.error(f"é£ä¹¦webhookè¿”å›é”™è¯¯: {result.get('msg') or result.get('message')}")
            else:
                logger.error(f"HTTPé”™è¯¯: {response.status_code} - {response.text}")
        except Exception as e:
            logger.error(f"å‘é€é£ä¹¦webhookæ¶ˆæ¯å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY)
    
    logger.error("âŒ ç´§æ€¥æ¶ˆæ¯å‘é€æœ€ç»ˆå¤±è´¥")
    return False

def send_error_alert(error_message: str, max_retries: int = MAX_RETRIES):
    """
    å‘é€é”™è¯¯è­¦æŠ¥åˆ°é£ä¹¦ï¼ˆä½¿ç”¨webhookæ–¹å¼ï¼‰
    """
    # æ„å»ºé”™è¯¯è­¦æŠ¥æ¶ˆæ¯
    alert_msg = f"ğŸš¨ æœºå™¨äººæ•…éšœè­¦æŠ¥\n\né”™è¯¯è¯¦æƒ…ï¼š{error_message}\n\nè¯·åŠæ—¶æ£€æŸ¥æœºå™¨äººçŠ¶æ€ï¼\n\nDeepSeek-V3 ç›‘æ§ç³»ç»Ÿ"
    
    # ä»ç¯å¢ƒå˜é‡è·å–webhook URLï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å ä½ç¬¦
    webhook_url = os.environ.get('FEISHU_WEBHOOK_URL', '')
    # æ„å»ºå¯Œæ–‡æœ¬æ¶ˆæ¯ï¼ˆä½¿ç”¨interactiveç±»å‹å®ç°å¡ç‰‡æ•ˆæœï¼‰
    data = {
        "msg_type": "interactive",
        "card": {
            "config": {
                "wide_screen_mode": True
            },
            "header": {
                "template": "red",  # ç´§æ€¥äº‹ä»¶ä½¿ç”¨çº¢è‰²æ ‡é¢˜
                "title": {
                    "content": "ğŸš¨ æœºå™¨äººæ•…éšœè­¦æŠ¥",
                    "tag": "plain_text"
                }
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": alert_msg
                },
                {
                    "tag": "note",
                    "elements": [
                        {
                            "tag": "plain_text",
                            "content": "ğŸ¤– DeepSeek-V3 ç›‘æ§ç³»ç»Ÿ | ğŸ“… " + time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                    ]
                }
            ]
        }
    }

    headers = {
        "Content-Type": "application/json; charset=utf-8"
    }

    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨å‘é€é”™è¯¯è­¦æŠ¥åˆ°é£ä¹¦webhook (å°è¯• {attempt + 1}/{max_retries})")
            response = requests.post(webhook_url, headers=headers, json=data, timeout=10)
            if response.status_code == 200:
                result = response.json()
                if result.get('StatusCode') == 0 or result.get('code') == 0:
                    logger.info("ğŸš¨ é”™è¯¯è­¦æŠ¥æˆåŠŸå‘é€åˆ°é£ä¹¦ï¼")
                    return True
                else:
                    logger.error(f"é£ä¹¦webhookè¿”å›é”™è¯¯: {result.get('msg') or result.get('message')}")
            else:
                logger.error(f"HTTPé”™è¯¯: {response.status_code} - {response.text}")
        except Exception as e:
            logger.error(f"å‘é€é£ä¹¦webhooké”™è¯¯è­¦æŠ¥å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY)
    logger.error("âŒ é”™è¯¯è­¦æŠ¥å‘é€å¤±è´¥")
    return False

# ==================== æ ¸å¿ƒåŠŸèƒ½æ¨¡å— ====================

def fetch_rss_feed(url: str, max_retries: int = MAX_RETRIES) -> Optional[feedparser.FeedParserDict]:
    """
    ä»RSSæºæŠ“å–æ–°é—»ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œä»£ç†æ”¯æŒ
    
    Args:
        url: RSSæºURL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        feedparserè§£æç»“æœæˆ–None
    """
    for attempt in range(max_retries):
        try:
            logger.info(f"æ­£åœ¨æŠ“å–RSSæº: {url} (å°è¯• {attempt + 1}/{max_retries})")
            
            # ä½¿ç”¨ä»£ç†æŠ“å–RSSï¼ˆå¦‚æœå¯ç”¨ï¼‰
            response = requests.get(
                url, 
                proxies=PROXIES, 
                timeout=10,
                headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            )
            response.raise_for_status()
            
            # è§£æRSS
            feed = feedparser.parse(response.content)
            logger.info(f"æˆåŠŸæŠ“å– {len(feed.entries)} æ¡æ–°é—»")
            return feed
            
        except Exception as e:
            logger.warning(f"æŠ“å–RSSå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(RETRY_DELAY)
            else:
                logger.error(f"RSSæŠ“å–æœ€ç»ˆå¤±è´¥: {url}")
                return None
    
    return None

def extract_urgent_news_items() -> List[Dict[str, str]]:
    """
    ä»å¤šä¸ªRSSæºæå–ç´§æ€¥æ–°é—»æ¡ç›®ï¼ˆåªå–æœ€æ–°çš„5æ¡ï¼‰
    
    Returns:
        æ–°é—»æ¡ç›®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«title, summary, link
    """
    all_news = []
    seen_titles = set()  # ç”¨äºå»é‡
    processed_urls = load_cache()  # åŠ è½½å·²å¤„ç†çš„URLç¼“å­˜
    
    for rss_url in URGENT_RSS_SOURCES:
        feed = fetch_rss_feed(rss_url)
        if not feed or not feed.entries:
            continue
            
        # åªå–æœ€æ–°çš„5æ¡æ–°é—»
        for entry in feed.entries[:5]:
            title = entry.get('title', '').strip()
            summary = entry.get('summary', '').strip()
            link = entry.get('link', '')
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡
            if link in processed_urls:
                continue
            
            # å»é‡æ£€æŸ¥
            if not title or title in seen_titles:
                continue
                
            seen_titles.add(title)
            
            # æ¸…ç†summaryä¸­çš„HTMLæ ‡ç­¾
            import re
            summary = re.sub(r'<[^>]+>', '', summary)
            summary = summary[:200] + '...' if len(summary) > 200 else summary
            
            all_news.append({
                'title': title,
                'summary': summary,
                'link': link
            })
    
    # æ›´æ–°ç¼“å­˜ï¼Œæ·»åŠ æ–°å¤„ç†çš„URL
    for item in all_news:
        processed_urls.add(item['link'])
    save_cache(processed_urls)
    
    logger.info(f"æ€»å…±æå–åˆ° {len(all_news)} æ¡å”¯ä¸€ç´§æ€¥æ–°é—»")
    return all_news[:5]  # æœ€å¤šå¤„ç†5æ¡ç´§æ€¥æ–°é—»

def analyze_urgent_news_with_llm(news_items: List[Dict[str, str]]) -> tuple:
    """
    è°ƒç”¨LLM APIå¯¹ç´§æ€¥æ–°é—»è¿›è¡Œå¿«é€Ÿæƒ…ç»ªè¯„åˆ†
    
    Args:
        news_items: æ–°é—»æ¡ç›®åˆ—è¡¨
        
    Returns:
        (åˆ†æç»“æœ, æƒ…ç»ªåˆ†æ•°åˆ—è¡¨)
    """
    if not news_items:
        return "æš‚æ— ç´§æ€¥æ–°é—»ã€‚", []
    
    # æ„å»ºæ–°é—»å†…å®¹
    news_content = ""
    for i, item in enumerate(news_items):
        news_content += f"**ID**: {i+1}\n**æ ‡é¢˜**: {item['title']}\n**æ‘˜è¦**: {item['summary']}\n**é“¾æ¥**: {item['link']}\n\n"
    
    # ç´§æ€¥ç›‘æ§çš„ç³»ç»Ÿæç¤ºè¯ - ä¸“æ³¨å¿«é€Ÿæƒ…ç»ªè¯„åˆ†
    system_prompt = """ä½ æ˜¯ä¸€åé¡¶çº§æ¸¸èµ„æ“ç›˜æ‰‹å’Œå®è§‚ç­–ç•¥å¸ˆã€‚ä½ çš„è¯»è€…æ˜¯æ—¶é—´å®è´µçš„ä¸­å›½æŠ•èµ„è€…/æ‰“å·¥äººã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼š**é€è¿‡æ–°é—»è¡¨è±¡ï¼Œç›´æ¥æ‹†è§£åˆ©ç›Šé“¾æ¡ï¼Œç»™å‡ºæœ€å†·è¡€çš„åˆ¤æ–­ã€‚**

# Constraints
1. **è¯¦ç»†åˆ†æ**ï¼šå…¨ç¯‡æ§åˆ¶åœ¨ 600 å­—å·¦å³ï¼Œæä¾›æ·±å…¥çš„åˆ†æå’Œè§è§£ã€‚
2. **é€šä¿—æ˜“æ‡‚**ï¼šä½¿ç”¨ç®€å•æ˜äº†çš„è¯­è¨€ï¼Œé¿å…å¤æ‚çš„ç®­å¤´ç¬¦å·ï¼Œè®©æ™®é€šç”¨æˆ·ä¹Ÿèƒ½ç†è§£ã€‚
3. **æ ¼å¼ä¸¥æ ¼**ï¼šå¿…é¡»éµå®ˆä¸‹æ–¹çš„ Markdown æ ¼å¼ã€‚
4. **ä¸­å›½è§†è§’**ï¼šæ‰€æœ‰å½±å“åˆ†æå¿…é¡»ç´§æ‰£ä¸­å›½å›½è¿ã€Aè‚¡/æ¸¯è‚¡å’Œæ‰“å·¥äººçš„é’±è¢‹å­ã€‚
5. **ä¸¥ç¦æè¿°å›¾ç‰‡**ï¼šä¸è¦è¾“å‡ºä»»ä½•å›¾ç‰‡æè¿°ã€‚

# Analysis Framework (Markdown Output)
è¯·å¯¹ç­›é€‰å‡ºçš„ Top æ–°é—»æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼š

### [æƒ…ç»ªåˆ† | åˆ†æ•°] æ–°é—»æ ‡é¢˜ (ä¸­æ–‡ï¼ŒåŠ ç²—)

> [ğŸ”— ç‚¹å‡»ç›´è¾¾åŸæ–°é—»](æ–°é—»URL) | æ¥æºï¼šæ–°é—»Source

**æ ¸å¿ƒè¦ç‚¹**ï¼šä¸€å¥è¯æ¦‚æ‹¬æ–°é—»çš„æ ¸å¿ƒå†…å®¹

**äº‹ä»¶è¯¦æƒ…**ï¼šè¯¦ç»†ä»‹ç»å‘ç”Ÿäº†ä»€ä¹ˆäº‹æƒ…ï¼Œæ¶‰åŠå“ªäº›å…³é”®äººç‰©ã€å…¬å¸æˆ–ç»„ç»‡ï¼Œä»¥åŠå…·ä½“çš„æ—¶é—´ã€åœ°ç‚¹ã€æ•°æ®ç­‰ã€‚

**æ·±å±‚è§£è¯»**ï¼šæ·±å…¥åˆ†æè¿™åˆ™æ–°é—»èƒŒåçš„åŠ¨æœºå’ŒåŸå› ã€‚ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µï¼Ÿæ˜¯å‡ºäºå•†ä¸šè€ƒè™‘ã€æ”¿ç­–é©±åŠ¨ã€å¸‚åœºç«äº‰è¿˜æ˜¯æŠ€æœ¯çªç ´ï¼Ÿè§£é‡Šæ¸…æ¥šäº‹ä»¶å‘ç”Ÿçš„æ ¹æœ¬åŸå› ã€‚

**å¯¹ä¸­å›½çš„æ½œåœ¨å½±å“**ï¼š
- **çŸ­æœŸå½±å“**ï¼šå¯¹ä¸­å›½ç»æµã€é‡‘èå¸‚åœºã€ç›¸å…³è¡Œä¸šæˆ–æ¶ˆè´¹è€…çš„ç›´æ¥å½±å“
- **é•¿æœŸå½±å“**ï¼šå¯¹æœªæ¥å‘å±•è¶‹åŠ¿ã€äº§ä¸šå¸ƒå±€ã€å›½é™…åœ°ä½ç­‰æ–¹é¢çš„æ·±è¿œå½±å“

**å¯¹è‚¡å¸‚å’ŒæŠ•èµ„çš„å½±å“**ï¼š
- **å¯èƒ½å—ç›Šçš„æ¿å—æˆ–è‚¡ç¥¨**ï¼šåˆ—å‡ºå¯èƒ½å› æ­¤å—ç›Šçš„è¡Œä¸šã€å…¬å¸æˆ–æŠ•èµ„æ ‡çš„
- **å¯èƒ½å—æŸçš„æ¿å—æˆ–è‚¡ç¥¨**ï¼šæŒ‡å‡ºå¯èƒ½é¢ä¸´è´Ÿé¢å½±å“çš„é¢†åŸŸ
- **æŠ•èµ„ç­–ç•¥å»ºè®®**ï¼šåŸºäºæ­¤æ–°é—»ï¼ŒæŠ•èµ„è€…åº”è¯¥å¦‚ä½•è°ƒæ•´ç­–ç•¥

**æœªæ¥å±•æœ›**ï¼šé¢„æµ‹è¿™ä¸€äº‹ä»¶å¯èƒ½å¸¦æ¥çš„åç»­å‘å±•ï¼Œä»¥åŠæˆ‘ä»¬åº”è¯¥å¦‚ä½•åº”å¯¹ã€‚

**å…³è”ä¿¡æ¯**ï¼šå¦‚æœè¿™åˆ™æ–°é—»ä¸å…¶ä»–äº‹ä»¶æœ‰å…³è”ï¼Œè¯´æ˜å®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚"""

    # ç”¨æˆ·æ¶ˆæ¯
    user_message = f"è¯·å¿«é€Ÿåˆ†æä»¥ä¸‹æ–°é—»çš„æƒ…ç»ªå€¾å‘ï¼Œå¹¶ä¸ºæ¯æ¡æ–°é—»æ·»åŠ æƒ…ç»ªè¯„åˆ†ï¼ˆåªåˆ†ææœ€é‡è¦çš„æ–°é—»ï¼‰ï¼š\n\n{news_content}"
    
    # è°ƒç”¨DeepSeek APIï¼ˆä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼ï¼‰
    headers = {
        'Authorization': f'Bearer {API_KEY}',
        'Content-Type': 'application/json'
    }
    
    payload = {
        "model": "deepseek-chat",  # DeepSeek V3.2çš„æ¨¡å‹åç§°
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ],
        "temperature": 0.7,
        "max_tokens": 2000  # å‡å°‘tokené™åˆ¶ä»¥åŠ å¿«å“åº”
    }
    
    for attempt in range(MAX_RETRIES):
        try:
            logger.info(f"æ­£åœ¨è°ƒç”¨DeepSeek APIè¿›è¡Œç´§æ€¥æ–°é—»åˆ†æ (å°è¯• {attempt + 1}/{MAX_RETRIES})")
            
            # DeepSeek APIåœ¨ä¸­å›½å¢ƒå†…ï¼Œä¸éœ€è¦ä»£ç†
            response = requests.post(
                f"{BASE_URL}/chat/completions",
                headers=headers,
                json=payload,
                proxies=None,  # ä¸ä½¿ç”¨ä»£ç†è®¿é—®DeepSeek API
                timeout=30  # å‡å°‘è¶…æ—¶æ—¶é—´ä»¥åŠ å¿«å“åº”
            )
            
            if response.status_code == 200:
                result = response.json()
                analysis = result['choices'][0]['message']['content']
                
                # æå–æƒ…ç»ªåˆ†æ•°
                sentiment_scores = []
                pattern = r'\[(?:ğŸ”¥|â„ï¸|âš¡|ğŸ“‰|ğŸ“ˆ)\s*([+-]?\d+)\]'
                matches = re.findall(pattern, analysis)
                for match in matches:
                    try:
                        score = int(match)
                        sentiment_scores.append(score)
                    except ValueError:
                        continue
                        
                logger.info(f"LLMåˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ°æƒ…ç»ªåˆ†æ•°: {sentiment_scores}")
                return analysis, sentiment_scores
            else:
                logger.warning(f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.warning(f"LLM APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
            
        if attempt < MAX_RETRIES - 1:
            time.sleep(RETRY_DELAY)
    
    # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬
    logger.error("LLMåˆ†æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬")
    fallback_analysis = ""
    sentiment_scores = []
    for i, item in enumerate(news_items[:3], 1):
        fallback_analysis += f"### 1. [ç‚¹å‡»ç›´è¾¾ï¼š{item['title']}]({item['link']})\n"
        fallback_analysis += f"- **ğŸ“ æ ¸å¿ƒäº‹å®**: {item['summary'][:30]}...\n"
        fallback_analysis += "- **ğŸ“Š æƒ…ç»ªè¯„åˆ†**ï¼š0 (å¾…åˆ†æ)\n"
        fallback_analysis += "- **ğŸŒ å½±å“èŒƒå›´**ï¼šå¾…è¯„ä¼°\n\n"
        fallback_analysis += "---\n"
    
    return fallback_analysis, sentiment_scores

def check_urgent_threshold(sentiment_scores: List[int]) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç´§æ€¥æ¨é€é˜ˆå€¼
    
    Args:
        sentiment_scores: æƒ…ç»ªåˆ†æ•°åˆ—è¡¨
        
    Returns:
        æ˜¯å¦è¾¾åˆ°æ¨é€é˜ˆå€¼
    """
    for score in sentiment_scores:
        if abs(score) >= 9:  # åªæœ‰å½“æƒ…ç»ªåˆ†ç»å¯¹å€¼>=9æ—¶æ‰æ¨é€
            logger.info(f"âœ… æ£€æµ‹åˆ°é«˜æƒ…ç»ªåˆ†æ–°é—»: {score}ï¼Œè§¦å‘ç´§æ€¥æ¨é€")
            return True
    logger.info(f"â„¹ï¸ æƒ…ç»ªåˆ†æœªè¾¾åˆ°é˜ˆå€¼: {sentiment_scores}ï¼Œä¿æŒé™é»˜")
    return False

def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œç´§æ€¥æ–°é—»ç›‘æ§æµç¨‹
    """
    logger.info("ğŸš¨ å¯åŠ¨ç´§æ€¥æ–°é—»ç›‘æ§æœºå™¨äºº...")
    try:
        # 1. æŠ“å–ç´§æ€¥æ–°é—»
        news_items = extract_urgent_news_items()
        if not news_items:
            logger.info("æœªè·å–åˆ°ä»»ä½•ç´§æ€¥æ–°é—»ï¼Œç»“æŸæœ¬æ¬¡ç›‘æ§")
            return
        
        # 2. LLMå¿«é€Ÿæƒ…ç»ªè¯„åˆ†
        analysis_result, sentiment_scores = analyze_urgent_news_with_llm(news_items)
        
        # 3. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ¨é€é˜ˆå€¼
        if check_urgent_threshold(sentiment_scores):
            # 4. æ¨é€åˆ°é£ä¹¦
            success = send_to_feishu(analysis_result)
            if success:
                logger.info("ğŸš¨ ç´§æ€¥æ–°é—»æ¨é€å®Œæˆï¼")
            else:
                logger.error("âŒ ç´§æ€¥æ–°é—»æ¨é€å¤±è´¥")
                send_error_alert("ç´§æ€¥æ–°é—»æ¨é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é£ä¹¦åº”ç”¨é…ç½®")
        else:
            logger.info("âš ï¸ æƒ…ç»ªåˆ†æœªè¾¾åˆ°æ¨é€é˜ˆå€¼ï¼Œä¿æŒé™é»˜")
            
    except Exception as e:
        logger.exception(f"ç´§æ€¥ç›‘æ§ä¸»å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}")
        send_error_alert(f"ç´§æ€¥ç›‘æ§æœºå™¨äººæ•…éšœï¼š{str(e)}ï¼Œè¯·ä¸»äººæ£€æŸ¥ï¼")

if __name__ == "__main__":
    main()